---
layout: post
comments: true
title:  "Diffusers - g√©n√©rer des images avec un mod√®le non condition√©"
ref: diffusers-tutorial-part1
date:   2024-06-02 08:00:00 +0200
categories: blogue deeplearning python
category: blogue
lang: fr
---

Voici mon premier article sur l'utilisation de la biblioth√®que [Diffusers](https://github.com/huggingface/diffusers).

Ces derni√®res ann√©es, les mod√®les de diffusion se sont impos√©s comme un outil puissant dans le domaine de la mod√©lisation g√©n√©rative, rivalisant souvent avec les GAN (Generative Adversarial Networks) dans la production d'images de haute qualit√© (voir [this paper](https://arxiv.org/abs/2105.05233) de 2021 et [stable diffusion](https://github.com/CompVis/stable-diffusion?tab=readme-ov-file) de 2022). Ces mod√®les fonctionnent en simulant un processus de diffusion o√π les donn√©es sont progressivement bruit√©es puis d√©bruit√©es pour g√©n√©rer de nouveaux √©chantillons.

De nouveaux cours apparaissent pour expliquer le fonctionnement de ces m√©thodes. L'un d'entre eux est [How Diffusion Models Works](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) de DeepLearning.ai. Il fournit d'excellents carnets de notes pour cr√©er notre premier mod√®le de diffusion/pipeline. Cependant, il est fastidieux de mettre en ≈ìuvre une telle approche √† partir de z√©ro en utilisant uniquement la biblioth√®que PyTorch (mais c'est un excellent moyen d'apprendre le fonctionnement d'une telle m√©thode).

Dans cet article de blog, nous allons nous plonger dans l'impl√©mentation d'un UNet inconditionnel en utilisant la biblioth√®que [Diffusers](https://github.com/huggingface/diffusers) de deeplearning.ai. Nous allons parcourir les composants cl√©s du code, expliquer comment le processus de diffusion est mod√©lis√© avec la biblioth√®que Diffusers. Tout le code et un notebook de d√©monstrations peuvent √™tre trouv√©s [ici](https://github.com/vroger11/diffusers-tutorial).

**Note:** Ce billet n'explique pas le fonctionnement des mod√®les de diffusion, mais comment utiliser la librairie diffusers pour se simplifier la vie.

## Pourquoi la biblioth√®que diffusers ?

Cette biblioth√®que est soutenue par Hugging Face et offre un cadre robuste et flexible pour travailler avec des mod√®les de diffusion. Voici les principales raisons pour lesquelles vous devriez envisager d'utiliser Diffusers pour vos projets :

- Facilit√© d'utilisation avec une API de haut niveau pour concevoir des mod√®les UNET.
- Documentation compl√®te, m√™me si les tutoriels et les exemples ne couvrent pas tous les cas.
- Communaut√© active et support, avec plus de 23k √©toiles sur leur d√©p√¥t GitHub.

## D√©finir le mod√®le UNET et un planificateur de bruit

Avant d'entra√Æner un mod√®le, nous devons le d√©finir et d√©finir l'ordonnanceur de bruit associ√© :

```python
from diffusers import DDPMScheduler
from diffusers.models import UNet2DModel

# D√©finir le mod√®le UNet
# Note : ce mod√®le utilise moins de param√®tres que le cours deeplearning.ai, car il n'est pas n√©cessaire d'avoir un mod√®le aussi grand pour cette t√¢che
model = UNet2DModel(
    sample_size=(16,16),                              # Taille de l'image d'entr√©e
    in_channels=3,                                    # Nombre de canaux d'entr√©e (par exemple, 3 pour RVB)
    out_channels=3,                                   # Nombre de canaux de sortie
    layers_per_block=2,                               # Couches par bloc dans l'UNet
    block_out_channels=(128, 64),                     # Canaux dans chaque bloc
    down_block_types=("DownBlock2D", "DownBlock2D"),  # Types de blocs descendants
    up_block_types=("UpBlock2D", "UpBlock2D")         # Types de blocks ascendants
)

# D√©finir le planificateur DDPM
noise_scheduler = DDPMScheduler(num_train_timesteps=500)
```

Et c'est tout ! Le [UNet2DModel](https://huggingface.co/docs/diffusers/api/models/unet2d) peut nous faire gagner √©norm√©ment de temps dans la d√©finition d'un tel mod√®le.

## Entra√Æner le mod√®le

Ici, j'ai fait une fonction simple de formation comprenant les bases pour ne pas submerger les lecteurs üòâ :

```python
def train(unet: UNet2DModel, noise_scheduler: DDPMScheduler, dataloader: DataLoader, num_epochs: int, lr: float) -> None:
    """Entra√Æne l'unet √† partir de son noise_scheduler et d'un dataloader.

    Parameters
    ----------
    unet : UNet2DModel
        Le mod√®le unet √† entra√Æner.
    noise_scheduler : DDPMScheduler
        Le planificateur de bruit √† utiliser lors de l'entra√Ænement.
    dataloader : DataLoader
        Le chargeur de donn√©es contenant les images √† reproduire.
    num_epochs : int
        Le nombre d'√©poques pour entra√Æner l'unet.
    lr : float
        Le taux d'apprentissage √† utiliser pour entra√Æner l'unet.
    """
    epochs = range(num_epochs)
    losses = np.zeros(num_epochs)

    optimizer = Adam(unet.parameters(), lr=lr)
    unet.train()

    for epoch in epochs:
        epoch_loss = 0
        for batch in tqdm(dataloader):
            optimizer.zero_grad()

            # En supposant que votre dataloader fournisse des images et des cibles (non utilis√© ici)
            images, _ = batch
            images = images.to(unet.device)

            # G√©n√©rer un bruit al√©atoire
            noise = torch.randn(images.shape).to(unet.device)

            # Forward du mod√®le
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=unet.device).long()
            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)
            predicted_noise = unet(noisy_images, timesteps).sample

            # Calcul de la perte (erreur quadratique moyenne entre le bruit r√©el et le bruit pr√©dit)
            loss = torch.nn.functional.mse_loss(predicted_noise, noise)

            # Backward et optimisation
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        epoch_loss /= len(dataloader)
        losses[epoch] = epoch_loss
        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}")
```

Cette fonction affiche la perte moyenne de chaque √©poque. Pour entra√Æner le mod√®le, nous faisons le forward du mod√®le sur des timeteps al√©atoires avec des donn√©es provenant d'un dataloader. Ensuite nous calculons la perte sur le bruit pr√©dit pour optimiser les param√®tres de notre mod√®le. Dans le notebook, nous avons utilis√© le dataloader de deeplearning.ai, donc rien de nouveau sous le soleil üòõ. Comme vous pouvez le voir, la biblioth√®que Diffusers n'est pas magique et n√©cessite que nous impl√©mentions certaines choses nous-m√™mes.

Pour sauvegarder notre mod√®le pr√©-entra√Æn√©, une seule ligne est n√©cessaire :

```python
model.save_pretrained(pre_trained_model_path)
```

## Faire des inf√©rences avec le mod√®le entra√Æn√©

Maintenant que nous avons appris notre mod√®le, nous pouvons le charger et l'utiliser sur un pipeline de la biblioth√®que diffusers.
Tout d'abord, chargeons les param√®tres de notre mod√®le :

```python
model = UNet2DModel.from_pretrained(pre_trained_model)
```

C'est assez facile ! Maintenant, d√©clarons notre pipeline et utilisons cuda si disponible :

```python
from diffusers import DDPMPipeline

pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)
pipeline.to("cuda" if torch.cuda.is_available() else "cpu")
```

Et c'est tout, maintenant g√©n√©rer de nouvelles images est aussi simple que :

```python
generated_image = pipeline(batch_size=16, num_inference_steps=500)
```

J'ai cr√©√© un outil simple pour montrer les images g√©n√©r√©es (vous pouvez jeter un oeil au d√©p√¥t GitHub pour plus d'informations).
Visualisons donc le r√©sultat :

```python
fig = plot_generated_images(generated_image.images, 4, 4)
fig.show()
```

![images g√©n√©r√©es](/assets/images/diffusers/tutorial_1_unconditional.png)

Nous y voil√†, nous avons g√©n√©r√© nos premiers exemples en utilisant la librairie diffusers.
Dans un prochain article de blog, nous verrons comment g√©n√©rer des exemples √† partir d'une v√©rit√© de terrain (au lieu de tutoriels n'utilisant que du texte et de suivre le cours deeplearning ai).

J'esp√®re que cela aidera et/ou inspirera certains d'entre vous.

A bient√¥t, Vincent.
