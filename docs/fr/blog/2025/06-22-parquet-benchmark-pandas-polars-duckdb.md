---
comments: true
title: "Benchmark : Pandas, Polars, DuckDB avec des colonnes cat√©gorielles Parquet"
date: 2025-06-22
tags:
  - Donn√©es
  - D√©veloppement
  - Python
hide:
  - navigation
excerpt: "Benchmark des performances de chargement et d'analyse sur des colonnes cat√©gorielles Parquet : Pandas vs Polars vs DuckDB"
---

Stocker des donn√©es de capteurs structur√©es au format Parquet permet de r√©duire significativement l‚Äôespace disque et l‚Äôusage m√©moire, en particulier lorsque l‚Äôon encode les colonnes cat√©gorielles plut√¥t que d‚Äôutiliser des cha√Ænes de caract√®res brutes.
Je voulais √©valuer comment diff√©rentes biblioth√®ques Python se comparent en termes de **vitesse de chargement**, **performance analytique** et **utilisation m√©moire** avec ce type de colonnes.

D‚Äôapr√®s mon exp√©rience, Polars est souvent le plus rapide sur des donn√©es num√©riques ou de simples cha√Ænes, mais le comportement change lorsqu‚Äôon utilise de v√©ritables types cat√©goriels.

Dans ce billet, je compare trois outils populaires :

- [Pandas](https://pandas.pydata.org)
- [Polars](https://pola.rs)
- [DuckDB](https://duckdb.org)

## üéØ Motivation

Parquet est un format de stockage en colonnes largement adopt√©, id√©al pour des structures tabulaires comme celles des DataFrames (Pandas/Polars) ou des bases de donn√©es relationnelles.
Ce format est particuli√®rement efficace quand on encode les colonnes cat√©gorielles.

Cependant, les performances varient fortement selon :

- La mani√®re dont les colonnes cat√©gorielles sont g√©r√©es
- L‚Äôusage d‚Äôune ex√©cution paresseuse (lazy) ou imm√©diate (eager)
- Le co√ªt des conversions internes pendant le chargement ou le calcul

Pour explorer ces diff√©rences, j‚Äôai con√ßu un benchmark bas√© sur des journaux r√©els de capteurs port√©s, o√π des m√©tadonn√©es cat√©gorielles (activit√©, type de capteur, orientation) compl√®tent les valeurs num√©riques.

Le script de g√©n√©ration du jeu de donn√©es est disponible dans la section *Code et reproductibilit√©*.

### Qu‚Äôest-ce qu‚Äôune colonne cat√©gorielle et pourquoi les utiliser ?

Les colonnes cat√©gorielles prennent un nombre fini de valeurs distinctes.
Elles sont souvent repr√©sent√©es sous forme de cha√Ænes dans les jeux de donn√©es bruts (par exemple `"walking"`, `"running"`), mais peuvent √™tre encod√©es en entiers pour plus d‚Äôefficacit√©.

Dans Parquet, cet encodage se fait via un dictionnaire : les valeurs sont stock√©es comme des entiers avec une table de correspondance pour retrouver la cha√Æne d‚Äôorigine.
Cela permet un stockage compact tout en conservant la lisibilit√© lors des requ√™tes.

Chaque framework g√®re ces colonnes diff√©remment, mais le format Parquet garantit une compatibilit√© entre biblioth√®ques comme Pandas, Polars et DuckDB.

## üì¶ Jeu de donn√©es : REALWORLD2016

Ce benchmark utilise le [jeu de donn√©es REALWORLD2016](https://www.uni-mannheim.de/dws/research/projects/activity-recognition/dataset/dataset-realworld/) compos√© de mesures acc√©l√©rom√®tre, gyroscope et magn√©tom√®tre sur 15 individus r√©alisant diff√©rentes activit√©s.

Chaque enregistrement contient :

- Acc√©l√©rations `x`, `y`, `z`
- Index temporel `time`
- ID utilisateur `user`
- M√©tadonn√©es cat√©gorielles :
  - `activity` (ex : marche, saut)
  - `orientation` (ex : torse, cuisse)
  - `sensor` (acc, gyr, mag)

J‚Äôai converti les fichiers CSV compress√©s d‚Äôorigine en fichiers Parquet columnaires √† l‚Äôaide de Pandas et PyArrow. Les colonnes cat√©gorielles ont √©t√© typ√©es explicitement au format standard d‚ÄôArrow, plus interop√©rable que les types enum sp√©cifiques √† Polars ou DuckDB.
Ainsi, les fichiers g√©n√©r√©s sont compatibles avec Pandas, Polars et DuckDB.

## üß™ T√¢ches de benchmark

Chaque outil a √©t√© √©valu√© sur quatre t√¢ches :

1. **Chargement** des fichiers Parquet
2. **Statistiques descriptives** : min, max, moyenne sur les colonnes num√©riques
3. **Comptes group√©s** sur les colonnes `activity`, `orientation` et `sensor`
4. **Utilisation m√©moire** : RAM minimale n√©cessaire pour charger enti√®rement les donn√©es

## üìÅ Code et reproductibilit√©

Tous les scripts et notebooks sont disponibles ici :

üëâ [D√©p√¥t GitHub](https://github.com/vroger11/parquet-categorical-benchmark)

Pour reproduire les benchmarks :

```bash
uv sync
uv run python download_extract_realworld.py   # g√©n√®re les fichiers Parquet
jupyter notebook load_data_benchmarks.ipynb   # ex√©cute les benchmarks
```

Je recommande d‚Äôouvrir le notebook pour consulter les impl√©mentations ainsi que les remarques et astuces ajout√©es dans les cellules.
Vos suggestions et contributions sont les bienvenues (issues, pull requests ou commentaires).

## üìà R√©sum√© des r√©sultats

### Temps de chargement

```mermaid
xychart-beta
    title "Temps de chargement (en secondes)"
    x-axis ["Polars Eager initial", "Polars Eager", "Polars Lazy", Pandas, DuckDB, "df depuis DuckDB"]
    bar [62.0, 9.0, 4.6, 4.5, 3.3, 16.1]
```

La premi√®re impl√©mentation en eager de Polars, utilisant la concat√©nation implicite, est de loin la pire (62 s).
La concat√©nation explicite avant de passer les donn√©es √† Polars s‚Äôest r√©v√©l√©e bien plus performante.
C‚Äôest un cas rare o√π d√©l√©guer √† la logique interne d‚Äôune biblioth√®que nuit √† la performance.

Polars en lazy est aussi rapide que Pandas, tandis que DuckDB est l√©g√®rement plus rapide encore.
R√©cup√©rer le DataFrame Pandas depuis DuckDB √©tait sensiblement lent et devrait √™tre √©vit√©, car c‚Äôest plus lent que de charger les donn√©es directement avec Pandas.

!!! note "Note"
    En g√©n√©ral, il faut √©viter d'appeler `.collect()` sur un DataFrame lazy Polars sans cha√Æne d‚Äôop√©rations.
    Ici, `.collect()` est volontairement appel√© pour isoler et comparer les performances entre outils.
    M√™me remarque pour DuckDB o√π il faut passer `hive_partitioning = true` dans votre `read_parquet` afin d‚Äô√©viter de pr√©charger des donn√©es non n√©cessaires.

### Describe + value_counts

On compare maintenant des op√©rations analytiques, √† commencer par `describe()` :

```mermaid
xychart-beta
    title "Calcul de describe (en secondes)"
    x-axis ["Polars", "Pandas", "DuckDB"]
    bar [1.6, 4.5, 9.8]
```

Les donn√©es √©tant d√©j√† en m√©moire, Polars ne peut pas exploiter ses optimisations lazy.
Il reste tout de m√™me le plus rapide.

Puis les comptes group√©s (value_counts) :

```mermaid
xychart-beta
    title "Comptes group√©s (en secondes)"
    x-axis ["Pandas", "Polars Lazy", "Polars Eager", "DuckDB"]
    bar [1.0, 1.2, 1.5, 0.3]
```

Surprise : Pandas surpasse ici Polars Lazy (1.0 s contre 1.2 s).
DuckDB est bien plus rapide.

!!! warning "Correction"
    Dans une version pr√©c√©dente de ce post, j'avais indiqu√© une valeur incorrecte pour DuckDB.
    Merci √† [√âric Mauvi√®re](https://www.linkedin.com/in/ericmauviere/) d'avoir signal√© cette incoh√©rence.

### Utilisation m√©moire

On mesure ici la RAM n√©cessaire pour contenir les donn√©es en m√©moire.
Attention, certaines biblioth√®ques utilisent temporairement plus de m√©moire pendant le chargement.

```mermaid
xychart-beta
    title "M√©moire n√©cessaire (en Go)"
    x-axis ["Pandas", "Polars", "DuckDB", "Dataframe depuis DuckDB"]
    bar [2.17, 2.71, 6.33, 11.83]
```

Polars utilise environ 25% de RAM en plus que Pandas, en raison d‚ÄôArrow et de son cache interne.

DuckDB consomme presque 3 fois plus que Pandas, car il stocke les colonnes cat√©gorielles comme des cha√Ænes compl√®tes (VARCHAR).

Exporter le DataFrame depuis DuckDB aggrave la situation : on atteint 18.16 Go n√©cessaires.

## üìã Conclusion

Chaque outil a ses points forts :

| Outil  | Avantages                        | Inconv√©nients                   |
| ------ | -------------------------------- | ------------------------------- |
| Pandas | API simple, chargement rapide    | Pas d‚Äôex√©cution lazy            |
| Polars | Analyse tr√®s rapide (en lazy)    | Moins performant si mal utilis√© |
| DuckDB | Chargement comp√©titif, SQL natif | Grosse consommation m√©moire     |

Polars est tr√®s efficace si bien utilis√©, mais n√©cessite de conna√Ætre le lazy execution model.
DuckDB reste un bon outil SQL, mais √† √©viter pour les colonnes cat√©gorielles Arrow, tant qu‚Äôelles sont repr√©sent√©es en VARCHAR.

---

*Merci pour la lecture. Vous pouvez ouvrir une issue, proposer une am√©lioration dans le d√©p√¥t, ou laisser un commentaire sur ce billet.*
