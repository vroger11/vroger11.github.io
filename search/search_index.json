{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About me","text":"<p>Data and machine learning scientist.</p> <p>PhD in computer science using machine learning (especially Neural Networks technologies). I work principally on image and audio signals (speech, music, animals and so on). I am a Python programmer and a FOSS enthusiast.</p>"},{"location":"#resume","title":"R\u00e9sum\u00e9","text":"<p>PhD in Data Science since 2022 (for more information, click here). My work focuses on 1D and 2D signal processing, with more experience in audio processing. I work with limited data sets as well as large ones. I focus on machine learning algorithms to model knowledge and integrate it into microservices (based on Docker). My preferred programming language is Python, allowing me to focus on functionalities (and benefit from a wide range of libraries). As an open-source enthusiast, I contribute to open-source projects (bug reports, patch proposals, discussions, etc.) whenever possible. I also perform data analysis to produce visualizations. Most of them are available on my blog (and as a preview on Reddit or my X (formerly Twitter) account).</p> <p>Finally, I also maintain a blog on visualizations I create, research I conduct, deep learning tutorials, and tips. Check it out.</p> <p>You can find my full r\u00e9sum\u00e9 here or visit my LinkedIn.</p>"},{"location":"contact/","title":"Contact me","text":"<p>There are several ways to contact me:</p> <ul> <li> Email me for professional inquiries.</li> <li> LinkedIn for professional contact and blog posts.</li> <li> Reddit for informal discussions or posts.</li> </ul>"},{"location":"blog/","title":"Welcome to my blog","text":"<p>This blog features articles on machine learning, visualizations I have created, as well as programming tips and advice on the Linux environment.</p> <p>All articles are accessible from the sidebar on the left, sorted by date with the most recent ones at the top. On small screens and smartphones, use the hamburger menu to navigate.</p> <p>You can also search for an article by its tag or use the categories below:</p>  Automation Machine Learning Visualizations <p>I've written several articles about automating various tasks during development. Here are a few highlights:</p> <ul> <li>Save your Git credentials to avoid repeatedly entering your password.</li> <li>Automation with SSH keys, where we explore useful tasks you can automate using SSH keys.</li> <li>Working remotely or from home, detailing how to configure your workstations for remote access.</li> </ul> <p>I've created tutorials on generating images using the Diffusers library:</p> <ol> <li>Generating images with an unconditional model.</li> <li>Generating images from labels with a conditional model.</li> <li>Improving the speed and quality of image generation.</li> </ol> <p>Additionally, I wrote a post on how to speed up training on AI servers.</p> <p>During my PhD, I conducted a survey on few-shot techniques for audio processing\u2014check it out here.</p> <p>In my free time, I create visualizations, particularly for Age of Empires 2 tournaments, which have garnered over 100k views. Some of my articles include:</p> <ul> <li>Red Bull Wololo Legacy</li> <li>Warlords</li> <li>The Grand Melee</li> </ul> <p>I also presented a survey on Python data visualization tools for the Toulouse Dataviz association.</p> <p>To stay up to date with new posts, subscribe to my Atom/RSS feed and/or to my social networks.</p>"},{"location":"blog/2019/09-08-hello-world/","title":"Hello world","text":"<p>This is my first post to present myself and what my blog will contain.</p> <p>I am Vincent a PhD student in Machine Learning at IRIT Toulouse. I use Deep Learning Techniques on speech signals with few data available (few shot context). My blog will consist in sharing some of my reading, papers, tips about Machine Learning, Programming (I mainly use Python, LaTeX and Markdown languages) and Linux.</p> <p>Stay tuned, I hope it will help my readers.</p> <p>See you again, Vincent.</p>","tags":["Archive"]},{"location":"blog/2019/09-09-save-git-credentials/","title":"Save Git Credentials","text":"","tags":["Automation","Development"]},{"location":"blog/2019/09-09-save-git-credentials/#what-are-git-credentials","title":"What are git credentials?","text":"<p>Git credentials are the username and password you used to log into a repository on a server. Saving them allows you to avoid typing your username and password when required (mainly pull and push operations).</p>","tags":["Automation","Development"]},{"location":"blog/2019/09-09-save-git-credentials/#why-saving-git-credentials","title":"Why saving git credentials?","text":"<p>The use of SSH and GPG keys is available on most popular git servers such as on GitLab or on GitHub websites. It allows the user to avoid typing their username and password.</p> <p>Some website does not support this feature (overleaf git server for instance). Also, if you are like me, you use a multitude of git servers (and configuring each SSH keys become a pain).</p>","tags":["Automation","Development"]},{"location":"blog/2019/09-09-save-git-credentials/#save-git-credentials","title":"Save git credentials","text":"<p>To save the credentials of every repository you log into, just type this:</p> <pre><code>git config credential.helper store\n</code></pre> <p>And you are good to go. You will be asked only once for each repository. Afterwards, git will fill automatically the credentials for each repository.</p>","tags":["Automation","Development"]},{"location":"blog/2019/09-09-save-git-credentials/#cache-expiration","title":"Cache expiration","text":"<p>For security reasons (especially if you are not on your personal computer), it is better that the stored credentials expire after a certain amount of time. Let's say you want that delay being 4 hours. You just have to type:</p> <pre><code>git config --global credential.helper 'cache --timeout 14400'\n</code></pre> <p>Note</p> <p>14400 is then the time in seconds (our 4 hours).</p> <p>Hope it helps some of you.</p> <p>See you again, Vincent.</p>","tags":["Automation","Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/","title":"Terminal multiplexers","text":"","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#what-are-terminal-multiplexers","title":"What are terminal multiplexers?","text":"<p>Terminal multiplexer is a tool allowing to have multiple sessions/windows in a single terminal display. It is useful to split screen into multiple shells or accessing to multiple windows from the current one. The handiest use is when you use remote computers/servers to do some computations (let say to learn a model on GPUs). Indeed, throw a ssh connection you can't leave your session without killing your current process. While terminal multiplexers allow the user to log out and keeping the session active. It requires to detach the current session. After a reconnection throw ssh, you have to reattach your detached session (without losing your processes).</p> <p>I use it a lot on my local machine and on remote servers to have a homogeneous way to interact with my different setups. In this article I will expose Tmux software. In this post I will show you how to install/configure and use it.</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#tmux","title":"Tmux","text":"<p>I love to use only one multiplexer simultaneously. Nevertheless, it is possible to use multiple ones with a specific profile for each of them. If it interests you, I recommend you to look into tmuxp project. In this post, I will only explain how I configure Tmux and how I use it.</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#installation","title":"Installation","text":"","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#on-ubuntu","title":"On Ubuntu","text":"<pre><code>sudo apt install tmux\n</code></pre>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#on-fedora","title":"On Fedora","text":"<pre><code>sudo dnf install tmux\n</code></pre>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#configuration","title":"Configuration","text":"<p>All configurations are in <code>~/.tmux.conf</code> file. In this section I will detail my personal configuration. My configuration files are here. I will update this post over time as my configuration evolve.</p> <p>First, lets look at my configuration:</p> <p>I do not modify a lot the tmux status bar, the only modifications are the following:</p> <ul> <li>the selected window appears in blue (the others are in green),</li> <li>the bottom right display the ram usage for the GPUs and the CPU RAM.</li> </ul>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#installation_1","title":"Installation","text":"<p>My configuration is in two parts: bash scripts to acquire system information and Tmux config file. To install my config you have to download the files here and do:</p> <pre><code>cp -r tmux_scripts ~/.tmux_scripts\ncp tmux.conf ~/.tmux.conf\n</code></pre> <p>My scripts are as follows:</p> <ul> <li><code>gpu.sh</code> script using nvidia-smi command (it is up to you to install it) to get GPU ram usage in Gio. It takes the GPU id as a parameter (first GPU being 0). It is up to you to add more calls to this script (in the tmux.conf file where I set status-right option) to add other GPUs. If you want to improve this behavior, feel free to contribute.</li> <li><code>ram.sh</code> script to get RAM usage in Gio.</li> </ul> <p>The config file enables mouse interactions (for scrolling, select my split or resize my splits) It also modifies some colors and information displayed in the status bar. The status bar is updated every 10 seconds. Look at The Tao of tmux book for more styling options and how to modify it.</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#usage","title":"Usage","text":"<p>First, let start a session. In your terminal (or terminal emulator) type this:</p> <pre><code>tmux\n</code></pre> <p>I currently use the default shortcuts combined with my mouse. In this post I will only mention the ones I'm using. To use tmux shortcuts, you have to press <code>&lt;Ctrl+b&gt;</code> first then type:</p> <ul> <li><code>\"</code> for a horizontal split.</li> <li><code>%</code> for a vertical split.</li> <li><code>z</code> to zoom in/out of the current terminal.</li> <li><code>c</code> to create a new tab in your multiplexer.</li> <li><code>d</code> to detach your session.</li> </ul> <p>I do every other actions like selecting terminal tab, resizing split or select split using my mouse. To reattach your previous session, just type:</p> <pre><code>tmux attach-session\n</code></pre> <p>If you want to know other shortcuts and how to deal with multiple sessions, I recommend you to read The Tao of tmux book or look into the cheatsheet here.</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#change-default-new-split-pane-behavior","title":"Change default new split pane behavior","text":"<p>As I found myself always typing the same <code>cd</code> command after creating new split pane. To tackle this, I add the following lines in my <code>tmux.conf</code> file:</p> <pre><code>bind % split-window -h -c \"#{pane_current_path}\"\nbind '\"' split-window -v -c \"#{pane_current_path}\"\n</code></pre> <p>Now new split panes will have the same directory as the current pane.</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#bonus-alternative-for-gnome-users","title":"Bonus: Alternative for Gnome users","text":"<p>If you want a tmux-like well configured (by default) and you use Gnome with gnome-terminal, I recommend you to check out Byobu. It has a well-explained video in its home page. Its shortcuts do not always work on other terminals (which is a no go for me as I use Konsole terminal emulator).</p>","tags":["Development"]},{"location":"blog/2019/09-23-terminal-multiplexers/#sources","title":"Sources","text":"<ul> <li>Tmux man page</li> <li>The Tao of tmux</li> </ul> <p>Hope it helps some of you.</p> <p>See you again, Vincent.</p>","tags":["Development"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/","title":"Check your writing in Neovim","text":"<p>Neovim is a text editor designed to be able to use either a command-line interface or a graphical user interface. I use it to write nearly everything (mainly Python, markdown and LaTeX files). Many plugins are available to add handy features to Neovim (including the ones designed for Vim). Basically, it is an enhanced Vim, see the Neovim about page for more details. After writing, automatic checks are useful and help you to save some time. In this post I share my Neovim configuration and usage.</p>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#spell-checking","title":"Spell-Checking","text":"<p>To do simple spell-checking, you have to add in your <code>~/.config/nvim/init.vim</code> file the following lines:</p> <pre><code>\" spell languages\nset spelllang=en\nnnoremap &lt;silent&gt; &lt;C-s&gt; :set spell!&lt;cr&gt;\ninoremap &lt;silent&gt; &lt;C-s&gt; &lt;C-O&gt;:set spell!&lt;cr&gt;\n</code></pre>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#usage","title":"Usage","text":"<p>In Neovim press Ctrl+S to highlight the misspelled words, then repress Ctrl+S to hide the highlighted misspelled words. To select a word suggestion in Insert mode, press Ctrl+X then S to select a suggestion. In Normal mode press <code>z=</code> to see the word suggestions.</p> <p>Note</p> <p>I know that my configuration inhibits the Ctrl+S user signal and it is my point to avoid unwanted behavior while typing.</p>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#grammar-checking","title":"Grammar Checking","text":"<p>To check my writing I use LanguageTool suite. It is available for free, have an offline mode using its open-source software (it is the main reason I'm using it). Here I will help you to install it and how to use it with Neovim.</p>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#languagetool-installation","title":"LanguageTool installation","text":"<p>LanguageTool requires java runtime. Here I will install OpenJDK and curl:</p>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#on-fedora","title":"On Fedora","text":"<pre><code>sudo dnf -y install java curl\n</code></pre>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#on-ubuntu","title":"On Ubuntu","text":"<pre><code>sudo apt -y install default-jre curl\n</code></pre>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#then-on-both-distributions","title":"Then, on both distributions","text":"<p>The following commands download LanguageTool and install it in <code>/usr/local</code>.</p> <pre><code>curl https://languagetool.org/download/LanguageTool-stable.zip &gt; /tmp/LanguageTool-stable.zip\nsudo unzip /tmp/LanguageTool-stable.zip -d /usr/local/LanguageTool\nsudo mv /usr/local/LanguageTool/LanguageTool-*/* /usr/local/LanguageTool/\nrm /tmp/LanguageTool-stable.zip\n</code></pre>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#neovim-configuration","title":"Neovim configuration","text":"<p>I use vim-plug plugin manager in Neovim. It is why I use it to install the LanguageTool plugin from vigoux (but other choices are possible). Before I continue, I recommend you to have a recent version of Neovim (\u2265 0.4.2) to use this plugin (as it use recent changes in Neovim).</p> <p>In your <code>~/.config/nvim/init.vim</code> between the <code>call plug#begin(&lt;plugging_folderpath&gt;)</code> and <code>call plug#end()</code> you have to add the following lines:</p> <pre><code>\" Language tool integration\nPlug 'vigoux/LanguageTool.nvim'\n</code></pre> <p>Then, outside the vim-plug calls you have to add:</p> <pre><code>\" grammar checking\nautocmd Filetype markdown LanguageToolSetUp\nautocmd Filetype tex LanguageToolSetUp\nlet g:languagetool_server_jar='/usr/local/LanguageTool/languagetool-server.jar'\n</code></pre>","tags":["Development","Writing"]},{"location":"blog/2019/10-08-check-your-writing-in-nvim/#usage_1","title":"Usage","text":"<p>To use this plugin you first have to call <code>:LanguageToolSetUp</code> in Normal mode to start the LanguageTool server (which is automatically launched for tex files and markdown files).</p> <p>Then, the plugin proposes the following useful commands:</p> <ul> <li><code>:LanguageToolCheck</code>: to highlight writing mistakes, you have to recall it when you change your code/text.</li> <li><code>:LanguageToolSummary</code>: to show details on highlighted errors in a split.</li> <li><code>:LanguageToolClear</code>: to clear LanguageTool displays.</li> </ul> <p>To see more options look at the plugin page.</p> <p>Note</p> <p>I frequently alternated between using LanguageTool and Antidote. Ultimately, I chose LanguageTool because it integrates seamlessly with my coding environment and workflows.</p> <p>Hope it helps some of you. If you have a better setup or want to improve mine feel free to contribute and contact me.</p> <p>See you again, Vincent.</p>","tags":["Development","Writing"]},{"location":"blog/2019/10-12-first-year-review-as-a-phd-student/","title":"Review of my first year as a PhD student","text":"<p>After almost a year (11 months), here I share with you my first year as a PhD student. In this post I will talk about the subject of my PhD thesis, the direction it takes, future works and the changes in my life (due to PhD).</p> <p>Before I continue, I would like to thank my advisors<sup>1</sup> for their help and advises, La R\u00e9gion Occitanie and Federal University of Toulouse for funding me, the University Paul Sabatier and the IRIT lab for their support. Also, I want to remind you that it is my blog (it only engages myself).</p>","tags":["Archive","PhD"]},{"location":"blog/2019/10-12-first-year-review-as-a-phd-student/#subject","title":"Subject","text":"<p>My thesis consists in creating an Automatic System of Intelligibility Measurement (SAMI in French). In the speech processing field, intelligibility can mean multiple things. First, lets define it by looking how a listener listens:</p> <p>Classic intelligibility measurements suppose that the signal from the listened speaker is 100% clear and is alterable by the environment. Hence, they measure the perturbations done by the environment. It is not what I am focused on for this thesis.</p> <p>In this thesis, we are interested in the clarity of the speaker independently of any environment and any knowledge from the listener. Hence, by intelligibility I mean the speaker intelligibility.</p> <p>This measures have multiple applications:</p> <ul> <li>evaluate people diction, especially foreigners to help them.</li> <li>refine information for hearing aids to improve their efficiency.</li> <li>evaluate the damages of one or multiple diseases (such as Parkinson or oral cancers) to help the reeducation process.</li> </ul> <p>The last point is the focus of my thesis. Hence, my automatic system has to work with pathological speech and unimpaired speech.</p> <p>To achieve my goal, I have access to the C2SI corpus. It contains around 2 hours of recording for multiple tasks (such as reading, spontaneous speech or maintained A). All speakers are French and are speaking French. The dataset contains patients (that suffer from oral cancer) and controls (which have no problems to speak). More importantly, in this dataset we have access to a subjective evaluation of the intelligibility for every speaker. A panel of 5 experts produced intelligibility scores valuated between 0 and 10. This dataset is the starting point of my thesis.</p>","tags":["Archive","PhD"]},{"location":"blog/2019/10-12-first-year-review-as-a-phd-student/#thesis-direction","title":"Thesis direction","text":"<p>I am a data scientist and I like to let the machine learn concepts for me. It is why my advisors and I decided to learn the concept of intelligibility using machine learning techniques with end-to-end systems (based on deep learning techniques).</p> <p>After some experiments on the C2SI dataset, it appears to me that:</p> <ul> <li>Using state-of-the-art learning techniques for speech processing (with big architectures) is impossible on the available amount of data I have.</li> <li>Using models with fewer parameters (with techniques such as SincNet and use small architectures) does not perform well enough on pathological speech.</li> <li>Use pre-trained models on unimpaired speech to do transfer learning performs worse than \"light\" models.</li> </ul> <p>I can't have more data (as it can be painful for the patients) to obtain better results. Classic data augmentation isn't a solution either. After listening some recordings, I can guess the intelligibility range of most speakers. It reminds me some reading about techniques that try to be as efficient as a baby to learn new concepts (in terms of quantity of data required): few-shot learning. As it corresponds to my problem, I started to review what existed in that domain. To share that expertise acquired, I started to write a survey on few-shot learning for speech processing.</p>","tags":["Archive","PhD"]},{"location":"blog/2019/10-12-first-year-review-as-a-phd-student/#coming-next","title":"Coming next","text":"<p>My planned publications (and future posts) are the following:</p> <ol> <li> <p>Survey on few-shot learning techniques for speech processing.</p> </li> <li> <p>Experiments using promising few-shot techniques on the C2SI dataset.</p> </li> </ol>","tags":["Archive","PhD"]},{"location":"blog/2019/10-12-first-year-review-as-a-phd-student/#feeling-and-impact-on-my-way-of-life","title":"Feeling and impact on my way of life","text":"<p>I stopped digital drawing as it took me a least two hours per session. I learn everything from scratch in that domain (softwares, drawing techniques and so on). It feels hard and it takes me a lot of mental energy. Instead, I started my website and this blog as I like to share stuff. Also, it feels easier and more fun to me as I expected (it is mainly due to the simplicity of Jekyll). Maybe I will redo digital drawing, but after my thesis.</p> <p>I'm starting learning acrobatic rock with my love (my Krikri). I believe having too much physical exercises can harm my body but also my moral. So I stopped running.</p> <p>As a computer scientist guy, I am often in a building behind a desk. The sun does not directly go on my skin and I produce less Vitamin D than peoples who work outside. To compensate I take some vitamin D3, look at here or there to better understand why I do so. I recommend you to talk about it with a professional. Also, to be more exposed to the sun (and meditate/relax) I avoid the subway as much as I can by using my scooter (a manual one \ud83d\ude04).</p> <p>When I have less motivation than usual, I listen to the following songs (maybe it can cheer you up):</p> <ul> <li>Into the Abyss, by Hilltop Hoods.</li> <li>Here, by Briggs.</li> <li>Fight back, by  Neffex.</li> </ul> <p>Hope it was instructive and can help some of you.</p> <p>See you again, Vincent.</p> <ol> <li> <p>Julien Pinquier, J\u00e9r\u00f4me Farinas and Virginie Woisard \u21a9</p> </li> </ol>","tags":["Archive","PhD"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/","title":"Use Zotero to generate BibTeX files for your papers","text":"","tags":["Research"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/#what-is-zotero","title":"What is Zotero?","text":"<p>Zotero is a software that allows you to collect, organize, share and synchronize research papers (or other research contents). I prefer Zotero to other solutions as it fulfill all my needs, it is open source and \"developed by an independent, nonprofit organization that has no financial interest in private information\".</p>","tags":["Research"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/#installation","title":"Installation","text":"<p>Zotero is not directly available in default Ubuntu repositories. I prefer to install it via Flatpak as it enables automatic updates like other classic applications in Ubuntu.</p> <p>First, let's install Flatpak and add Flathub repository for Flatpak:</p> <pre><code>sudo apt install flatpak\nflatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\n</code></pre> <p>Now let's install Zotero:</p> <pre><code>flatpak install flathub org.zotero.Zotero\n</code></pre> <p>Zotero is now installed and a shortcut is available in your application menu.</p>","tags":["Research"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/#extensions","title":"Extensions","text":"<p>To be able to use all the features presented in this post you also need to install the following extensions:</p> <ul> <li>The Zotero connector for your web browser.</li> </ul> <p>To install it, go to the Chrome (compatible with chromium-based browsers) or the Firefox add-on page.</p> <ul> <li>Zotero software can add extensions, in my daily use I only use one: Better BibTeX.</li> </ul> <p>To install it:   1. Download the latest xpi file.   2. Open Zotero, go to <code>Tools</code> -&gt; <code>Add-ons</code>, click on the gear -&gt; <code>Install Add-on From File...</code> and select your previously downloaded extension.</p>","tags":["Research"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/#add-papers-to-your-collections","title":"Add papers to your collections","text":"<p>First, I recommend you to create an online account on the Zotero website. It will help you to synchronize your library for all your devices (in my case at home and at work). To sign in the Zotero application, go under <code>Edit</code> -&gt; <code>Preferences</code> -&gt; <code>Sync</code> tab.</p> <p>Now you can use your browser to look into papers on the web (in arXiv for example). When you want to add a paper to your collection, just click on the Zotero connector button in your web browser while you are looking into the PDF of the desired paper. It will automatically download the paper for offline use and extract all the information required to cite this paper (such as Title, Authors, Date and so on). If any uploaded information looks wrong, you can modify it at any time.</p>","tags":["Research"]},{"location":"blog/2019/11-06-using-zotero-to-generate-bibtex/#generate-your-bibtex-file-using-zotero","title":"Generate your BibTeX file using Zotero","text":"<p>Before generating your BibTeX file, you may want to personalize the citation key format you will use in your TeX file. To do so, go to the Better BibTeX menu: Edit -&gt; Preferences -&gt; Better BibTeX tab. I use the default citation key format which please me: <code>[auth:lower][shorttitle3_3][year]</code> It corresponds to the first author (in lower cases), followed by the title of the paper (truncated) and the year of publication. If you want to modify the format of the generated keys, have a look to the official documentation here.</p> <p>After doing this, don't forget to refresh the citation key for every item in your collection (it is not automatic, but Ctrl+A key combination is your ally). Even if you want to keep the default citation key format from Better BibTeX extension, you have to do it. At least once after installing better BibTeX extension, unless Zotero keeps its default key format which uses underscores (which is better to avoid if you use LaTeX).</p> <p>To generate the BibTeX file:</p> <ol> <li> <p>Right-click on the collection (or library) containing all the papers you want for your Bib file then click on <code>Export Collection</code>, just as follows:</p> <p> </p> </li> <li> <p>In the contextual menu, select \"Better BibTeX\" format (which use a cleaner format compared to the default BibTeX format of Zotero) and check the \"Keep updated\" box to let Zotero update your bib file when you add/modify your Library.      </p> </li> </ol> <p>You have now a bib file usable for your paper with your citation key format.</p> <p>Hope it helps some of you.</p> <p>See you again, Vincent.</p>","tags":["Research"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/","title":"Improve your bash terminal experience","text":"<p>This post is focused on bash configuration. I use bash as it is the default shell in most Linux distributions. I use it daily and after some tweaks it better suits me. In this post I will describe my main usage and how I configure it.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#using-the-default-ubuntu-bashrc-configuration","title":"Using the default Ubuntu bashrc configuration","text":"<p>The default configuration in Ubuntu is nice and I use it as a base for my default configuration. First, let review the basic features it offers.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#default-completion","title":"Default completion","text":"<p>Completion in terminal is essential to gain productivity. In bash it is by default activated by pressing the Tab key. Ubuntu enables most of the useful completions by default, such as command options completion (<code>apt ins&lt;Tab&gt;</code> gives you <code>apt install</code>) or folders/files completion. Nevertheless, the default completion behavior seems weird to me. If you press multiple times the Tab key you end up with the same listing showed to you multiple times. Such as in the following clip:</p> <p>In the next section, we will see how I configure it to be more consistent and useful.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#recursive-search","title":"Recursive search","text":"<p>To get previous typed commands in a terminal you can use the upper arrow key. But, if you know a part of the command you want to reuse (and it is high in your bash history) you can use the recursive mode. To enter in recursive mode, you have to press CTRL+R keys and type the first letters of your choice (such as <code>ssh</code>). The result is the first command matching your search in your bash history. If you want the next matching results in your bash history, you just have to press the CTRL+R key combination. I appreciate this behavior, so I let it as is in my configuration.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#alerts","title":"Alerts","text":"<p>It is a handy alias to send notification and it is in the default Ubuntu bashrc. To have this functionality working, you have to install the <code>libnotify</code> library:</p> <pre><code>sudo apt install libnotify-bin\n</code></pre> <p>It sends an alert to your desktop environment when the command is executed. I often use it after logical and while I prototype light models. Here is an example you can try at home to better understand its usefulness:</p> <pre><code>sleep 3 &amp;&amp; alert\n</code></pre>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#personalize-your-bashrc","title":"Personalize your bashrc","text":"<p>First, I advise you to do a backup of your bashrc.</p> <pre><code>cp ~/.bashrc ~/.bashrc_back\n</code></pre>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#personalize-the-prompt","title":"Personalize the prompt","text":"<p>It is a fancy part, but as you will see the prompt on every line you type I advise you to personalize it. To do so, you have to change the PS1 variable. Mine is as follows:</p> <pre><code>PS1=\"\\[\\033[38;5;11m\\]\\u\\[$(tput sgr0)\\]\\[\\033[38;5;15m\\]@\\H:\\[$(tput sgr0)\\]\\[\\033[38;5;32m\\]\\w\\[$(tput sgr0)\\]\\[\\033[38;5;15m\\]\\\\$ \\[$(tput sgr0)\\]\"\n</code></pre> <p>You have to put it under the following line in your <code>~/.bashrc</code>:</p> <pre><code>if [ \"$color_prompt\" = yes ]; then\n</code></pre> <p>You have to replace the default PS1 value by yours.</p> <p>Create the PS1 value can be long as it uses a complex syntax. If you are lazy like me, you can use ezprompt. It is an interactive website to create PS1 value.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#path-modifications","title":"PATH modifications","text":"<p>I like to put some scripts in my home directory (to not mess with my system). So I add a bin folder under my home directory and the following line in my bashrc:</p> <pre><code>PATH=~/bin:$PATH\n</code></pre>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#completion","title":"Completion","text":"<p>As you saw in the previous section, the default completion mode is not fantastic. By default, bash bind the Tab key with the <code>complete</code> command. Fortunately, bash ships with an alternative: menu-complete. menu-complete makes tab cycle through suggestions after listing them. Here is the same example shown in the previous section but using menu-complete:</p> <p>To have this behavior, you have to add the following lines in your .bashrc:</p> <pre><code># tab cycle through commands after listing\nbind '\"\\t\":menu-complete'\n# completion results configuration\nbind \"set show-all-if-ambiguous on\"\nbind \"set completion-ignore-case on\"\nbind \"set menu-complete-display-prefix on\"\n# colored completion, for folders and others\nbind 'set colored-stats on'\n</code></pre> <p>Furthermore, menu-complete allows doing backward cycling (in case you miss the desired output). Unfortunately, I didn't manage to bind the Shift-Tab key combination to this behavior. Instead, I bind this functionality with the \u00b2 key. Which is only handy for AZERTY keyboards (sorry for the others). If anyone wants to help me on this, feel free to contribute I will modify this post with your solution (and I will cite you of course).</p> <pre><code># reverse tab cycle\nbind '\"^[[Z\":menu-complete-backward' # Shift+Tab, but it is not working in Konsole.\nbind '\"\u00b2\":menu-complete-backward' # for azerty keyboard\n</code></pre>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#my-full-configuration-file","title":"My full configuration file","text":"<p>You can find my full bashrc here. It contains more customizations, like gem configuration and miniconda initialization. I tested it under Konsole terminal emulator under Ubuntu 18.04 with two username. Nevertheless, be careful of what you do with it. I do not provide any warranty.</p>","tags":["Development"]},{"location":"blog/2020/01-28-improve-your-bash-navigation/#sources","title":"Sources","text":"<ul> <li>Bash man pages</li> </ul> <p>Hope it helps some of you.</p> <p>See you again, Vincent.</p>","tags":["Development"]},{"location":"blog/2020/01-31-format-a-bootable-usb-key/","title":"Format a bootable USB key","text":"<p>This post is mostly a remainder for me as it struggles me every time \ud83d\ude1c. After making a USB key bootable with any distribution, I get an error message when I want to format it with a graphical interface (tested with gnome-disk, kde partition manager and gparted).</p> <p>This is the steps I have to do to be able to modify this USB key using one of the graphical interfaces I listed.</p>","tags":["Tips"]},{"location":"blog/2020/01-31-format-a-bootable-usb-key/#identify-the-usb-key-name-on-your-system","title":"Identify the USB key name on your system","text":"<p>First type the following command:</p> <pre><code>lsblk\n</code></pre> <p>It gives you an output like this:</p> <pre><code>NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda           8:0    1  28,7G  0 disk\n\u2514\u2500sda1        8:1    1  28,6G  0 part /media/vincent/Kde Neon\nnvme0n1     259:0    0 126,9G  0 disk\n\u251c\u2500nvme0n1p1 259:1    0   512M  0 part /boot/efi\n\u2514\u2500nvme0n1p2 259:2    0 126,4G  0 part /\n</code></pre> <p>Here, my bootable key has the name sda.</p>","tags":["Tips"]},{"location":"blog/2020/01-31-format-a-bootable-usb-key/#fill-the-usb-key-with-zeros","title":"Fill the USB key with zeros","text":"<p>Be careful on this part, it can harm your system if you do it wrong! Now you have to type the following command and replace <code>/dev/sda</code> by <code>/dev/&lt;your usb name&gt;</code>.</p> <pre><code>sudo dd if=/dev/zero of=/dev/sda bs=4k conv=fsync &amp;&amp; alert\n</code></pre> <p>Now you can use your favorite graphical interface to format your key. The <code>&amp;&amp; alert</code> command is not necessary. If you want to understand what it is, I suggest you to look at my post about my bash configuration.</p> <p>Hope it helps some of you, it surely helps me \ud83d\ude04.</p> <p>See you again, Vincent.</p>","tags":["Tips"]},{"location":"blog/2020/03-11-survey/","title":"Survey from Large Speech Corpora to Limited Data","text":"<p>My new survey is out. It reviews automatic techniques used for automatic speech recognition tasks from Large Corpora to Limited Data. It includes an opening to few-shot techniques (to use few data to train the model) and first promising results over speech/audio signals. Check it out.</p> <p>Cheers, Vincent</p>","tags":["Archive","PhD"]},{"location":"blog/2020/03-18-work-at-home/","title":"Configurations for working at home","text":"<p>This post represents all configurations that I have done to work at home. It is especially useful when you are under quarantine (or will be as the COVID-19 progress are alarming). I will describe all steps for Ubuntu users that use KDE environment, but it is similar for GNOME users and some steps are identical.</p>","tags":["Automation","Development","Tips"]},{"location":"blog/2020/03-18-work-at-home/#connect-your-personal-computer-to-the-vpn-of-your-companylab-using-openvpn","title":"Connect your personal computer to the VPN of your company/lab using OpenVPN","text":"<p>It is the first step required if you want to access to the intranet of your company/lab. Mine support OpenVPN, so I will describe the steps required to use it.</p> <p>First, install OpenVPN:</p> <pre><code>sudo apt install openvpn\n</code></pre> <p>Then install the connector for your personal environment.</p> <pre><code># for KDE users\nsudo apt install network-manager-openvpn\n# for GNOME users\nsudo apt install network-manager-openvpn-gnome\n</code></pre> <p>Then you can use the graphical interface of your environment to connect to your desired VPN. For KDE users, go to the Network widget:</p> <p>Then click on the \"Configure network connections...\" icon (surrounded with blue on the previous image). It opens you the following system settings module:</p> <p>To add a VPN connection, click on the \"Add new connection\" icon. Then it opens you this menu:</p> <p>Here click on OpenVPN, then click on \"Create\" and follow the instruction of your company/lab. If your company/lab provide an OpenVPN file it is easier, just click on \"Import VPN connection...\" and select the provided file.</p>","tags":["Automation","Development","Tips"]},{"location":"blog/2020/03-18-work-at-home/#connect-to-distant-servers","title":"Connect to distant servers","text":"<p>I use ssh connections to access to the machines I use at work using my local terminal. To recall this is how you can connect to distant server using ssh:</p> <pre><code>ssh &lt;login&gt;@&lt;server_address&gt; -p &lt;port number&gt;\n</code></pre> <p>If you want to create a ssh server or having more details, I advise you to look at the official Ubuntu documentation.</p> <p>Just after being connected, I always start/attach a tmux session. If you don't know what is tmux, have a look at my post here for more details. To automatize this behavior, I add the following code to my <code>~/.bashrc</code> file on the server side:</p> <pre><code># automatically open tmux when using ssh connection without X server\nif [ ! -z \"$SSH_CLIENT\" ] &amp;&amp; [ -z \"$DESKTOP_SESSION\" -a -z \"$TMUX\" ] ; then\n    tmux attach -t \"ssh\" 2&gt; /dev/null || tmux new -s \"ssh\"\nfi\n</code></pre> <p>Now every time I use ssh to connect to servers from work, it starts or attach to a tmux session called \"ssh\".</p>","tags":["Automation","Development","Tips"]},{"location":"blog/2020/03-18-work-at-home/#working-with-jupyter-lab-notebooks","title":"Working with Jupyter Lab Notebooks","text":"<p>If you use Jupyter Lab, you may want to use your local browser to access to your notebooks. There is many solutions on the Internet to do this. Here is my solution:</p> <ol> <li> <p>Create a password for jupyter notebooks (server side):</p> <pre><code>jupyter notebook password\n</code></pre> </li> <li> <p>Launch jupyter lab using a specified port (here 8887):</p> <pre><code>jupyter lab --port=8887 --no-browser\n</code></pre> </li> <li> <p>On your local terminal forward the 8887 port of the server to, let say the 8888 port of your local machine using ssh tunneling:</p> <pre><code>ssh -N -f -L 8888:localhost:8887 &lt;login&gt;@&lt;server_address&gt;\n</code></pre> </li> <li> <p>Use your local browser and go to localhost:8888/ and enter the password entered in step 1.</p> </li> <li> <p>Enjoy</p> </li> </ol> <p>When you have finished, you maybe want to unset the forwarding port on your local machine. To do so, identify the PID of the command used in step 3 using the following command:</p> <pre><code>ps -ax | grep \"ssh -N -f -L\"\n</code></pre> <p>Once you identify the PID use the kill command to stop the forwarding port:</p> <pre><code>kill -9 &lt;PID_identified&gt;\n</code></pre> <p>Hope it helps some of you.</p> <p>Cheers, Vincent</p>","tags":["Automation","Development","Tips"]},{"location":"blog/2020/03-27-audio-loader/","title":"Audio loader","text":"<p>Note</p> <p>This project began as a side endeavor alongside my thesis. However, with the emergence of <code>torchaudio</code> and <code>speechbrain</code>, I no longer saw the need to continue it.</p> <p>Today, I released an early version of the library I am working on: It is called Audio Loader and it loads audio batches (with features and ground truth) for deep learning libraries such as PyTorch or TensorFlow.</p> <p>For now, it is only an early work and have basic mechanisms implemented such as:</p> <ul> <li>PyTorch bindings (basic one that fill all RAM and designed for windowed sampler).</li> <li>Ground truth generic loader, with TIMIT example and C2SI example.</li> <li>Generic samplers (loading using windows or use the full files).</li> <li>Basic features using librosa framework (only raw audio and MFCC for now).</li> <li>Basic activity detection system to filter features.</li> <li>Most features have a corresponding testing module using pytest.</li> </ul> <p>My TODO list for the future (it is not an exhaustive one):</p> <ol> <li>Adding a documentation website.</li> <li>Add other features (spectrograms and so on)</li> <li>Have an elegant system of cache as an alternative to fill everything into the RAM.</li> <li>TensorFlow bindings</li> </ol> <p>For more details look into the GitHub repository. If you want to contribute, feel free to contact me or propose your pull request \ud83d\ude04.</p> <p>Hope it helps some of you.</p> <p>Cheers, Vincent</p>","tags":["Archive","Development","AI"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/","title":"Tweaks to speedup Deep Learning on servers","text":"<p>Learning a Deep Learning model can take a very long time to learn. Indeed, it is possible to take a month to learn a state-of-the-art model (depending on your hardware available). Lately, I searched how to improve speed time of some Deep Learning algorithms as even 1% improvement can lead a great deal. In this post, I will share configurations I made on my systems. Ubuntu-based systems (mostly derivatives). These configurations save me time and here they are.</p>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#mount-data-disk-with-noatime-option","title":"Mount data disk with noatime option","text":"<p>This trick is the most important I have found. By default, Linux file systems use the atime option. It consists in writing the last access time in every read files. This induces high IO disk usage when learning on large datasets with many files. In my experiments, I gain roughly 5% of computing time on a training set of around 30000 audio files (with each audio being less than 10s). More files will yield to improve this gain. Remenber that having a High-performance data management like hdf5 is a good alternative to this solution (and may yield to greater performances).</p> <p>In the next subsections, I will explain how to set the option noatime to a disk to disable the atime behavior.</p>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#identify-the-disk-id","title":"Identify the disk id","text":"<p>The id of a disk is called UUID. The best way I know to identify a disk UUID is to type the following command:</p> <pre><code>sudo lsblk -fm\n</code></pre>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#check-if-the-disk-is-already-mounted","title":"Check if the disk is already mounted","text":"<p>Now we have identified the UUID, we need an unmounted disk. To check this, just type the following command:</p> <pre><code>df -h\n</code></pre> <p>To unmount mounted disk, you have to type the following command:</p> <pre><code>umount &lt;mounted point&gt;\n</code></pre> <p>Note</p> <p>If you want to apply the noatime on your system disk you still can modify your <code>/etc/fstab</code> file and it will be applied after a restart.</p>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#create-mount-point","title":"Create mount point","text":"<p>Before mounting your data disk, you have to create the folder from which you will access to your data disk (or already use an existing one). To do so, just type:</p> <pre><code>mkdir &lt;your access destination&gt;\n</code></pre>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#create-a-fstab-entry-for-the-disk","title":"Create a fstab entry for the disk","text":"<p>Now we have the UUID of your data disk (which is unmounted) and a mount point, we will edit the <code>/etc/fstab</code> file. Doing so, it will mount automatically your data disk after each restart. To do so, add the following line in the <code>/etc/fstab</code> file:</p> <pre><code>UUID=&lt;UUID-identified&gt; &lt;absolute_path_mount_point&gt; ext4 errors=remount-ro,noatime  0 0\n</code></pre> <p>Note</p> <p>The trick here is to use the noatime option. Add it on system partitions (by modifying lines in the <code>/etc/fstab</code> file).</p>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#mount-the-disk","title":"Mount the disk","text":"<p>As we configured the <code>fstab</code> file, the system will automatically mount the disk (with the <code>noatime</code> option) after each reboot. If you don't want to restart your machine, you can mount the partition for your data as follows:</p> <pre><code>sudo mount &lt;absolute_path_mount_point&gt;\n</code></pre>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#have-the-latest-driver","title":"Have the latest driver","text":"<p>Another trick I use is to have an up-to-date graphic driver. Hence, I can benefit from the last optimizations. Since July 2019, Ubuntu offers the latest Nvidia drivers to LTS users.</p> <p>To see available drivers, you need to type the following command:</p> <pre><code>ubuntu-drivers devices\n</code></pre> <p>Then you choose the last driver (let's say the 560) and use <code>apt</code> to install it:</p> <pre><code>sudo apt install nvidia-driver-560\n</code></pre>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#follow-the-recommendations-of-your-deep-learning-library","title":"Follow the recommendations of your Deep Learning library","text":"<p>There are numerous techniques to optimize the training and inference time of your models. While some techniques are specific to each library, many are common to both PyTorch and TensorFlow.</p> <p>Among the most useful in these two frameworks are the use of mixed precision and model compilation to significantly improve training speed. Mixed precision combines single precision (32-bit) and half precision (16-bit) computations, which speeds up training while reducing memory usage, without sacrificing result accuracy. Compilation, on the other hand, optimizes graph computation to make it more efficient. This allows for faster execution on CPUs and GPUs, by minimizing redundant calculations and maximizing parallelism.</p> <p>To go further, you can explore quantization (using fewer bits to encode parameters) and model pruning (removing parameters while maintaining performance).</p> <p>You can find more details on implementing these techniques here for TensorFlow and here for PyTorch.</p>","tags":["Development"]},{"location":"blog/2020/06-24-tweaks-to-speedup-AI-servers/#improve-compilation-time","title":"Improve compilation time","text":"<p>To improve compilation time, you can use your RAM instead of your ROM (useful if you do not have an SSD). To do so, you have to add the following line to your <code>/etc/fstab</code> file:</p> <pre><code>none    /tmp/    tmpfs    noatime,size=10%    0    0\n</code></pre> <p>Note</p> <p>This limits the <code>/tmp</code> partition size (by 10% of the max RAM) and can block large usage of the <code>/tmp</code> cache (such as building singularity images). Don't forget to change the cache to use for these cases (or increase the size of <code>/tmp</code> in the line above depending on your RAM available).</p> <p>Hope it helps some of you. If you have advices or more trick don't hesitate to share your knowledge \ud83d\ude04.</p> <p>Cheers, Vincent</p>","tags":["Development"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/","title":"Automation using ssh key","text":"<p>I started my blog with a post telling you how to let Git remember your credentials for websites not supporting ssh keys (the post is here for those interested). In that day, I was struggling with Overleaf Git servers that do not support ssh keys (and still does not at the moment I am writing). It helped me for this case. Nevertheless, it is only compatible with Git servers and is not the best way to automatize your identifications (at least in my point of view). Today we will see how to use ssh keys to automatize many login steps.</p> <p>In this post we will see two use cases:</p> <ul> <li>Automatic identification to servers using the ssh protocol.</li> <li>Automatic identification when doing push/pull commands on Git servers (such as GitHub or GitLab).</li> </ul> <p>SSH keys contain a public key to encode messages (destined for servers) and a private key to be able to read those messages (destined for the client of the servers). If you want more information of the protocol, have a look here.</p> <p>The distribution used (and tested) for this tutorial is Kubuntu 20.04 LTS (my new main distribution, but this story is for a future post).</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#prepare-the-client-side","title":"Prepare the client side","text":"<p>In this section we will prepare the client that will do the identifications to the servers. We will first create a pair of public and private keys. Afterwards, we will start the ssh-agent to configure it with the private key.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#check-if-you-already-have-a-pair-of-ssh-keys","title":"Check if you already have a pair of SSH keys","text":"<p>Check the directory listing of <code>~/.ssh/</code> to see if you already have a public SSH key. By default, the filename of the public key is <code>id_rsa.pub</code>. You can use a different pair of keys per server if you want. I prefer to keep one pair of keys per machine (and change it regularly).</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#generate-a-pair-of-ssh-keys","title":"Generate a pair of SSH keys","text":"<p>To generate a pair of keys linked to an email address (to better identify the connected user) you have to type the following line:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <p>Then, follow the instructions. You can let the default key path if you do not have a default pair of keys. You will also set a password to unlock your private key, be sure to remember it. Be careful to not revel your private key (the <code>id_rsa</code> file) in any case.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#start-ssh-agent-and-add-your-private-key","title":"Start ssh-agent and add your private key","text":"<p>To let your system remember your private key for your session you can use the ssh-agent as follows:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>Now we have a pair of private and public key and the ssh-agent configured with it. We are ready to automatize our SSH identifications to servers or Git servers.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#automatize-your-login-on-servers-using-ssh","title":"Automatize your login on servers using SSH","text":"<p>To automatize SSH password typing, you can add the public key to the authorized keys of your servers. Before that, make sure that your remote user space contains a <code>~/ssh</code> folder:</p> <pre><code>ssh &lt;login&gt;@&lt;server_adress&gt; mkdir -p .ssh\n</code></pre> <p>Then you have to add your public key into the authorized keys file of your server:</p> <pre><code>cat ~/.ssh/id_rsa.pub | ssh &lt;login&gt;@&lt;server_adress&gt; 'cat &gt;&gt; .ssh/authorized_keys'\n</code></pre> <p>Now your server is configured with your ssh pair of keys. You can repeat those two steps for each server you can access.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#automatize-your-authentifications-on-git-servers","title":"Automatize your authentifications on Git servers","text":"<p>To automatize your identification (typing your login and password) after using a <code>pull</code> or <code>push</code> command, you can add your public key to your Git server (via their web interface). To copy your public key on a website (such as GitHub or GitLab) you may want to add your key to the clipboard (to use Ctrl+V inside your web browser). For this purpose, you need to install <code>xclip</code>:</p> <pre><code>sudo apt install xclip\n</code></pre> <p>or if you are on manjaro:</p> <pre><code>sudo pamac install xclip\n</code></pre> <p>Then, it is as simple as this:</p> <pre><code>xclip -sel clip &lt; ~/.ssh/id_rsa.pub\n</code></pre> <p>After, follow the steps from this link for GitHub and that link for GitLab.</p> <p>Now your configuration is ready for your Git server. The next subsection will be about testing this configuration without modifying your repositories.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#test-your-configuration","title":"Test your configuration","text":"<p>To test your new configuration on your Git server, you can use the ssh protocol. It will avoid doing modifications to one of your Git repository. The next subsections show you how to do it for different Git servers.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#github","title":"GitHub","text":"<pre><code>ssh -T git@github.com\n</code></pre>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#gitlab","title":"GitLab","text":"<pre><code>ssh -T git@gitlab.com\n</code></pre>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#keeping-ssh-agent-identities-on-kubuntu-2204-after-restart","title":"Keeping ssh-agent identities on Kubuntu 22.04 after restart","text":"<p>With the above instructions, you have to (in Kubuntu at least) reconfigure the ssh-agent after each logout or restart. In this section, we will use kwallet to bypass this limitation.</p> <p>First, we have to install the <code>ssh-askpass</code> package:</p> <pre><code>sudo apt install ssh-askpass\n</code></pre> <p>Then, we have to create a script that will automatically unlock your private key when logged in. To achieve this, type the following lines:</p> <pre><code>mkdir -p ~/.config/autostart-scripts\necho '#!/bin/sh' &gt; ~/.config/autostart-scripts/ssh-add.sh\necho 'export SSH_ASKPASS=/usr/bin/ksshaskpass' &gt;&gt; ~/.config/autostart-scripts/ssh-add.sh\necho 'ssh-add &lt; /dev/null' &gt;&gt; ~/.config/autostart-scripts/ssh-add.sh\nchmod +x ~/.config/autostart-scripts/ssh-add.sh\n</code></pre> <p>For the next step, type the following command and check the remember checkbox:</p> <pre><code>~/.config/autostart-scripts/ssh-add.sh\n</code></pre> <p>It will let the KDE wallet retain the password for your private key and unlocks it after each login.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#keeping-ssh-agent-identities-on-manjaro-after-a-reboot","title":"Keeping ssh-agent identities on Manjaro after a reboot","text":"<p>After my switch to Manjaro linux (see my post about my migration), I realized that the method for Kubuntu didn't work (and might not work on next LTS). Here is my solution, which is a combination of two approaches that you can find in my sources.</p> <p>First of all, make sure you have the necessary tools:</p> <pre><code>sudo pamac install kwallet ksshaskpass kwalletmanager\n</code></pre> <p>Next, let's configure our system and zsh to use the appropriate socket for the ssh agent:</p> <pre><code>sudo echo '#!/bin/sh' &gt; /etc/profile.d/ssh-askpass.sh\nsudo echo 'export SSH_ASKPASS=/usr/bin/ksshaskpass' &gt;&gt; /etc/profile.d/ssh-askpass.sh\necho 'export SSH_AUTH_SOCK=\"$XDG_RUNTIME_DIR\"/ssh-agent.socket' &gt;&gt; ~/.zshrc\n</code></pre> <p>Next, we create the user directory for systemd:</p> <pre><code>mkdir -p ~/.config/systemd/user\n</code></pre> <p>Next, create the file <code>~/.config/systemd/user/ssh-agent.service</code> and fill it with the following content:</p> <pre><code>[Unit]\nDescription=SSH agent (ssh-agent)\n\n[Service]\nType=simple\nEnvironment=SSH_AUTH_SOCK=%t/ssh-agent.socket\nEnvironment=DISPLAY=:0\nEnvironment=KEY_FILE=/home/%u/.ssh/id_rsa\nExecStart=ssh-agent -D -a $SSH_AUTH_SOCK\nExecStartPost=/bin/sleep 3\nExecStartPost=/usr/bin/ssh-add $KEY_FILE\nExecStop=kill -15 $MAINPID\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>This service starts the ssh agent at each login on your machine and adds the private key you created at the beginning of this article. We will now enable it and run it:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user enable ssh-agent.service\n</code></pre> <p>Now you can restart your machine and everything should work \ud83d\ude04. I hope this was helpful \ud83d\ude09.</p>","tags":["Automation"]},{"location":"blog/2020/08-02-automation-using-ssh-keys/#sources-and-inspirations","title":"Sources and inspirations","text":"<ul> <li>GitHub Official instructions for SSH keys</li> <li>GitLab Official instructions for SSH keys</li> <li>Configure ssh-agent on Ubuntu</li> <li>Kubuntu and ssh-agent</li> <li>Manjaro and ssh-agent (1/2)</li> <li>Manjaro and ssh-agent (2/2)</li> </ul> <p>Hope it helps some of you.</p> <p>Cheers, Vincent</p>","tags":["Automation"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/","title":"Create a documentation with Sphinx on GitHub","text":"<p>Note</p> <p>I now use <code>mkdocs</code> for all my documentation projects, including this website.</p> <p>Previously I announced a preview of the Audio Loader library. It prepares audio batches for Neural Network libraries (have a look here). For this library I planned to add a documentation website. A preliminary version is now available here. As Audio Loader is a Python library, I chose Sphinx to generate the documentation as it is a classic with Python libraries. It uses the reStructuredText file format to create the documentation. Nevertheless, I prefer the markdown syntax as it is simpler and I use it more often. Hence, I use as much markdown files as I can to create Sphinx documentation. Next, to host this documentation I chose GitHub Pages as I already used it on my website.</p> <p>In this post, I will explain how to create a documentation using Sphinx and how to use as much as possible markdown files. Finally, I will explain how to set up your repository on GitHub to host your documentation.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#create-your-first-documentation","title":"Create your first documentation","text":"<p>In this section, we will see how to create a documentation using sphinx.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#install-sphinx","title":"Install Sphinx","text":"<p>First let us install sphinx:</p> <pre><code>pip install sphinx\n</code></pre> <p>Now we have everything we need to create our documentation \ud83d\ude04.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#create-the-initial-files","title":"Create the initial files","text":"<p>Sphinx organize the documentation in three things:</p> <ul> <li>makefiles to generate the documentation</li> <li>source pages that contain the documentation instruction</li> <li>the resulting documentation (pdf file, html webpages and so on)</li> </ul> <p>Now let's create this organization using <code>quickstart</code> tool from sphinx:</p> <pre><code>mkdir docs\ncd docs\nsphinx-quickstart\n</code></pre> <p>In the quickstart instructions, choose separate source folder and build folder (as it will be more convenient for next steps). Fill the rest as you please.</p> <p>Afterwards, sphinx created two folders (<code>docs/sources</code> and <code>docs/build</code>) and a makefile (<code>docs/Makefile</code>). Sphinx source files are in <code>docs/sources</code> and the <code>index.rst</code> file represents the first page of your documentation (the equivalent of the <code>index.html</code> of a website). To compile your documentation into a website or a pdf file you can use the makefile (using <code>make html</code> or <code>make pdf</code>). The resulted documentation format will be putted into <code>docs/build</code>.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#includelink-files-into-a-rst-file","title":"Include/link files into a rst file","text":"<p>Here I will not detail how the rst format works as I avoid it as much as I can. Instead, I will show you how to add documents or link documents into a rst file. If you want to learn how the rst format works, go there.</p> <p>I prefer to avoid using rst files as much as I can as it is possible to use markdown for most of the time. However, in cases where rst files are needed (such as <code>index.rst</code>) it is useful to know how to include or link files. Therefore this is what I will explain in this section.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#link-a-rst-file-into-a-doctree","title":"Link a rst file into a doctree","text":"<p>To add a link to a file (say <code>extra_document.rst</code>) you need to add <code>extra_document</code> to the doctrine as follows:</p> <pre><code>.. toctree::\n    :maxdepth: 2\n\n    extra_document\n</code></pre> <p>Warning:</p> <p>The <code>extra_document</code> indentation must be like the indentation of the <code>:maxdepth: 2</code> line. If it is not the case, you will have a warning telling you sphinx can't find your file (took me hours to figure it out).</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#include-a-rst-file-into-a-rst-file","title":"Include a rst file into a rst file:","text":"<p>To add a <code>rst</code> file inline of a <code>rst</code> file, you just need to add the following line:</p> <pre><code>.. include:: my_file.rst\n</code></pre> <p>The document that contains this line will be filled with the <code>my_file.rst</code> content.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#write-sphinx-documentation-using-markdown-and-autodoc","title":"Write sphinx documentation using markdown and autodoc","text":"","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#configure-sphinx-with-markdown","title":"Configure sphinx with markdown","text":"<p>To add markdown support for sphinx I use <code>m2r</code> instead of <code>recommonmark</code> (advised in the official documentation of sphinx). The reason is simple: <code>recommonmark</code> do not support <code>mdinclude</code> to include markdown documents into <code>rst</code> files. Now let's install <code>m2r</code>:</p> <pre><code>pip install m2r\n</code></pre> <p>Then let's edit the <code>source/config.py</code> file to add this line:</p> <pre><code>extensions.append(\"m2r\")\n</code></pre> <p>Now you can use markdown files in your documentation. You can add it in the doctree (same as for rst files) or include markdown files into rst files using <code>mdinclude</code>. One little example with <code>mdinclude</code>:</p> <pre><code>.. mdinclude:: my_file.md\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#autodoc","title":"Autodoc","text":"<p>While creating your documentation, it is nice to use the docstring of your python source files to fill your content. Here I will explain how you can do it using the autodoc extension.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#install","title":"Install","text":"<p>To enable autodoc you have to add the extension in the <code>source/config.py</code> file:</p> <pre><code>extensions.append('sphinx.ext.autodoc')\n</code></pre> <p>Also add the following lines to the beginning of the <code>source/config.py</code> file:</p> <pre><code>import os\nimport sys\nsys.path.insert(0, os.path.abspath('../..'))\n</code></pre> <p>To use the numpy format in the docstring you also have to add the napoleon extension:</p> <pre><code>extensions.append('sphinx.ext.napoleon')\n</code></pre> <p>Now we are ready to use autodoc.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#show-elements-of-a-docstring","title":"Show elements of a docstring","text":"<p>All the following commands works in rst files (I still search a simple way to use it in markdown files, don't hesite to share in comments \ud83d\ude04). For more details follow the official documentation.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#use-the-docstring-of-the-module","title":"Use the docstring of the module","text":"<pre><code>.. automodule:: project_folder.module\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#use-all-the-docstrings-of-a-module","title":"Use all the docstrings of a module","text":"<pre><code>.. automodule:: project_folder.module\n    :members:\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#use-the-docstring-of-a-specific-class","title":"Use the docstring of a specific class","text":"<pre><code>.. autoclass:: project_folder.module.class\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#sphinx-theming","title":"Sphinx theming","text":"<p>Now let's be funky \ud83d\ude04. We will use themes in there: showcase of themes.</p> <p>On this website, choose your desired documentation style, then click on the pypi hyperlink to install the theme using the <code>pip</code> command. To apply your theme click on the associated <code>conf.py</code> hyperlink and look for the <code>html_theme</code> variable. Then copy the corresponding code to modify your <code>html_theme</code> variable in your <code>source/config.py</code> file.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#host-your-documentation-on-github","title":"Host your documentation on GitHub","text":"<p>Now we know how to create a documentation, it will be nice to upload it into a server. Here I will explain how I do it for GitHub servers.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#prepare-the-structure-of-the-projects","title":"Prepare the structure of the projects","text":"<p>For my projects, I think the cleanest structure is as follows:</p> <pre><code>my_project\n|-  my_project_code      --&gt; folder containing your source codes and documentation sources\n|-  my_project_gh_pages  --&gt; folder containing your built website\n|   |-  html             --&gt; folder containing your documentation (synced with your gh_pages branch)\n</code></pre> <p>Now let us create this structure. First, let's create a directory for your project:</p> <pre><code>mkdir my_project\ncd my_project\n</code></pre> <p>Now let's create <code>my_project/my_project_code</code> and <code>my_project/my_project_gh_pages</code> folders:</p> <pre><code>mkdir my_project_gh_pages\ngit clone https://github.com/username/my_project\nmv my_project my_project_code\n</code></pre> <p>Now let's create and prepare the <code>html</code> folder:</p> <pre><code>git clone https://github.com/username/my_project\nmv my_project html\ncd html\ngit branch gh-pages\n\ngit symbolic-ref HEAD refs/heads/gh-pages  # auto-switches branches to gh-pages\n# remove all files and git indexes for the branch\nrm .git/index\ngit clean -fdx\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#modify-sphinx-makefile","title":"Modify Sphinx makefile","text":"<p>Now our makefile require modifications to use our gh-pages branch and <code>my_project_gh_pages</code> folder.</p> <p>Edit your <code>my_project/my_project_code/docs/Makefile</code> file such as your <code>BUILDDIR</code> variable is such as the following:</p> <pre><code>BUILDDIR      = ../../my_project_gh_pages/\n</code></pre> <p>Now you can generate your documentation:</p> <pre><code>make html\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#build-html-files-for-github","title":"Build html files for GitHub","text":"<p>Your documentation is now under <code>my_project/my_project_gh_pages/html</code>. Now <code>commit</code> and <code>push</code> your documentation such as:</p> <pre><code>cd my_project_gh_pages/\ngit add html\ngit commit -m \"First documentation commit\"\ngit push --set-upstream origin gh-pages\n</code></pre> <p>It is now available under <code>https://username.github.io/my_project</code>, unless you have made a redirection for your GitHub Pages (like for this blog).</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#force-no-jekyll","title":"Force no jekyll","text":"<p>GitHub server use jekyll which provoque errors while interpreting html files generated by sphinx. Hopefully, we can disable jekyll. Inside the <code>my_project/my_project_gh_pages/html</code> folder type the following commands:</p> <pre><code>touch .nojekyll\ngit add .nojekyll\ngit commit -m \"added .nojekyll\"\ngit push origin gh-pages\n</code></pre>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#build-and-commit-automatically","title":"Build and commit automatically","text":"<p>You can add a command inside your Makefile:</p> <pre><code>    pushhtml: html\n\n    cd $(BUILDDIR)/html; git add . ; git commit -m \"rebuilt docs\"; git push origin gh-pages\n</code></pre> <p>Now by tipping <code>make pushhtml</code> you will build your documentation, commit it and push it on GitHub automatically. It comes alongside the <code>make html</code> command that I use to test some modifications without pushing it.</p>","tags":["Archive","Development"]},{"location":"blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#sourcesinspirations","title":"Sources/inspirations","text":"<ul> <li> <p>Official sphinx Documentation</p> </li> <li> <p>Getting started with sphinx</p> </li> <li> <p>Getting started with autodoc</p> </li> <li> <p>Publishing sphinx documentation on GitHub</p> </li> </ul> <p>Hope it helps some of you.</p> <p>Cheers, Vincent</p>","tags":["Archive","Development"]},{"location":"blog/2021/09-12-badaboom-asteroids/","title":"BadaBoom - Asteroids","text":"<p>In this new blog post, I'm going to present you a little project I've been doing lately. Being a space enthusiast, I was looking for a simple database that would allow me to better understand the immensity of the world around us. I finally found the 'Asteroids - NeoWs' database provided by NASA (for more information, look here). This database is focused on asteroids passing near the planet Earth. In this database, the closest point of an asteroid passing near the Earth represents an event.</p> <p>So I made a repository named badaboom. This repository contains a parser (data extractor) and a program allowing to make some figures and statistics. In this blog post, I will detail the first results I obtained (and will update this post according to my progress).</p> <p>Note</p> <p>You can recreate all the plots with updated information. To do so, you can check out the project on GitHub at badaboom. I also use this repository as a template for my Python projects. Feel free to take a look and share any suggestions for improvement!</p>","tags":["Archive","Visualizations"]},{"location":"blog/2021/09-12-badaboom-asteroids/#results","title":"Results","text":"<p>All the results that follow are calculated for the years between 1980 and 2030. Knowing that the data was retrieved on September 10, 2021, all data from that date to the end of 2030 are NASA's projections.</p> <p>Before starting, here are the 10 largest asteroids for the years between 1980 and 2030:</p> Asteroid identifier Asteroid name Estimated minimum diameter (km) Estimated maximum diameter (km) Absolute magnitude h Asteroid information link 2001036 1036 Ganymed (A924 UB) 37.5452 83.9537 9.25 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001036 2000433 433 Eros (A898 PA) 21.8049 48.7573 10.43 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2000433 2001866 1866 Sisyphus (1972 XA) 8.64082 19.3215 12.44 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001866 2004954 4954 Eric (1990 SQ) 8.06408 18.0318 12.59 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2004954 2001627 1627 Ivar (1929 SH) 7.7724 17.3796 12.67 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001627 2003552 3552 Don Quixote (1983 SA) 6.80072 15.2069 12.96 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2003552 2002212 2212 Hephaistos (1978 SB) 5.73528 12.8245 13.33 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2002212 2025916 25916 (2001 CP44) 4.88152 10.9154 13.68 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2025916 2001980 1980 Tezcatlipoca (1950 LA) 4.61907 10.3286 13.8 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001980 2005587 5587 (1990 SB) 4.57673 10.2339 13.82 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2005587 <p>Thus, we realize that gigantic asteroids pass over our heads without us even realizing it! Some of you will notice that some of the asteroid names are familiar (especially if you watch series like The Expanse).</p> <p>Now let's take a look at the number of asteroids visiting us per month (the figure is interactive so feel free to move your mouse over the bars):</p> Bokeh Plot <p>By seeing this graph we can draw the following points:</p> <ul> <li>The more the years pass and the more asteroids are observed (probably linked to an improvement of our observatories, even if more asteroids can cross us these last years).</li> <li>The number of predicted asteroid passes is lower than the last 5 years. This is perhaps due to the fact that the efforts are put on the largest asteroids (see the following figure which can suggest this).</li> <li>Asteroids do not usually make more than one pass per year.</li> </ul> <p>Now let's go into more detail with the following figure (also interactive):</p> Bokeh Plot <p>The asteroid categories are completely arbitrary. I can visualize these different sizes with these categories. We notice that we detect more and more asteroids whose size is less than 500m. While when we look at the forecasts we see that the number expected is much less. Probably because the largest asteroids are a priority for the surveillance around our planet.</p> <p>In addition to this graph, I calculated the following numbers:</p> <ul> <li>Single asteroid encountered and observed from 1980 to 2021-09-10: 25810</li> <li>Unique asteroid whose encounter is planned between 2021-09-10 and 2030: 11211</li> </ul> <p>There are still quite a few visits planned, hoping that the biggest ones do not come into contact (for the most anxious, it is not on the menu).</p> <p>That's it for today, I made this project for my curiosity and I thought I could share it with you.</p> <p>I hope this helps/inspires some of you. In any case, I hope it was interesting.</p> <p>See you again, Vincent.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2021/09-19-hackaviz/","title":"Hackaviz 2021","text":"<p>In this new blog post, I will present you my participation to the Hackaviz 2021. The Hackaviz is a contest organized by the data visualization association called Toulouse dataviz. It consists in telling a story using data from Toulouse.</p> <p>I have fun making data visualizations in my free time, so I participated solo. The code I used is available here. In this blog post, I will reveal the story I wanted to tell through my work.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2021/09-19-hackaviz/#results","title":"Results","text":"<p>The Occitanie region is a sunny region with its capital city being Toulouse. It is therefore expected to be the most dynamic city in terms of land transactions between the years 2016 and 2020.</p> <p>Let's see this through my realization:</p> Bokeh Plot <p>Being an interactive figure feel free to move your mouse over the colored regions to get more details (the number of transactions per type and the average value of these transactions).</p> <p>Thus, Toulouse is not the city (among the cities for which data is available) with the most land transactions. Montpellier (capital of the former Languedoc-Roussillon region) and N\u00eemes are slightly ahead (2332 for Toulouse vs. 2481 for Montpellier vs. 2729 for N\u00eemes). While B\u00e9ziers with its 4251 transactions is largely ahead of the other cities. Looking at this graph in more detail, we can see that, with the exception of Toulouse, the most dynamic cities are those close to the Mediterranean Sea. The average value of transactions in this area is generally higher than in the less dynamic cities.</p> <p>Thus, the Mediterranean basin is an attractive area for real estate activities.</p> <p>I hope this will help/inspire some of you. In any case, I hope it has been interesting.</p> <p>See you again, Vincent.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2021/12-08-badaboom-fireballs/","title":"Badaboom - Fireballs","text":"<p>In this new blog post, I'm going to present you the continuation of my work on my library badaboom). This article follows my previous article about my badaboom project (you can find this article here). For this second part, I realized the analysis of fireballs. Fireballs are astronomical terms for meteors that are exceptionally bright and spectacular enough to be seen over a wide area.</p> <p>For this work, I used the CNEOS database (owned by NASA). For this article, I will start by presenting you the elements present in this database and finish by showing you the first results I obtained (I will update this post as I progress).</p> <p>Note</p> <p>You can recreate all the plots with updated information. To do so, you can check out the project on GitHub at badaboom. I also use this repository as a template for my Python projects. Feel free to take a look and share any suggestions for improvement!</p>","tags":["Archive","Visualizations"]},{"location":"blog/2021/12-08-badaboom-fireballs/#results","title":"Results","text":"<p>Since this database lists fireball impacts, I first turned to the light emission energies and the impact energies. Note that I put these energies on a logarithmic scale, because the scale of values is so varied that I found it simpler and more readable to do so.</p> Bokeh Plot Bokeh Plot <p>We can see that most of the fireballs recorded are in the same order of magnitude with some exceptions that can emit up to 375000 GJ of light energy (with an impact energy of 440kt). These values are difficult to apprehend without a reference and let me dream about the effect that we must feel if we have the chance to observe in real such a phenomenon (while being at a distance allowing to be safe of course \ud83d\ude1c).</p> <p>Before showing you the map of the impacts recorded by the CNEOS, it should be noted that on their database there are missing locations for some fireballs. That's why I made the following histogram:</p> Bokeh Plot <p>Nevertheless, it can be noted that there are less and less detections made without localization. It is therefore from the data with a geolocation that I made the following map. This map is interactive, by passing the mouse over the bubbles you can see the details for each impact. You can also zoom on the map to see more precisely some regions of the world. The size of the bubbles corresponds to the impact energy of the fireball.</p> <p>Note</p> <p>The map below lost its background because the previous API is no longer available. I\u2019ve updated the API in badaboom by switching to Plotly with Mapbox, replacing the Bokeh-based solution. You can recreate the map yourself. I haven't hosted the result because the API incurs costs after a certain number of views.</p> Bokeh Plot <p>As a reminder, all the code needed to make these figures is available on my library badaboom).</p> <p>I hope this will help/inspire some of you. In any case, I hope it was interesting.</p> <p>See you again, Vincent.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2022/04-03-hackaviz/","title":"Hackaviz 2022","text":"<p>In this blog post, I will present you my participation to the Hackaviz 2022. The Hackaviz is a contest organized by the data visualization association called Toulouse dataviz. It consists in telling a story using data made available.</p> <p>I have fun making data visualizations in my free time, so I participated solo. You can find my participation to the previous year here. The code I used will be available soon.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2022/04-03-hackaviz/#results","title":"Results","text":"<p>This year, a database of funding collected and distributed by the collective management organizations of the author's rights for private copy is in the spotlight.</p> <p>I have devoted my work to the type of aid granted, all organizations included. So I made a first graph (it is interactive, you can use your mouse or finger) showing the evolution of the number of grants by type:</p> Bokeh Plot <p>Here we notice the effect of the covid because in 2020 there was a decrease in the number of grants awarded (which were increasing for all types every year until then). With live performances suffering the most, this is certainly related to the confinements. Following this first observation, I wanted to observe the distribution of the number of grants with the following graph (also interactive):</p> Bokeh Plot <p>Thus with this graph we realize that the distribution of funding has remained stable from 2016 to 2019 (regardless of the increase in the number of grants allocated). Nevertheless, for the year 2020 this distribution has changed to the disadvantage of live performances and in favor of funding for creations. While the proportion of funding for artistic and cultural education and artist training has remained stable.</p> <p>I hope this will help/inspire some of you. In any case, I hope it has been interesting.</p> <p>See you again, Vincent.</p>","tags":["Archive","Visualizations"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/","title":"My switch to Manjaro Linux","text":"<p>I do not recommend this distribution</p> <p>After two years of use, I no longer recommend Manjaro due to recurring security and stability concerns. Users often face issues with Arch User Repository (AUR) packages, where updates can break your system. Additionally, there are persistent problems with SSL certificates, which can affect package management and secure connections. For more details, you can check out this video.</p> <p>I am leaving this post as an archive because it offers solutions to some of the common issues that Manjaro users may encounter.</p> <p>I started my KDE adventure with Fedora (for 6 months) where the graphical interface was buggier than KDE Neon where I stayed for a bout a year and finished into Kubuntu LTS with flatpaks apps. It has been three years I was using Kubuntu, while I enjoy some of its aspects (LTS, based on Ubuntu packages, easy to configure and with many resources) my day-to-day experience was inconvenient. Every time I updated my NVIDIA drivers I feared to get unusable GUI (totally black screen with only tty terminals available) because even if all required packages for the NVIDIA driver are not in the repositories, the installation is done by apt. Even worse, those missing files can take 1 week to get in the repositories. Less game changer, but also annoying, snap is forced via apt in Ubuntu distribution (like <code>apt install firefox</code> or <code>apt install chromium</code> results in a snap install). Finally, having a really outdated KDE environment is not ideal if you want to benefit from last bug fixes of the environment (their team is making good effort to address many bugs and to improve the user experience, you can look into Nate\u2019s blog to have a preview of their efforts).</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#my-requirements","title":"My requirements","text":"<p>Before telling you why I choose Manjaro KDE, I will expose you my needs:</p> <ul> <li>a stable distribution that has the necessary applications for all my needs (I can use flatpaks for missing apps), don\u2019t break when I update my drivers and with a minimum of manual maintenance required,</li> <li>an easy way to install my system,</li> <li>a large community with documentation and wikis,</li> <li>I don\u2019t want to reinstall my system each 6 months (it was why I was on Kubuntu LTS),</li> <li>a KDE environment up-to-date, but no bleeding-edge without breaks and bugs.</li> </ul> <p>Given these requirements, both Manjaro and openSUSE seem great solutions. I already used Arch/Manjaro documentation and wikis to solve some of my Kubuntu problems (it\u2019s an irony as there was nothing on Ubuntu/Kubuntu forums). Moreover, in 2021 on distrowatch Manjaro seems to be a more popular solution (so maybe more people to help in case of problems).</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#experience","title":"Experience","text":"<p>As no distribution is perfect, there are great things to take and bad ones to overcome. First the installation process, it is not the best UI out there, but I had no problems or difficulties, everything was smooth. Then comes the first software to install, combining the official packages with flatpak and AUR, I did not install a single package manually. It is a first for me, the software manager look for updates of all my applications (a very handy thing).</p> <p>Then comes the first troubles to finalize my setup, which I solved with a some research on Arch and Manjaro forums/wikis (I listed them on the last part of this post for anyone curious). Finally, comes the routines and some changes in habits.</p> <p>Overall, it has been a great experience and I did not regret my switch. I even installed Manjaro on all my computers. It is surely not a beginner distribution (as you have to do little maintenance), but once implemented the user experience is excellent compared to Kubuntu LTS (in terms of performance and stability of the system). If you do not care about the system environment, then PopOS is a great alternative (suitable for beginners and experts). Also I do not have to worry about major upgrades between LTS like with Kubuntu as Manjaro is a rolling release (the downside is the maintenance for some updates). Finally, I am not afraid to update my system, which was my main reason to switch from Kubuntu!</p> <p>In the two remaining subsections, I expose a more detailed feedback by listing all advantages and disadvantages I have in my day-to-day usage. Then I list the troubles I got and how I solved them (followed by the sources I used to solve those problems).</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#day-to-day-usage","title":"Day-to-day usage","text":"","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#advantages","title":"Advantages","text":"<ul> <li> <p>Same interface as in my Kubuntu, but with a more up-to-date plasma (which gave me more up-to-date features from it).</p> </li> <li> <p>You can select your kernel and change it easily, with access to last kernels (LTS ones and non-LTS one, which can contain drivers for last pieces of hardware).</p> </li> <li> <p>No bleeding-edge plasma version unless it is not stable (compared to KDE Neon), it is a clear win for my needs.</p> </li> <li> <p>It runs way smoother on my laptop compared to my Kubuntu with the same battery life!</p> </li> <li> <p>The package manager has a similar interface to <code>apt</code>: <code>pamac</code> (and not the <code>pacman</code> manager). Hence, it helps a lot for my transition, plus <code>pamac</code> suggests optional packages to install (which can save you time if you miss something).</p> </li> <li> <p>Manjaro\u2019s default zsh shell is handy and useful to me out of the box (I even switched from bash to it).</p> </li> <li> <p>Large panel of software plus some great out of the box integrations (like the languagetool + texstudio integration if you install both of them).</p> </li> <li> <p>NVIDIA updates went all well (I observed it after 10 months of usage).</p> </li> </ul>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#disadvantages","title":"Disadvantages","text":"<ul> <li> <p>AUR packages can break on some update. It happens as it is not an official source of packages supported by the Manjaro team (but having access to them is very handy). As I use official packages first, then flatpak apps and as a last resort AUR packages I have few AUR apps, and it is not a dealbreaker for me. Nevertheless, solutions to solve this problem can be:</p> </li> <li> <p>Delay an AUR package update to comply with its dependencies from Manjaro\u2019s repositories (as they usually come later in Manjaro than in Arch). I did not need to do this in my usage.</p> </li> <li> <p>You may need to manually start a rebuild of an AUR package(<code>pamac remove &lt;package&gt;</code> then <code>pamac build &lt;package&gt;</code>) after a dependency update from Manjaro\u2019s repositories. It occurred once to me until now.</p> </li> <li> <p>Some updates can break some of your config files (as format can change with new major releases). It can happen between each Kubuntu LTS upgrade, but now I am on an Arch based distribution, it can occur each update (for each specific config you set on your system). In practice, it only occurred to me once due to a gnome-keyring update (which I hopefully will be able to get rid of with plasma 5.26, as kwallet will implement the missing protocols handled by gnome-keyring). A good habit to solve those kinds of problems is to follow the official Manjaro releases feed (it is a RSS one). There, they describe the updates with possible problems and most of the time solutions to solve them. As each release is associated to a post on the Manjaro\u2019s forum, you can ask the community for help (if there is not yet a solution \ud83d\ude04).</p> </li> <li> <p>You can\u2019t use discover to install and update for new applications. I was used to this, and it integrates well in the KDE environment. But on Arch based systems, using it can break your system. Fortunately, there is a similar manager on Manjaro which fills all my needs (search for apps, packages and handle flatpaks). Except it uses a gtk UI instead of a qt one, so it is a minor issue.</p> </li> </ul>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#problems-i-got-and-solutions","title":"Problems I got and solutions:","text":"","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#enable-bluetooth-devices-on-the-login-screen","title":"Enable Bluetooth devices on the login screen","text":"<p>This feature is quite useful for users of Bluetooth mouses and/or keyboards (to type logins and select users). I think it should be enabled by default, but you can enable it quickly:</p> <pre><code>sudo sed -i.back 's/#AutoEnable=false/AutoEnable=true/g' /etc/bluetooth/main.conf\n</code></pre>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#bluetooth-speakerheadphone-crackling-with-poor-quality","title":"Bluetooth speaker/headphone crackling with poor quality","text":"<p>To address this issue, first disable all power-saving options over Bluetooth devices (only for tlp users). To do this easily, you can install <code>tlpui</code>:</p> <pre><code>pamac install tlpui\n</code></pre> <p>Then, still in tlpui -&gt; audio section, toggle off the \u201csound power save controller\u201d option. This should improve some crackling, but it was not enough on my side. You may also look into USB_DENYLIST to add your Bluetooth receptor (even if you are using a laptop or the Bluetooth is integrated with your motherboard).</p> <p>I combined this solution with replacing pulseaudio by pipewire (which provide low-latency connections, implement more Bluetooth codecs and handle all pulseaudio call as pipewire is compatible with almost all pulseaudio API). More info on pipewire there.</p> <p>To do it under Manjaro, you have to type the following commands:</p> <pre><code>pamac remove pulseaudio pulseaudio-jack pulseaudio-lirc pulseaudio-rtp pulseaudio-zeroconf pulseaudio-bluetooth pulseaudio-alsa pulseaudio-ctl manjaro-pulse plasma-pa\npamac install manjaro-pipewire\npamac install plasma-pa\n</code></pre> <p>As I am writing this, we need to remove <code>plasma-pa</code> first to be able to remove pulseaudio. We reinstall it once pipewire is installed to be able to control audio via the KDE interface.</p> <p>Then restart your computer and it is done. All applications using previously pulseaudio should work with better latency and using better codecs \ud83d\ude04.</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#address-bluetooth-devices-lags-like-bluetooth-mouse-lags","title":"Address Bluetooth devices lags like Bluetooth mouse lags","text":"<p>First, if you are using TLP disable powersave for your device using tlpui (look at the USB_DENYLIST to add your Bluetooth receptor). Then, to address this issue, we\u2019re going to set a service that reduces Bluetooth latency on the kernel level for all devices. Finally, we will discuss Bluetooth codecs for speakers and their interference\u00a0with other Bluetooth devices.</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#set-a-service-to-force-low-latency","title":"Set a service to force low latency","text":"<p>To do so, we need to add the following script in <code>/etc/systemd/system/fix-mouse-lag.service</code> to create our service:</p> <pre><code>[Unit]\nDescription=run root script at boot/wake to fix mouse lag\nBefore=bluetooth.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/sleep 2\nExecStart=/usr/local/bin/fix-mouse-lag.sh\n\n[Install]\nWantedBy=bluetooth.service\n</code></pre> <p>Then, add the following script in <code>/usr/local/bin/fix-mouse-lag.sh</code> that will be run by our service on every boot/wake:</p> <pre><code>#!/bin/sh\n\necho 0 &gt; /sys/kernel/debug/bluetooth/hci0/conn_latency\necho 6 &gt; /sys/kernel/debug/bluetooth/hci0/conn_min_interval\necho 7 &gt; /sys/kernel/debug/bluetooth/hci0/conn_max_interval\n</code></pre> <p>Finally, let us enable and start our service:</p> <pre><code>sudo chown root:root /etc/systemd/system/fix-mouse-lag.service\nsudo chmod a+rx /usr/local/bin/fix-mouse-lag.sh\nsudo systemctl enable fix-mouse-lag.service --now\n</code></pre>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#selecting-a-bluetooth-codec-for-my-headset","title":"Selecting a Bluetooth codec for my headset","text":"<p>I own a Bose QC 35 II, and on paper it can only be used with HSP/HFC, AAC and SBC codec. The first one being a low-quality codec that let use the microphone of the headset, and the last ones are for high-quality audio output only. With my testing, I found out it can also handle SBC XQ codec (which is a better codec compared to AAC and SBC, see this link for details).</p> <p>I tested these three high-quality codecs with a Logitech MX Vertical and I found out that while SBC and SBC XQ codecs do not disrupt my mouse connection, the AAC codec does. Pipewire use the AAC codec by default, as Bose recommend it for this headphone<sup>1</sup>. To modify that behavior, we have to edit <code>/usr/share/pipewire/media-session.d/bluez-monitor.conf</code> file. In it, I  uncommented the <code>bluez5.enable-sbc-xq    = true</code> line and specified the <code>bluez5.codecs</code> as follows:</p> <p><code>bluez5.codecs = [ sbc_xq ldac aptx aptx_hd aptx_ll aptx_ll_duplex faststream faststream_duplex ]</code></p> <p>Hence, I removed the SBC and AAC capacity of Pipewire to be sure it will not use those codecs with my devices (the first one is not a problem, but I prefer higher quality for my headset \ud83d\ude1b).</p>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#fix-themes-not-working-with-flatpak-apps","title":"Fix themes not working with flatpak apps","text":"<p>Most flatpak apps will not have their theme following the system one on Manjaro KDE. Specifically gtk and electron apps. If it is not addressed upstream on Arch, Manjaro devs will not fix it (see my sources for more details). To address the issue, we need to type the following commands:</p> <pre><code>flatpak override --filesystem=xdg-config/gtk-3.0:ro\nflatpak override --filesystem=xdg-config/gtk-4.0:ro\n</code></pre> <p>It adds the minimum permissions to flatpak apps needed by them to see the theme used under the KDE environment. Then, you must install the corresponding flatpak GTK theme to the one you are using under your system. For example, if you are using the breeze-dark theme under KDE, you have to install the flatpak theme version for gtk applications like this:</p> <pre><code>flatpak -y install org.gtk.Gtk3theme.Breeze-Dark\n</code></pre>","tags":["Archive","Linux"]},{"location":"blog/2022/09-01-my-switch-to-manjaro/#sources","title":"Sources","text":"<ul> <li> <p>AUR usage in Manjaro</p> </li> <li> <p>Enable Bluetooth at startup</p> </li> <li> <p>Reduce mouse input lags</p> </li> <li> <p>Address flatpak theming issue (by your servant)</p> </li> </ul> <ol> <li> <p>Note I talk only for the pipewire case. I did not test it for pulseaudio as I have crackling issues with it.\u00a0\u21a9</p> </li> </ol>","tags":["Archive","Linux"]},{"location":"blog/2022/10-30-my-phd-experience/","title":"My PhD experience","text":"<p>It's been a while; there was no second-year review due to mixed feelings during COVID. But thanks to my supervisors, I managed to overcome my negative thoughts. I am now proud to be a Doctor of Computer Science \ud83c\udf89.</p> <p>As a reminder, this thesis was supervised by Julien Pinquier, J\u00e9r\u00f4me Farinas, and Virginie Woisard. It was conducted at IRIT (Toulouse, France).</p> <p>In this post, I will share with you a summary of my work, some feelings and advises I have to share.</p>","tags":["Archive","PhD"]},{"location":"blog/2022/10-30-my-phd-experience/#thesis-summary","title":"Thesis summary","text":"<p>People with head and neck cancers have speech difficulties after surgery or radiation therapy. It is important for health practitioners to have a measure that reflects the severity of speech. To produce this measure, a perceptual study is commonly performed with a group of five to six clinical experts. This process limits the use of this assessment in practice. Thus, the creation of an automatic measure, similar to the severity index, would allow a better follow-up of the patients by facilitating its obtaining.</p> <p>To realize such a measure, we relied on a reading task, classically performed. We used the recordings of the cancer corpus, which includes more than 100 people <sup>1</sup>. This corpus represents about one hour of recording to model the severity index. In this PhD work, a review of state-of-the-art methods on speech, emotion and speaker recognition using little data was undertaken. We then attempted to model severity using transfer learning and deep learning. Since the results were not usable, we turned to the so-called \u00ab few shot \u00bb techniques (learning from only a few examples). Thus, after promising first attempts at phoneme recognition<sup>2</sup>, we obtained promising results for categorizing the severity of patients. Nevertheless, the exploitation of these results for a medical application would require improvements.</p> <p>We therefore performed projections of the data from our corpus. As some score slices were separable using acoustic parameters, we proposed a new divergence measurement method <sup>3</sup>. This one is based on self-supervised speech representations on the Librispeech corpus: the PASE+ model, which is inspired by the Inception Score (generally used in image processing to evaluate the quality of images generated by models). Our method allows us to produce a score similar to the severity index with a Spearman correlation of 0.87 on the reading task of the cancer corpus. The advantage of our approach is that it does not require data from the cancer corpus for training. Thus, we can use the whole corpus for the evaluation of our system. The quality of our results has allowed us to consider a use in a clinical environment through an application on a tablet: tests are underway at the Larrey Hospital in Toulouse.</p> <p>Keywords: Speech pathology, severity index, speech disorder, ENT cancer, deep learning, learning with a few examples, self-supervised, entropic measurement, few-shot, limited data, limited amount of data, automatic speech processing.</p> <p>For my manuscript, I used Latex (as all of my articles). To collaborate with others (especially my supervisors) on it, I used overleaf to have nice comments (more information here), a history of modifications and to synchronize my work in progress on GitHub (for safety as I did not want to rewrite this manuscript from scratch). Once my manuscript goes online, I will add a link to it here for those interested.</p>","tags":["Archive","PhD"]},{"location":"blog/2022/10-30-my-phd-experience/#thesis-presentation","title":"Thesis presentation","text":"<p>Like my manuscript, I used Latex (with beamer) for my presentation! It is not a common choice (I did not see any thesis presentation using Latex) but if you are used to it, you gain so much time to focus on the content instead of the form. I did create my own theme (not from scratch I am not a mad man \ud83e\udd23) and it was near complete before working on my thesis presentation (as I used beamer for my weekly presentations with my supervisors).</p> <p>Now let\u2019s talk a little about my feelings, I was stressed for sure, this moment intimidated me. It is so formal, especially the time to complete the presentation (45 min in my case). Usually when I present things, if I take 5/10 more or fewer minutes than expected, and it is fine. But there, I did not have this margin of error. This stressed me and even if I did rehearsal a lot for this one, the more I prepared, the more I felt stressed. Funny thing is when there were problems (people arriving while I was started, missing slides, slides not shared at the beginning and so on) it makes me forgot about the stressing things of the experience. For those who weren't there, you can see my presentation/\"performance\" here (sorry folks, this video is in French). I think I could have reduced my stress if I did a rehearsal in the amphitheater and use a digital chronometer (with a large screen) on both my rehearsal and the final day.</p> <p>I was more at easy with the questions, there I was not limited by the time to answer the jury. Also, the fact that it was interactive reduce the formal aspect of it. Then, after the jury deliberated, I received my PhD title! What an incredible feeling, a mix of joy, stress and relief. All make me cry at the end, what a great moment!</p> <p>I hope this will help and/or inspire some of you.</p> <p>See you again, Vincent.</p> <ol> <li> <p>I participated in the analysis of the dataset in hal-02921918 \u21a9</p> </li> <li> <p>I conducted the review of state-of-the-art techniques and the experiments that lead to this paper:  DOI:10.1186/s13636-022-00251-w \u21a9</p> </li> <li> <p>This work led to the following publication (yet to be published): Roger, V., Farinas, J., Woisard, V., and Pinquier, J. (2022b). Cr\u00e9ation d\u2019une mesure entropique de la parole pour \u00e9valuer l\u2019intelligibilit\u00e9 de patients atteints de cancers des voies a\u00e9rodigestives sup\u00e9rieures. In 34e Journ\u00e9es d\u2019\u00c9tudes sur la Parole (JEP2022).\u00a0\u21a9</p> </li> </ol>","tags":["Archive","PhD"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/","title":"My Red Bull Wololo Legacy dataviz","text":"<p>On spare time, I play Age of Empires 2 (the video game of my childhood). I also like to watch some tournaments with pro players. Recently, there was the biggest tournament of Age of Empires 2 organized by Red Bull: Red Bull Wololo Legacy. This tournament included a prize pool of $200,000! Not bad for a 25 years old game.</p> <p>As you can see on my blog (also on Twitter and Reddit), I like to post data visualizations. Also, I integrated an association of data visualization (Toulouse dataviz). To improve myself on data visualization, why not merge theses two hobbies?</p> <p>Here we are, in this post I will show all data visualization I have done for this Tournament. In this post, you have the enhanced ones (mostly aesthetic changes) compare to those I published on Twitter and Reddit. To make these graphs, I used data from Liquipedia, generated graphs with Plotly and refine them (with logos, backgrounds on figures and so on) using Inkscape. I have chosen to use the two main colors of the Red Bull logo for my creations, as at the time I was thinking it could be nice to make create some sort of series.</p> <p>I am happy it worked out that well, so this post is the beginning of a series of post about dataviz for Age of Empires 2 tournaments!</p> <p>Enough talk, let's look at my work now \ud83d\ude03.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#players-achievements","title":"Players achievements","text":"<p>My first dataviz was to show the achievements of each player regarding their rank alongside the number of games they won, lost. It also includes the prize they gained. I am pretty happy with this one as it looks pleasant with multiple information on it.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#civilizations-matchups","title":"Civilizations matchups","text":"<p>Next, we have the dataviz with the most feedback (positive and negative ones). In this work, I created a confusion matrix and as they are symmetrical, I removed the redundancy (a habit as a machine learning guy \ud83d\ude05). I think the result looks pleasant, but I am not convinced of readability (I have a few comments on that point). As it required significant effort in Inkscape, I may not include this one (as is or at all) for future tournaments.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#which-civilization-was-played-on-each-map","title":"Which civilization was played on each map?","text":"<p>Next, we have a matrix of which civilization was played for each map. Simple and efficient, I like the content but not that pleased on the aesthetic. I will try to improve that for next tournaments.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#which-maps-were-popular","title":"Which maps were popular?","text":"<p>This one got less attention from people, but as I like the info it gives, I will keep it and try to improve it for future tournaments.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#killdeath-ratio-of-each-civilization-played","title":"Kill/death ratio of each civilization played","text":"<p>One that created numerous discussions on strategies and civilizations balances. I love to discuss and see explanations from others, it helps me as a player, and it is always fun \ud83d\ude04.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#the-wheel-of-victory","title":"The wheel of victory","text":"<p>Finally, my last work, a wheel of the victorious civilizations on each map. I did it as a wheel to avoid the blanks from an equivalent using matrices. It resulted in an aesthetic (I am biased as I am pleased of the look) and informative creation. It includes the proportions of games on each map and proportions of civilization victory on each map.</p> <p>This work will require work for next tournaments (to let people see numbers and not only proportions), but it was fun to make.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-08-redbull-wololo-legacy/#what-is-next","title":"What is next?","text":"<p>For this event, I published on both Twitter and Reddit. My creations obtained more views and reactions on Reddit. For a comparison, my first visualization has been viewed 30 times on Twitter (with no interactions) and more than 35,000 on Reddit! On Reddit, those dataviz generated discussions about the tournament and I got feedbacks with good advices to improve my work (which will be seen for future tournaments).</p> <p>If you did not see it yet, the next tournament I will cover will be Warlords.</p> <p>I hope this will help and/or inspire some of you.</p> <p>See you again, Vincent.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/","title":"My dataviz for Warlords","text":"<p>This post is the second episode of my Age of Empires 2 tournaments series (have a look at my first one if you missed it). All work done here concern the main event (it is what interest me the most in tournaments).</p> <p>Here, the two main colors I picked from the logo result in more pleasant dataviz (at least to my point of view). This time, I improved my dataviz on the following aspects:</p> <ul> <li>use a better font (here Nimbus Sans);</li> <li>avoid titles in my graphs, as we have them in Reddit post and on this blog;</li> <li>integrate logo of the tournament inside each visualization:</li> <li>integrate information about the data used (here I only used Liquipedia data, but I might add more sources in future tournaments);</li> <li>increase font size to be easier to read (maybe not enough for smartphones, though)</li> </ul> <p>Enough talk, let's look at my work now \ud83d\ude03.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#players-achievements","title":"Players achievements","text":"<p>My first dataviz was the one to show the achievements of each player regarding their rank alongside the number of games they won, lost. Here Classic Pro had to forfeit and Jordan had an automatic win. I tried things to illustrate it but was not happy about the result, so it ended with the following one:</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#which-civilization-was-played-on-each-map","title":"Which civilization was played on each map?","text":"<p>Next, we have the matrix of which civilization was played for each map. In this tournament, the civilizations not played on this main event were: Goths, Slavs, Vietnamese, Huns, Celts, and Sicilians. Here we have a huge matrix as we have more maps compare to Red Bull Wololo Legacy. It is now easier to read it, but the esthetic is not enough for me (maybe next time will be better).</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#which-maps-were-popular","title":"Which maps were popular?","text":"<p>This one got more attention from people compared to the one from the previous tournament. I think it is due to the way I sorted the abscises, but also because the map pool is big enough to require such visualization to better understand what happened.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#killdeath-of-each-civilization-played","title":"Kill/death of each civilization played","text":"<p>Here, as for the visualization above, I sorted the abscises with the amount for game played. I think it is better than sorting them alphabetically. Some asked for the win rate, while it is understandable for civilizations played more than 10 times, I am uncertain if it is good for the other case. I still love the discussions it generates \ud83d\ude04. Even so, I plan to work on this one to improve its quality.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#the-wheel-of-victory","title":"The wheel of victory","text":"<p>Finally, the wheel of victory! Compared to last time, I did not publish a png one on Reddit. It is an exclusivity for those reading post \ud83d\ude09. As I wanted it to provide additional information compared to last tournament (also as the map pool is huge), I opted for an interactive visualization. The result is less pretty than the previous from Red Bull, but I hope you will enjoy it. You can hover each segment with your mouse to gather information, and even click on categories to look at a specific map or category of map (click on the warlord logo to go back). Don't hesitate to give me advice to improve this work.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2022/12-13-warlords/#what-is-next","title":"What is next?","text":"<p>I want to gather more data using different sources to complete or create new data visualizations. For now, I will focus on captain mode data, and if you know API I can use, feel free to comment. Also, if you know contacts I should reach, feel free to share \ud83d\ude04.</p> <p>The next tournament I will cover is the Grand Melee, the hype is there!</p> <p>I hope this will help and/or inspire some of you.</p> <p>See you again, Vincent.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/","title":"My dataviz for The Grand Melee","text":"<p>This post is the third episode in my Age of Empires 2 tournament series (check out my first and my second if you missed them). All the work done here is about the main event (that's what interests me the most in tournaments). As a reminder, I published most of these visualizations on Reddit before making this post. This one is a summary of the tournament and the feedback I got.</p> <p>This time, I improved my visualizations with additional data from Age of Empires Captains Mode when it was useful and by adding a QR code as my signature for my works.</p> <p>Now let's take a look at that work \ud83d\ude03.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/#player-achievements","title":"Player Achievements","text":"<p>My first visualization is the one showing each rank obtained from the players with the number of games they won, lost. Here, there were fewer players for the main event, still we saw some great matches.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/#which-civilization-was-played-on-each-map","title":"Which civilization was played on each map?","text":"<p>Next, we have a sankey on which civilization was played on each map. In this tournament, the civilizations that have not been played are: Celts, Chinese, Hungarians, Malays, Malians, Sicilians, Slavs, Spanish, Teutons, Vietnamese and Vikings. I preferred the sankey to the heat matrix, as the matrix has many zeros. To be seen if sankey is more readable on events with more matches \ud83d\ude09</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/#which-maps-were-popular","title":"Which maps were popular?","text":"<p>Here I added the ban information from the player drafts. I liked this, but I'd like to find a more suitable display for the future.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/#killdeath-of-each-civilization-played","title":"Kill/Death of each civilization played","text":"<p>Here I added the snipe and ban information from the player drafts. I still love the discussions this graph generates \ud83d\ude04. I had some ideas to improve this graph (including having snipe/bans/death as negative values), we'll see at the next tournament if I get better visualizations.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/01-16-the_grand_melee/#the-wheel-of-victory","title":"The wheel of victory","text":"<p>Finally, the victory wheel! Like my previous post, this is exclusive to those reading this post \ud83d\ude09. You can hover over each segment with your mouse to gather information, and even click on the categories to examine a specific card or card category (click on the tournament logo to go back). Please don't hesitate to give me tips on how to improve this work.</p> <p>I hope this helps and/or inspires some of you.</p> <p>See you again, Vincent.</p>","tags":["Archive","Age of Empires","Visualizations"]},{"location":"blog/2023/12-03-tools-python-dataviz/","title":"Datavisualization tools for Python","text":"<p>Hello again, it has been a while!</p> <p>Recently, I did a presentation on Python libraries for data visualizations. I compared them and presented usage examples. All code and examples are available here. If by any chances the slides do not work well for you, you can download the pdf version (with no animated elements).</p> <p>If you understand French you can watch my presentation:</p> <p>It was a talk done for the Toulouse Dataviz association. I hope this helps and/or inspires some of you.</p> <p>See you again, Vincent.</p>","tags":["Archive","Python","Visualizations"]},{"location":"blog/2024/06-02-diffusers-unconditional-model/","title":"Generate images with unconditional model","text":"<p>In recent years, diffusion models have emerged as a powerful tool in the domain of generative modeling, often rivaling GANs (Generative Adversarial Networks) in producing high-quality images (have a look at this paper from 2021 and stable diffusion from 2022). These models operate by simulating a diffusion process where data is progressively noised and then denoised to generate new samples.</p> <p>New courses appear to explain how such method function. One of them is How Diffusion Models Works from DeepLearning.ai. It provides great notebooks to create our first diffusion model/pipeline. But, it is tedious to implement such an approach from scratch using only the PyTorch library (but a great way to learn the way such a method works).</p> <p>In this blog post, we will delve into the implementation of an Unconditional UNet using the Diffusers library from deeplearning.ai. We will walk through the key components of the code, explain how the diffusion process is modeled with the Diffusers library. All the code (as notebook) and trained weights can be found there.</p> <p>Note</p> <p>This post will not explain how diffusion models works, but how to use diffusers library to simplify our life.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-02-diffusers-unconditional-model/#why-the-diffusers-library","title":"Why the diffusers library?","text":"<p>This library is backed by Hugging Face and offers a robust and flexible framework for working with diffusion models. Here are the main reasons why you should consider using Diffusers for your projects:</p> <ul> <li>Ease of use with high-level API to design UNET models.</li> <li>Comprehensive documentation even if tutorials and examples are not covering all cases.</li> <li>Active community and support, with more than 23k stars on their GitHub repository.</li> </ul>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-02-diffusers-unconditional-model/#define-the-unet-model-and-a-noise-scheduler","title":"Define the unet model and a noise scheduler","text":"<p>Before training a model, we need to define it and to define the associated noise scheduler:</p> <pre><code>from diffusers import DDPMScheduler\nfrom diffusers.models import UNet2DModel\n\n# Define the UNet model\n# Note: this model use less parameters compared to deeplearning.ai course as it is not necessary to have such huge model for this task\nmodel = UNet2DModel(\n    sample_size=(16,16),                              # Input image size\n    in_channels=3,                                    # Number of input channels (e.g., 3 for RGB)\n    out_channels=3,                                   # Number of output channels\n    layers_per_block=2,                               # Layers per block in the UNet\n    block_out_channels=(128, 64),                     # Channels in each block\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\"),  # Types of down blocks\n    up_block_types=(\"UpBlock2D\", \"UpBlock2D\")         # Types of up blocks\n)\n\n# Define the DDPM scheduler\nnoise_scheduler = DDPMScheduler(num_train_timesteps=500)\n</code></pre> <p>And that is it! The UNet2DModel can save us a huge amount of time defining such a model.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-02-diffusers-unconditional-model/#train-the-model","title":"Train the model","text":"<p>Here I made a simple train function including the basics to not overwhelm readers \ud83d\ude09:</p> <pre><code>def train(unet: UNet2DModel, noise_scheduler: DDPMScheduler, dataloader: DataLoader, num_epochs: int, lr: float) -&gt; None:\n    \"\"\"Train the unet given its noise_scheduler and a dataloader.\n\n    Parameters\n    ----------\n    unet : UNet2DModel\n        The model unet to train.\n    noise_scheduler : DDPMScheduler\n        noise scheduler to use while training.\n    dataloader : DataLoader\n        The dataloader containing the images to reproduce.\n    num_epochs : int\n        The number of epochs to train the unet.\n    lr : float\n        The learning rate to use to train the unet.\n    \"\"\"\n    epochs = range(num_epochs)\n    losses = np.zeros(num_epochs)\n\n    optimizer = Adam(unet.parameters(), lr=lr)\n    unet.train()\n\n    for epoch in epochs:\n        epoch_loss = 0\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n\n            # Assuming your dataloader provides images and targets (not used here)\n            images, _ = batch\n            images = images.to(unet.device)\n\n            # Generate random noise\n            noise = torch.randn(images.shape).to(unet.device)\n\n            # Forward pass through the model\n            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=unet.device).long()\n            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n            predicted_noise = unet(noisy_images, timesteps).sample\n\n            # Compute loss (mean squared error between actual and predicted noise)\n            loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        epoch_loss /= len(dataloader)\n        losses[epoch] = epoch_loss\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n</code></pre> <p>It prints the mean loss of each epoch. To train the model, we forward the model on random timesteps on data from a dataloader. Then, we compute the loss on the predicted noise to optimize the parameters of our model. In the notebook we used the dataloader from deeplearning.ai so nothing new under the sun \ud83d\ude1b. As you can see, Diffusers library is not magical and require we implement some stuff ourselves.</p> <p>To save our pretrained model, a single line is required:</p> <pre><code>model.save_pretrained(pre_trained_model_path)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-02-diffusers-unconditional-model/#do-inferences-with-the-trained-model","title":"Do inferences with the trained model","text":"<p>Now we learned our model, we can load it and use it on a pipeline from the diffusers library. First, let's load our model parameters:</p> <pre><code>model = UNet2DModel.from_pretrained(pre_trained_model)\n</code></pre> <p>Pretty easy! Now let's declare our pipeline and use cuda if available:</p> <pre><code>from diffusers import DDPMPipeline\n\npipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\npipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <p>And that's all, now generate new images is as simple as:</p> <pre><code>generated_image = pipeline(batch_size=16, num_inference_steps=500)\n</code></pre> <p>I made a simple tool to show the generated images (you can take a look at the GitHub repository for more information). So let's visualize the result:</p> <pre><code>fig = plot_generated_images(generated_image.images, 4, 4)\nfig.show()\n</code></pre> <p></p> <p>Here we go, we have generated our first examples using the diffusers library. In the next blog post, we will see how to generate examples given one-hot ground truth (instead of tutorials using only text and to follow the deeplearning ai course).</p> <p>I hope this helps and/or inspires some of you.</p> <p>See you again, Vincent.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-16-diffusers-conditional-model/","title":"Generate images from labels with conditional model","text":"<p>Following my previous post on unconditional generation using the diffusers library (link here), we will dive into conditional generation using labels. While many tutorials and examples focus on text-guided diffusion models, in this blog post, we will explore the use of one-hot labels to condition our diffusion model. This approach can be particularly useful in scenarios where we want to generate images based on discrete categories rather than textual descriptions.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-16-diffusers-conditional-model/#conditional-diffusion","title":"Conditional diffusion","text":"<p>Conditional diffusion models extend unconditional diffusion by introducing additional information during the generation process. This additional information, or conditioning input, can guide the model to produce samples that belong to a specific category or possess certain characteristics. Common conditioning inputs include text descriptions, class labels, or other image attributes.</p> <p>In this tutorial, we will use one-hot encoded labels as the conditioning input. One-hot encoding is a representation of categorical variables as binary vectors. This method is especially suitable for tasks like image generation where each image corresponds to a discrete category. Taken back to our previous dataset, we will have 5 categories : <code>[hero, non-hero, food, spell, side-facing hero]</code>.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-16-diffusers-conditional-model/#define-the-unet-model-the-noise-scheduler-and-network-to-embed-our-labels","title":"Define the unet model, the noise scheduler and network to embed our labels","text":"<p>Compared to last time, we will use the <code>UNet2DConditionModel</code> class for our model, and we will pass the ground truth as embeddings.</p> <pre><code>from diffusers import DDPMScheduler\nfrom diffusers.models import UNet2DConditionModel\n\nclass_emb_size = 64\n\n# Define the UNet model\nunet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,\n    sample_size=(16, 16),                                       # Input image size\n    in_channels=3,                                              # Number of input channels (e.g., 3 for RGB)\n    out_channels=3,                                             # Number of output channels\n    layers_per_block=2,                                         # Layers per block in the UNet\n    block_out_channels=(64, 128),                               # Channels in each block\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),   # Types of down blocks\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),         # Types of up blocks\n)\n\n# Define the DDPM scheduler\nnoise_scheduler = DDPMScheduler(num_train_timesteps=200)\n</code></pre> <p>To introduce conditions with such a model, you can use cross attention block, hence the use of <code>\"CrossAttnDownBlock2D\"</code> and <code>\"CrossAttnUpBlock2D\"</code> in our unet. Now we need some network to embed our one-hot data into the desired embedding size, here we will take the similar network as in the deeplearning.ai course:</p> <pre><code>class UnsqueezeLayer(nn.Module):\n    \"\"\"Generic layer to unsqueeze its input.\"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super(UnsqueezeLayer, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.unsqueeze(x, dim=self.dim)\n\n\n# We have to create this custom class to be able to use our sequential model inside our pipeline.\nclass CustomSequential(nn.Sequential):\n    \"\"\"Extend sequential to add `device` and `dtype` properties.\n\n    It supposes that all parameters shares the same device and uses the same dtype.\n    \"\"\"\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def dtype(self):\n        return next(self.parameters()).dtype\n\nnum_classes = 5\n\nemb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size),\n    nn.GELU(),\n    nn.Linear(class_emb_size, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n</code></pre> <p>Here, we used a custom class to have <code>device</code> and <code>dtype</code> properties on this model (to be able to use it in our final pipeline). As <code>UNet2DConditionModel</code> is designed to have text embeddings of shape <code>[batch, sequence_length, feature_dim]</code>, our <code>emb_net</code> finish with a squeeze to have the embedding of shape <code>[batch_size, 1, class_emb_size]</code>. Having no sequence in our labels, this suits better our case.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-16-diffusers-conditional-model/#train-the-model","title":"Train the model","text":"<p>Here I modified our train function to add mixed precision (to speed up the training) and our conditioning:</p> <pre><code>def train(\n    unet: UNet2DConditionModel,\n    emb_net: nn.Module,\n    noise_scheduler: DDPMScheduler,\n    dataloader: DataLoader,\n    num_epochs: int,\n    lr: float,\n):\n    epochs = range(num_epochs)\n\n    optimizer = Adam(chain(unet.parameters(), emb_net.parameters()), lr=lr)\n    scaler = GradScaler(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # For mixed precision\n    unet.train()\n\n    for epoch in epochs:\n        epoch_loss = 0\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n\n            # Assuming your dataloader provides images and associated target\n            images, labels = batch\n            images = images.to(unet.device)\n            labels = labels.to(dtype=torch.float32, device=unet.device)\n\n            with autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):  # Mixed precision\n                # Generate random noise\n                noise = torch.randn(images.shape, device=unet.device)\n\n                # Generate random timesteps and apply the noise scheduler\n                timesteps = torch.randint(\n                    0,\n                    noise_scheduler.config.num_train_timesteps,\n                    (images.shape[0],),\n                    device=unet.device,\n                )\n\n                noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n\n                # Compute the class embeddings\n                enc_labels = emb_net(labels)\n\n                # Forward pass through the model with labels embeddings\n                predicted_noise = unet(\n                    noisy_images, timesteps, enc_labels, class_labels=labels\n                ).sample\n\n                # Compute loss (mean squared error between actual and predicted noise)\n                loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n\n            # Backward pass and optimization\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        epoch_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n</code></pre> <p>It prints the mean loss of each epoch. To train the model, we forward the model on random timesteps with our data and embeddings. Then, we compute the loss on the predicted noise to optimize the parameters of our <code>unet</code> and <code>emb_net</code>. In the notebook we kept the dataloader from deeplearning.ai so nothing new under the sun. As you can see, our training loop is not that much different from the training loop of our unconditional unet.</p> <p>Note</p> <p>I did not randomly mask out the labels (like in the deeplearning.ai tutorial) as it did not improve the results. But we will see in the next article why it can be important (in the right circumstances).</p> <p>Now we can save our pretrained <code>unet</code> and <code>emb_net</code>.</p> <pre><code>unet.save_pretrained(pre_trained_unet_path)\ntorch.save(emb_net.state_dict(), pre_trained_emb_net_path)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/06-16-diffusers-conditional-model/#do-inferences-with-the-trained-unet","title":"Do inferences with the trained unet","text":"<p>Now we learned our model, we can load it and use it on a pipeline from the diffusers library. First, let's load our models:</p> <pre><code>unet = UNet2DConditionModel.from_pretrained(pre_trained_unet)\nemb_net.load_state_dict(torch.load(pre_trained_emb_net))\n</code></pre> <p>Pretty easy! Now the hard part, we need to create a custom pipeline to use our models. Here, we will not start from zero by extending the <code>DDPMPipeline</code>:</p> <pre><code>from diffusers import DDPMPipeline\n\n\nclass ConditionalDDPMPipeline(DDPMPipeline):\n    def __init__(\n        self, unet: UNet2DConditionModel, class_net: CustomSequential, scheduler: DDPMScheduler\n    ) -&gt; None:\n        super().__init__(unet=unet, scheduler=scheduler)\n        self.class_net = class_net\n        # to let the pipeline change the model device and/or type\n        self.register_modules(class_net=class_net)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        class_label: list[list[float]],\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        num_inference_steps: int = 1000,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -&gt; Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            class_label (list[list[float]]):\n                list of one-hot examples. len(class_label) represents the number of examples to generate.\n            generator (`torch.Generator`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            num_inference_steps (`int`, *optional*, defaults to 1000):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n                returned where the first element is a list with the generated images\n        \"\"\"\n        batch_size = len(class_label)\n        # Sample gaussian noise to begin loop\n        if isinstance(self.unet.config.sample_size, int):\n            image_shape = (\n                batch_size,\n                self.unet.config.in_channels,\n                self.unet.config.sample_size,\n                self.unet.config.sample_size,\n            )\n        else:\n            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n\n        if self.device.type == \"mps\":\n            # randn does not work reproducibly on mps\n            image = randn_tensor(image_shape, generator=generator)\n            image = image.to(self.device)\n        else:\n            image = randn_tensor(image_shape, generator=generator, device=self.device)\n\n        labels = torch.tensor(class_label, device=self.device)\n        enc_labels = self.class_net(labels)\n\n        # set step values\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        for t in self.progress_bar(self.scheduler.timesteps):\n            # 1. predict noise model_output\n            model_output = self.unet(image, t, enc_labels, class_labels=labels, return_dict=False)[\n                0\n            ]\n\n            # 2. compute previous image: x_t -&gt; x_t-1\n            image = self.scheduler.step(model_output, t, image, generator=generator).prev_sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image,)\n\n        return ImagePipelineOutput(images=image)\n</code></pre> <p>Basically, it is the same code as the <code>DDPMPipeline</code> from diffusers except we have our <code>emb_net</code> added to it to do generations conditioned with labels. Now let's declare our pipeline with cuda if available, generate some samples and visualize them:</p> <pre><code>pipeline = ConditionalDDPMPipeline(unet=unet, class_net=class_net, scheduler=noise_scheduler)\npipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ngenerated_image = pipeline(\n    [\n        # hero, non-hero, food, spell, side-facing hero\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n    ],\n    num_inference_steps=200,\n)\n\nfig = plot_generated_images(generated_image.images, 5, 4)\nfig.show()\n</code></pre> <p></p> <p>Here we go, we have generated our first conditioned examples using the diffusers library with one-hot labels. The full notebook and weights is available here.</p> <p>I hope this helps and/or inspires some of you.</p> <p>See you again, Vincent.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/","title":"Improving Speed and Quality of Image Generation","text":"<p>Welcome to the third installment of our series on the Diffusers library. In this post, we\u2019ll explore strategies to enhance both the inference speed and the quality of generated images. Thus, we will improve the techniques discussed in our previous article, also, we will also use new ones.</p> <p>Note on Best Practices</p> <p>Before we dive into the technical details, it\u2019s important to note that this post does not adhere to all best practices typically recommended for machine learning projects. Specifically, we are not utilizing dataset versioning, tracking tools like MLflow, or hyperparameter optimization frameworks such as Optuna. These practices are essential for a robust ML workflow and will be addressed in future posts, though not immediately. The focus of this tutorial is to present a simplified workflow that is easier to follow and understand.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#data-analysis-the-crucial-first-step","title":"Data Analysis: The Crucial First Step","text":"<p>Analyzing the data is a critical first step in improving image generation quality.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#understanding-data-distribution","title":"Understanding Data Distribution","text":"<p>Let's start by examining the distribution of our dataset. This will help us identify any imbalances or anomalies in the data.</p> <p></p> <p>Currently, our dataset is not balanced. Balancing it will enhance the model's performance by improving its ability to generalize and produce more accurate results. We can do it with the following addition in our training notebook:</p> <pre><code>dataset = SpritesDataset(\n    \"./dlai_lib/sprites_1788_16x16.npy\",\n    \"./dlai_lib/sprite_labels_nc_1788_16x16.npy\",\n    null_context=False\n)\n\n# Create dataset sampler to compensate the unbalanced dataset\nlabels = dataset.slabels.argmax(axis=1)\nu_labels, class_counts = np.unique(labels, return_counts=True)\nclass_weights = 1 - class_counts / class_counts.sum()\nsample_weights = tuple(class_weights[label] for label in labels)\nnum_samples = ceil(len(labels)/batch_size) * batch_size # num_samples is a multiple of batch_size to avoid recompiling\n\ndataset_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n\ndataloader = DataLoader(\n    dataset, sampler=dataset_sampler, batch_size=batch_size, num_workers=1, pin_memory=True\n)\n</code></pre> <p>In this approach, we calculate weights for each sample in our dataset and use a random data sampler to select samples according to these weights. As a result, samples from the least represented classes are selected more frequently than those from the more prevalent classes.</p> <p>Despite the non-human class being one of the most prevalent in our dataset, the model struggles to generate anything beyond indistinct blobs, indicating that there may be underlying issues with the data itself.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#analyzing-non-human-class","title":"Analyzing Non-Human Class","text":"<p>A major challenge for our model is the non-human class, which frequently includes poorly representative images. To illustrate this issue, let\u2019s first examine the original non-human images:</p> <p></p> <p>As observed, some examples are of poor quality and resemble the non-human outputs produced by our model. To address this, we will investigate and develop a function to automatically detect such problematic samples in the database, as reviewing over 80,000 samples manually is not feasible.</p> <pre><code>def detect_wrong_images(image: np.ndarray, number_of_white: int = 26) -&gt; bool:\n    \"\"\"Function to detect if an image is a wrong one and should be excluded from training.\n\n    Parameters\n    ----------\n    image : np.ndarray\n        the image in RGB with shape [width, heigh, channel]\n    number_of_white : int, optional\n        the number of white pixel a function must have to be considered valid, by default 26\n\n    Returns\n    -------\n    bool\n        True if it is a wrong image for generation and False if it is a good one.\n    \"\"\"\n\n    wrong_image = False\n    # Note white is (255, 255, 255) hence the sum is 765\n    # White background so a minimum of pixel should be white\n    wrong_image |= np.sum(image.sum(axis=-1) == 765) &lt; number_of_white\n\n    # 4 corners must be white\n    wrong_image |= image[0, 0].sum() != 765\n    wrong_image |= image[-1, 0].sum() != 765\n    wrong_image |= image[0, -1].sum() != 765\n    wrong_image |= image[-1, -1].sum() != 765\n\n    return wrong_image\n</code></pre> <p>The purpose of this function is straightforward: good images should have all corners white and a significant portion of the pixels should be white, as we expect white backgrounds. While this approach is sufficient for a relatively simple dataset, real-world datasets will require additional checks, such as for blurriness and color palette constraints.</p> <p>If we plot some bad samples we obtain this:</p> <p></p> <p>Using this approach, we identified and removed 30,000 problematic samples from our dataset, effectively capturing the key issues for the non-human class.</p> <p>And if we show the 30 non-human examples of our new dataset, we obtain this:</p> <p></p> <p>Applying this function to refine our dataset can accelerate model training and significantly improve results. However, you might still find that the quality of the generated images is insufficient and the generation process is too slow. I created a custom dataset to enable this filtering with <code>clean_version=True</code> parameter.</p> <p>Note</p> <p>For a detailed explanation of this process, please refer to the dedicated notebook here: https://github.com/vroger11/diffusers-tutorials/tree/main/tutorials/03-dataset_analysis.ipynb</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#enhancements-in-network-design","title":"Enhancements in Network Design","text":"<p>Typically, people might think that adding more layers and parameters will improve the model. While this can be true in some cases, it's not the complete solution and often leads to inefficient models. In the following subsections, we will explore techniques to enhance the results and inference speed.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#conditional-network-warm-up","title":"Conditional Network Warm-Up","text":"<p>To enforce the conditioned result concordance we use a warm-up strategy for the conditional network. But, this induce overfitting of the UNet over the ground truth. To tackle this problem, after freezing the parameters of the conditional network, we apply random masking of labels (context) using a Bernoulli distribution. This approach stabilizes the training process and helps prevent the UNet from overfitting to the conditional network.</p> <p>I\u2019ve chosen an arbitrary warm-up value of 80 for demonstration purposes in this tutorial. This may not be the optimal value for all scenarios. In real-world applications, it is advisable to monitor model performance and adjust the warm-up step accordingly to determine the best time to freeze the conditional network.</p> <p>These adjustments are implemented by incorporating the following two conditions into our training loop:</p> <pre><code>if epoch &gt;= warmup:\n    warmup_done = True\n    # freeze all layers of the embedding network\n    for param in emb_net.parameters():\n        param.requires_grad = False\n</code></pre> <pre><code>if warmup_done:\n    # randomly mask out labels (context)\n    context_mask = torch.bernoulli(torch.zeros(labels.shape[0]) + 0.95).to(unet.device)\n    labels = labels * context_mask.unsqueeze(-1)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#network-architecture-improvements","title":"Network Architecture Improvements","text":"","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#unet-enhancements","title":"UNet Enhancements","text":"<p>Adding both a down block and an up block to the UNet architecture has proven effective. My experiments showed that omitting these blocks led to a decline in the quality of generated images.</p> <p>Therefore, we modify our UNet like follows:</p> <pre><code>unet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,\n    sample_size=16,\n    in_channels=3,\n    out_channels=3,\n    layers_per_block=2,\n    block_out_channels=(64, 128, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\"),  # One more down block\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),  # One more up block\n)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#conditional-network-bottleneck","title":"Conditional Network Bottleneck","text":"<p>Implementing a bottleneck in the conditional network enforces better embedding quality, which in turn enhances overall performance.</p> <p>As a result, our embedding network becomes:</p> <pre><code>emb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size//2), # bottleneck to force better embeddings quality\n    nn.GELU(),\n    nn.Linear(class_emb_size//2, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#regularization-techniques","title":"Regularization Techniques","text":"<p>Regularization is essential for improving model generalization and preventing overfitting:</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#dropout","title":"Dropout","text":"<p>We apply dropout to both the UNet and the embedding network to reduce overfitting and improve model robustness.</p> <p>This results in the following modifications:</p> <pre><code>emb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size//2),\n    nn.Dropout(0.1), # add of dropout to regularize the model\n    nn.GELU(),\n    nn.Linear(class_emb_size//2, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n\n# Define the UNet model\nunet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,\n    sample_size=16,\n    in_channels=3,\n    out_channels=3,\n    layers_per_block=2,\n    block_out_channels=(64, 128, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n    dropout=0.2, # add of dropout to regularize the model\n)\n</code></pre> <p>Note that we avoid applying Dropout to the output of our embedding network to prevent issues, such as optimizing zeroed embedding values from Dropout, which can affect the attention mechanism in our UNet.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#adamw-optimizer","title":"AdamW Optimizer","text":"<p>We use the AdamW optimizer, which incorporates weight decay. This technique aids in better regularization by penalizing large weights.</p> <p>So set this up, we have to change an import:</p> <pre><code>from torch.optim import AdamW\n</code></pre> <p>Also change the optimizer creation and using a <code>weight_decay</code>, here I chose an arbitrary value that works well in most cases but deserve to be chosen more carefully if you have the computations capacities:</p> <pre><code>optimizer = AdamW(chain(unet.parameters(), emb_net.parameters()), lr=lr, weight_decay=1e-4)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#scheduler-and-loss-function-adjustments","title":"Scheduler and Loss Function Adjustments","text":"","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#ddim-vs-ddpm","title":"DDIM vs. DDPM","text":"<p>DDIM (Denoising Diffusion Implicit Models) offers several advantages over DDPM (Denoising Diffusion Probabilistic Models). Unlike DDPM, which relies on a Markov chain to sequentially denoise samples, DDIM bypasses the explicit Markov process by using a non-Markovian approach that allows for fewer diffusion steps. This results in faster and more efficient sampling while maintaining or even improving the quality of the generated images.</p> <p>Switching from DDPM to DDIM is straightforward; we simply need to start by replacing the noise scheduler creation:</p> <pre><code># Use of DDIMScheduler instead of DDPMScheduler\nnoise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n</code></pre> <p>Since the API of the <code>DDIMScheduler</code> is similar to that of the <code>DDPMScheduler</code>, there's no need to modify the code related to the <code>noise_scheduler</code>. However, we do need to rewrite our pipeline, though it closely resembles our custom <code>DDPMPipeline</code>.</p> <pre><code>class ConditionalDDIMPipeline(DDIMPipeline):\n    def __init__(\n        self, unet: UNet2DConditionModel, class_net: CustomSequential, scheduler: DDIMScheduler\n    ) -&gt; None:\n        super().__init__(unet=unet, scheduler=scheduler)\n        self.class_net = class_net\n        self.class_net.eval()\n        self.register_modules(class_net=class_net)\n        self.unet.eval()\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        class_label: list[list[float]],\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        eta: float = 0.0,\n        num_inference_steps: int = 1000,\n        use_clipped_model_output: Optional[bool] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -&gt; Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            class_label (list[list[float]]):\n                list of one-hot examples. len(class_label) represents the number of examples to generate.\n            generator (`torch.Generator`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers. A value of `0` corresponds to\n                DDIM and `1` corresponds to DDPM.\n            num_inference_steps (`int`, *optional*, defaults to 1000):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            use_clipped_model_output (`bool`, *optional*, defaults to `None`):\n                If `True` or `False`, see documentation for [`DDIMScheduler.step`]. If `None`, nothing is passed\n                downstream to the scheduler (use `None` for schedulers which don't support this argument).\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n                returned where the first element is a list with the generated images\n        \"\"\"\n        batch_size = len(class_label)\n        # Sample gaussian noise to begin loop\n        if isinstance(self.unet.config.sample_size, int):\n            image_shape = (\n                batch_size,\n                self.unet.config.in_channels,\n                self.unet.config.sample_size,\n                self.unet.config.sample_size,\n            )\n        else:\n            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n\n        image = randn_tensor(image_shape, generator=generator, device=self._execution_device, dtype=self.unet.dtype)\n\n        # get encoded ground truth\n        labels = torch.tensor(class_label, device=self.device)\n        with autocast(str(self.unet.device)):\n            enc_labels = self.class_net(labels)\n\n            # set step values\n            self.scheduler.set_timesteps(num_inference_steps)\n\n            for t in self.progress_bar(self.scheduler.timesteps):\n                # 1. predict noise model_output\n                model_output = self.unet(image, t, enc_labels, class_labels=labels, return_dict=False)[\n                    0\n                ]\n\n                # 2. predict previous mean of image x_t-1 and add variance depending on eta\n                # eta corresponds to \u03b7 in paper and should be between [0, 1]\n                # do x_t -&gt; x_t-1\n                image = self.scheduler.step(\n                    model_output, t, image, eta=eta, use_clipped_model_output=use_clipped_model_output, generator=generator\n                ).prev_sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image,)\n\n        return ImagePipelineOutput(images=image)\n</code></pre> <p>We modified the inheritance and added the <code>eta</code> parameter to ensure compatibility with the <code>DDPMScheduler</code>. With this scheduler, I was able to generate high-quality samples in just 10 steps, instead of 100, significantly improving the inference speed of our final model by over 10 times, as we also employ a non-Markovian approach.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#loss-function","title":"Loss Function","text":"<p>We switch from L2 loss to L1 loss. L1 loss, or Mean Absolute Error (MAE), is less sensitive to outliers compared to L2 loss (Mean Squared Error), making it more robust and stable during training. This robustness helps in generating clearer and more visually coherent images by reducing the influence of extreme values and ensuring stable gradient updates.</p> <p>In our previous code, this requires modifying the loss function computation:</p> <pre><code># Compute l1 loss instead of mse, it provides better robustness to noises\n# which is suited for diffusion models, but the cost is slower training\nloss = torch.nn.functional.l1_loss(predicted_noise, noise)\n</code></pre>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#training-enhancements","title":"Training Enhancements","text":"","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#increased-epochs","title":"Increased Epochs","text":"<p>With the aforementioned modifications, we have increased the number of training epochs to 100. This ensures that the model is trained thoroughly and can better leverage our previous modifications.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#torch-compile","title":"Torch Compile","text":"<p>To speed up training we already used mixed precision. But, we can go further using <code>torch.compile</code>. This results in a 30% reduction in training time. Note that <code>torch.compile</code> was not used for the final pipeline, as we required a single forward pass for evaluation. However, <code>torch.compile</code> offers significant efficiency benefits for serving models.</p> <p>To compile our networks, we need to add the following lines:</p> <pre><code># compile networks for faster inferences even for training\nunet = torch.compile(unet, fullgraph=True)\nemb_net = torch.compile(emb_net, fullgraph=True)\n</code></pre> <p>For those interested in achieving even faster results through techniques like quantization and distillation, I recommend checking out the Hugging Face tutorial on fast diffusion.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-08-diffusers-obtain-better-results/#final-result","title":"Final result","text":"<p>Before presenting the final results, I\u2019d like to set some realistic expectations. With the current dataset (excluding poor examples), we can expect good results. However, keep in mind that we are working with only five categories in our ground truth, and the examples within each category are quite heterogeneous. As a result, while the model will learn to distinguish between the categories, it won't capture the finer \"sub-categories,\" limiting our control over them. This lack of homogeneity within categories leaves room for the ground truth to be more easily mixed.</p> <p>Now, let's take a look at some samples from each class:</p> <p></p> <p>As you can see, the images appear quite polished, but many of the generated results are nearly identical to some training examples. You might notice subtle differences between the originals and generated samples (especially with small details like for flames). To better control the subcategories, we could enhance the ground truth by subdividing the existing categories. This task is labor-intensive if done manually. A more efficient approach is to automate it by applying algorithms like k-means in the encoded space of our UNet over the training samples and assigning names to the resulting categories. This would make the mixing of categories more meaningful and allow for more flexibility in tweaking the embeddings.</p> <p>However, since I need to set limits for this tutorial to avoid it becoming overly extensive, I\u2019m marking this part as out of scope.</p> <p>I hope this post helps and/or inspires some of you. If you have suggestions feel free to comment or reach me.</p> <p>See you again, Vincent.</p>","tags":["Development","Generation","Machine Learning","Python"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/","title":"Website migration to MkDocs","text":"<p>After maintaining this website using Jekyll for a couple of years, I decided it was time for a change. As much as I enjoyed the flexibility Jekyll offered, the needs of my projects had evolved. I had already been using MkDocs for documenting Python projects, so I decided to consolidate everything under MkDocs. This post will explain the rationale behind this migration, the benefits, some trade-offs, and why I switched from Disqus to Giscus for managing comments.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#why-move-from-jekyll-to-mkdocs","title":"Why Move from Jekyll to MkDocs?","text":"<p>Jekyll is a fantastic static site generator, especially if you're hosting on GitHub Pages. It provides great support for blog posts, themes, and plugins, but it wasn\u2019t meeting all my needs anymore. I did not have a search bar, light dark themes and my website felt overall hacky. Maintaining separate platforms for blogging and documentation started to feel counterproductive. MkDocs, a static site generator designed primarily for documentation, seemed like the perfect alternative to unify everything.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#benefits-of-using-mkdocs-for-my-site","title":"Benefits of Using MkDocs for My Site","text":"","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#integrated-documentation","title":"Integrated Documentation","text":"<p>MkDocs is built for documentation, making it easier to make tutorials with blocks for notes, search bar, navigation features. This integration allows me to work seamlessly between my blogs and technical write-ups without switching frameworks.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#simplified-workflow","title":"Simplified Workflow","text":"<p>MkDocs is incredibly simple. With its Markdown-based structure, there's no need to deal with complex templating like with Jekyll. I can write everything in Markdown, which significantly speeds up content creation.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#multilingual-support-with-ease","title":"Multilingual Support with Ease","text":"<p>One of my core requirements was to support both English and French. While Jekyll does provide ways to handle multilingual websites, MkDocs has plugins, such as mkdocs-static-i18n, that simplify managing translations for pages. It's also easier to keep track of language-specific documentation without complicated folder structures or configurations.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#mkdocs-material-theme","title":"MkDocs Material Theme","text":"<p>The Material theme for MkDocs provides an elegant, modern look right out of the box. With a few tweaks, I was able to make the blog look visually appealing without spending much time on custom CSS or theme development, unlike Jekyll where I often found myself tinkering with themes to get things right.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#cons-of-switching-to-mkdocs","title":"Cons of Switching to MkDocs","text":"","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#fewer-plugins-for-blogging","title":"Fewer Plugins for Blogging","text":"<p>Jekyll has a rich ecosystem of plugins specifically designed for bloggers, while MkDocs has fewer options. For example, managing a series of posts or custom taxonomies is more mature in Jekyll, while MkDocs requires some workarounds to achieve similar functionality. Nevertheless, I never used this functionality, so it is not a huge loss for me.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#limited-theme-flexibility","title":"Limited Theme Flexibility","text":"<p>MkDocs is more rigid in terms of theming compared to Jekyll. While the Material theme looks great, it doesn\u2019t allow the same level of customization that Jekyll themes do without delving into custom CSS. This is a big plus for me, as I spent too much time tweaking Jekyll's default theme, only to achieve somewhat average results.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#giscus-instead-of-disqus","title":"Giscus Instead of Disqus","text":"<p>One of the more technical changes I made was switching from Disqus to Giscus for comment handling. Here\u2019s why:</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#privacy-and-performance","title":"Privacy and Performance","text":"<p>Disqus, while popular, comes with a cost to privacy and page performance. It injects ads and loads several third-party trackers, which results in slower page loads. Giscus, on the other hand, is lightweight and leverages GitHub Discussions for comments. It\u2019s fast, doesn\u2019t serve ads, and aligns more with my privacy-focused approach.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#github-integration","title":"GitHub Integration","text":"<p>Since I already use GitHub for version control and collaboration on various projects, Giscus seamlessly integrates comments into GitHub Discussions. This creates a unified space where users can not only leave comments but also engage in discussions related to specific blog posts.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#open-source","title":"Open Source","text":"<p>Giscus is open source, which aligns with my preference for using open and transparent tools wherever possible. Disqus\u2019s proprietary nature and ad-driven model had been a drawback for me for a while, so Giscus was a natural choice when looking for alternatives.</p>","tags":["Development"]},{"location":"blog/2024/09-25-website-migration-to-mkdocs/#final-thoughts","title":"Final Thoughts","text":"<p>Switching from Jekyll to MkDocs has streamlined my workflow, making it easier to manage both website and my blog. While MkDocs might not be the go-to choice for traditional bloggers, its simplicity and focus on documentation make it a powerful tool for technical content creators like myself. The transition has allowed me to simplify my setup and keep everything more organized.</p> <p>For those who run documentation-heavy blogs or have technical projects that need detailed documentation, I highly recommend trying out MkDocs (especially with the Material theme). And if you\u2019re looking for a fast, privacy-respecting comment system, Giscus is a perfect fit.</p> <p>I hope this post helps and/or inspires some of you. If you have suggestions feel free to comment or reach me.</p> <p>See you again, Vincent.</p>","tags":["Development"]},{"location":"fr/","title":"\u00c0 propos de moi","text":"<p>Scientifique des donn\u00e9es et de l'apprentissage automatique.</p> <p>Docteur en informatique en utilisant l'apprentissage automatique (en particulier les technologies de r\u00e9seaux neuronaux). Je travaille principalement sur les signaux d'image et audios (parole, musique, animaux, etc.). Je suis un programmeur Python et un passionn\u00e9 de logiciels libres.</p>"},{"location":"fr/#resume","title":"R\u00e9sum\u00e9","text":"<p>Docteur en science des donn\u00e9es depuis 2022 (pour plus d'informations, c'est par ici). Mes travaux se focalisent sur le traitement de signaux 1D et 2D, avec une plus grande exp\u00e9rience sur le traitement audio. Je travaille sur des corpus \u00e0 quantit\u00e9 limit\u00e9e, mais \u00e9galement avec de larges bases de donn\u00e9es. Je me concentre sur les algorithmes d'apprentissage automatique pour mod\u00e9liser les connaissances et les int\u00e9grer dans des microservices (\u00e0 base de dockers). Mon langage de programmation de pr\u00e9dilection est Python, il me permet de me concentrer sur les fonctionnalit\u00e9s (ainsi que de profiter d'un large panel de librairies). Ayant l'\u00e2me d'un libriste, je participe \u00e0 des projets open source (reports de bug, propositions de patch, discussions, etc.) lorsque cela m'est possible. Il m'arrive par ailleurs de r\u00e9aliser l'analyse de donn\u00e9es pour produire des visualisations. La plupart d'entre elles sont disponibles sur mon blogue (et en avant-premi\u00e8re sur Reddit).</p> <p>Enfin, je tiens aussi un blogue sur des visualisations que je r\u00e9alise, sur la recherche que j'effectue, des tutoriels en apprentissage profond et sur des astuces. C'est par ici.</p> <p>Vous pouvez retrouver mon r\u00e9sum\u00e9 au complet ici ou aller sur mon LinkedIn.</p>"},{"location":"fr/contact/","title":"Me contacter","text":"<p>Il existe plusieurs fa\u00e7ons de me contacter :</p> <ul> <li> Envoyez-moi un email pour des demandes professionnelles.</li> <li> LinkedIn pour des contacts professionnels et des articles de blog.</li> <li> Reddit pour des discussions informelles ou des publications.</li> </ul>"},{"location":"fr/blog/","title":"Bienvenue dans mon blogue","text":"<p>Ce blog propose des articles sur l'apprentissage machine, des visualisations que j'ai cr\u00e9\u00e9es, ainsi que des astuces de programmation et des conseils sur l'environnement Linux.</p> <p>Tous les articles sont accessibles depuis la barre lat\u00e9rale \u00e0 gauche, class\u00e9s par date avec les plus r\u00e9cents en t\u00eate de liste. Sur les petits \u00e9crans et smartphones, utilisez le menu hamburger pour naviguer.</p> <p>Vous pouvez \u00e9galement rechercher un article par son tag ou utiliser les cat\u00e9gories ci-dessous :</p>  Automatisation Apprentissage Automatique Visualisations <p>J'ai \u00e9crit plusieurs articles sur l'automatisation de diverses t\u00e2ches pendant le d\u00e9veloppement de solutions. Voici quelques uns :</p> <ul> <li>Sauvegarder vos identifiants Git pour \u00e9viter de retaper votre mot de passe \u00e0 chaque fois.</li> <li>Automatisation avec des cl\u00e9s SSH, o\u00f9 nous explorons des t\u00e2ches utiles \u00e0 automatiser avec des cl\u00e9s SSH.</li> <li>Travailler \u00e0 distance ou depuis chez soi, expliquant comment configurer vos postes de travail pour un acc\u00e8s \u00e0 distance.</li> </ul> <p>J'ai cr\u00e9\u00e9 des tutoriels sur la g\u00e9n\u00e9ration d'images en utilisant la biblioth\u00e8que Diffusers :</p> <ol> <li>G\u00e9n\u00e9rer des images avec un mod\u00e8le non conditionn\u00e9.</li> <li>G\u00e9n\u00e9rer des images \u00e0 partir d'\u00e9tiquettes avec un mod\u00e8le conditionnel.</li> <li>Am\u00e9liorer la vitesse et la qualit\u00e9 de la g\u00e9n\u00e9ration d'images.</li> </ol> <p>En outre, j'ai r\u00e9dig\u00e9 un article sur comment acc\u00e9l\u00e9rer l'entra\u00eenement sur des serveurs AI.</p> <p>Pendant mon doctorat, j'ai r\u00e9alis\u00e9 une enqu\u00eate sur les techniques de few-shot pour le traitement audio. Consultez-la ici.</p> <p>Pendant mon temps libre, je cr\u00e9e des visualisations, notamment pour les tournois d'Age of Empires 2, qui ont accumul\u00e9 plus de 100 000 vues. Voici quelques-uns de mes articles :</p> <ul> <li>Red Bull Wololo Legacy</li> <li>Warlords</li> <li>The Grand Melee</li> </ul> <p>J'ai \u00e9galement pr\u00e9sent\u00e9 une enqu\u00eate sur les outils de visualisation de donn\u00e9es en Python pour l'association Toulouse Dataviz.</p> <p>Pour ne manquer aucune publication, abonnez-vous \u00e0 mon flux Atom/RSS et/ou \u00e0 mes r\u00e9seaux sociaux.</p>"},{"location":"fr/blog/2019/09-08-hello-world/","title":"Bonjour le monde","text":"<p>Ceci est mon premier article de blog pour me pr\u00e9senter et parler du contenu de ce blog.</p> <p>Je m'appelle Vincent, je suis un \u00e9tudiant en th\u00e8se \u00e0 l'IRIT (laboratoire d'informatique de Toulouse) au moment o\u00f9 j'\u00e9cris ces lignes. J'utilise les techniques d'apprentissage par r\u00e9seaux profonds sur des signaux de paroles dans un contexte o\u00f9 j'ai tr\u00e8s peu de donn\u00e9es de disponibles. Dans mon blogue, je partage mes lectures, papiers, astuces sur l'apprentissage, programmation (j'utilise principalement Python, LaTeX et le Markdown) et l'environnement Linux en g\u00e9n\u00e9ral.</p> <p>Restez donc \u00e0 l'affut, j'esp\u00e8re que ce blog aidera mes lecteurs.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives"]},{"location":"fr/blog/2019/09-09-save-git-credentials/","title":"Sauver les identifiants git","text":"","tags":["Automatisation","D\u00e9veloppement"]},{"location":"fr/blog/2019/09-09-save-git-credentials/#quest-ce-que-les-identifiants-git","title":"Qu'est-ce que les identifiants git?","text":"<p>Les identifiants git sont le nom d'utilisateur et le mot de passe associ\u00e9 que vous utilisez pour vous connecter sur un serveur git. Les sauvegarder vous permet d'\u00e9viter de les taper lorsqu'ils sont requis (principalement lors d'un push ou d'un pull).</p>","tags":["Automatisation","D\u00e9veloppement"]},{"location":"fr/blog/2019/09-09-save-git-credentials/#pourquoi-sauver-ces-identifiants","title":"Pourquoi sauver ces identifiants?","text":"<p>L'utilisation de cl\u00e9s SSH ou m\u00eame GPG est disponible sur la plupart des serveurs git (tel GitLab ou m\u00eame sur GitHub). Cela permet \u00e9galement de se connecter sans taper son nom d'utilisateur et son mot de passe.</p> <p>Certains sites internet ne supportent pas cette fonctionnalit\u00e9 (les serveurs git d'overleaf par exemple). Aussi, si vous \u00eates comme moi, vous utilisez une multitude de serveurs git (et configurer chaque cl\u00e9 SSH devient lourd).</p>","tags":["Automatisation","D\u00e9veloppement"]},{"location":"fr/blog/2019/09-09-save-git-credentials/#sauver-ses-identifiants","title":"Sauver ses identifiants","text":"<p>Pour sauver ses identifiants pour chaque d\u00e9p\u00f4t auquel vous vous connectez, il suffit de taper juste avant:</p> <pre><code>git config credential.helper store\n</code></pre> <p>Et c'est tout. Vous allez remplir seulement une fois vos identifiants pour chaque d\u00e9p\u00f4t. Les prochaines fois, ils seront remplis automatiquement.</p>","tags":["Automatisation","D\u00e9veloppement"]},{"location":"fr/blog/2019/09-09-save-git-credentials/#le-cache-dexpiration","title":"Le cache d'expiration","text":"<p>Pour des raisons de s\u00e9curit\u00e9 (surtout si vous n'\u00eates pas sur votre ordinateur personnel), il est pr\u00e9f\u00e9rable de sauvegarder vos identifiants pour un temps limit\u00e9. Disons qu'un d\u00e9lai de 4 heures vous semble appropri\u00e9. Il vous suffit donc taper cette commande:</p> <pre><code>git config --global credential.helper 'cache --timeout 14400'\n</code></pre> <p>Note</p> <p>14400 est le temps en secondes (nos 4 heures)</p> <p>J'esp\u00e8re que cela aidera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Automatisation","D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/","title":"Multiplexeur de terminaux","text":"","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#quest-quun-multiplexeur-de-terminaux","title":"Qu'est qu'un multiplexeur de terminaux?","text":"<p>Un multiplexeur de terminaux est un outil permettant d'avoir plusieurs sessions/fen\u00eatres dans un seul terminal. C'est utile pour partager un \u00e9cran de terminal en plusieurs invit\u00e9s de commandes et/ou d'avoir une multitude d'invit\u00e9s de commandes accessibles \u00e0 partir d'un seul terminal. L'utilisation la plus int\u00e9ressante est d'utiliser les multiplexeurs de terminaux sur un serveur/ordinateur distant pour r\u00e9aliser des calculs (par exemple apprendre un mod\u00e8le sur GPU). En effet, \u00e0 travers une connexion ssh vous ne pouvez pas vous d\u00e9connecter sans tuer le processus r\u00e9alisant le calcul. Tandis que le multiplexeur de terminaux permet de garder une session active m\u00eame apr\u00e8s d\u00e9connexion. Il vous suffit de d\u00e9tacher la session du multiplexeur avant de vous d\u00e9connecter. Apr\u00e8s vous \u00eatre reconnect\u00e9 via ssh, il vous suffit de vous rattacher \u00e0 la session effectuant votre calcul.</p> <p>J'utilise \u00e9norm\u00e9ment le multiplexeur de terminaux sur mes machines personnelles et sur des serveurs distants. Cela me donne un environnement homog\u00e8ne sur l'ensemble des machines auxquelles j'ai acc\u00e8s. Dans cet article de blogue, je vais parler du multiplexeur Tmux. Nous verrons comment l'installer, le configurer et l'utiliser.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#tmux","title":"Tmux","text":"<p>J'utilise g\u00e9n\u00e9ralement une session de multiplexage simultan\u00e9ment. N\u00e9anmoins l'utilisation de multiples sessions est possible avec la possibilit\u00e9 d'avoir un profil diff\u00e9rent par session. Pour plus de d\u00e9tails, je vous recommande de regarder le projet tmuxp. Dans cet article de blogue, je vais exclusivement me focaliser sur la configuration de Tmux et de son utilisation.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#pour-ubuntu","title":"Pour Ubuntu","text":"<pre><code>sudo apt install tmux\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#pour-fedora","title":"Pour Fedora","text":"<pre><code>sudo dnf install tmux\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#configuration","title":"Configuration","text":"<p>Toutes les configurations de tmux doivent \u00eatre mises dans le fichier ~/.tmux.conf. Dans cette partie, je vais d\u00e9tailler ma configuration et comment l'installer. Elle peut \u00eatre trouv\u00e9e ici. Je mettrai \u00e0 jour cette partie lorsque celle-ci \u00e9voluera.</p> <p>Avant, observons le r\u00e9sultat attendu:</p> <p>Je n'ai pas \u00e9norm\u00e9ment chang\u00e9 la configuration par d\u00e9faut, on retrouve la barre de status en bas. Cette derni\u00e8re contient:</p> <ul> <li>la fen\u00eatre courante en bleue (les autres sont en vert)</li> <li>en bas \u00e0 droite se trouve l'utilisation globale en RAM CPU et GPU.</li> </ul>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#installation_1","title":"Installation","text":"<p>Mes fichiers de configuration sont compos\u00e9s en deux parties: des scripts bash pour r\u00e9cup\u00e9rer des informations syst\u00e8me et un fichier de configuration Tmux. Pour installer ma configuration, vous devez t\u00e9l\u00e9charger ces fichiers. Ensuite il vous faut taper:</p> <pre><code>cp -r tmux_scripts ~/.tmux_scripts\ncp tmux.conf ~/.tmux.conf\n</code></pre> <p>Mes scripts de r\u00e9cup\u00e9ration d'informations syst\u00e8me sont les suivants:</p> <ul> <li>le script <code>gpu.sh</code> utilise la commande nvidia-smi (que vous devez installer quand vous installez vos pilotes graphiques) pour obtenir la quantit\u00e9 de RAM GPU utilis\u00e9e.   ce script prend l'id du GPU en param\u00e8tre (le premier ayant le num\u00e9ro 0).   \u00c0 vous d'ajouter d'autres appels \u00e0 ce script (dans l'option status-right du fichier tmux.conf) pour monitorer plusieurs GPUs.   Si vous voulez am\u00e9liorer ce comportement, n'h\u00e9sitez pas \u00e0 contribuer.</li> <li>le script <code>ram.sh</code> permet de r\u00e9cup\u00e9rer l'utilisation de la RAM CPU par la machine.</li> </ul> <p>Le fichier de configuration de tmux active l'int\u00e9gration avec la souris (pour utiliser la molette, s\u00e9lectionner l'invit\u00e9 de commande et redimensionner les fen\u00eatres adjacentes) Il modifie \u00e9galement certaines couleurs et les informations affich\u00e9es dans la barre de status. La barre de status est mise \u00e0 jour toutes les 10 secondes. Allez voir le livre The Tao of tmux pour personnaliser votre tmux \u00e0 vos besoins.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#utilisation","title":"Utilisation","text":"<p>Avant tout, d\u00e9marrons une session. Dans votre terminal (ou \u00e9mulateur de terminal) tapez ceci:</p> <pre><code>tmux\n</code></pre> <p>Actuellement, les raccourcis par d\u00e9faut combin\u00e9s \u00e0 la souris me conviennent. Dans cet article, je mentionne uniquement les raccourcis que j'utilise couramment. Pour utiliser les raccourcis de tmux, il vous faut tout d'abord appuyer sur la combinaison de touche <code>&lt;Ctrl+b&gt;</code> et ensuite taper un des raccourcis suivants:</p> <ul> <li><code>\"</code> pour partager la fen\u00eatre courante horizontalement.</li> <li><code>%</code> pour partager la fen\u00eatre courante verticalement.</li> <li><code>z</code> pour zoomer sur une des sous-fen\u00eatres.</li> <li><code>c</code> pour cr\u00e9er un nouvel onglet dans votre multiplexeur.</li> <li><code>d</code> pour d\u00e9tacher votre session.</li> </ul> <p>Je r\u00e9alise toutes mes autres actions (s\u00e9lectionner un onglet, redimensionner une fen\u00eatre...) \u00e0 l'aide de ma souris. Pour rattacher tmux \u00e0 la session pr\u00e9c\u00e9dente, il vous faut taper:</p> <pre><code>tmux attach-session\n</code></pre> <p>Si vous voulez connaitre d'autres raccourcis et savoir comment faire pour g\u00e9rer plusieurs sessions, je vous recommande le livre The Tao of tmux et l'antis\u00e8che en suivant ce lien.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#changement-du-comportement-par-defaut-de-partage-decran","title":"Changement du comportement par d\u00e9faut de partage d'\u00e9cran","text":"<p>\u00c0 chaque fois que je partageais un \u00e9cran en deux, je me suis rendu compte que j'effectuais toujours les m\u00eames commandes <code>cd</code>. Pour pallierr \u00e0 ceci, j'ai ajout\u00e9 les lignes suivantes dans mon fichier <code>tmux.conf</code>:</p> <pre><code>bind % split-window -h -c \"#{pane_current_path}\"\nbind '\"' split-window -v -c \"#{pane_current_path}\"\n</code></pre> <p>Maintenant les nouveaux \u00e9crans s'ouvrent dans le m\u00eame r\u00e9pertoire que le terminal courant.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#bonus-alternative-pour-les-utilisateurs-de-gnome","title":"Bonus: Alternative pour les utilisateurs de Gnome","text":"<p>Si vous voulez un environnement pr\u00e9configur\u00e9 par d\u00e9faut et que vous utilisez Gnome ainsi gnome-terminal, je vous recommande de regarder Byobu. Leur site contient une vid\u00e9o explicative sur leur page d'accueil. Les raccourcis de byobu ne fonctionnent pas avec tous les terminaux (ce qui ne me convient pas vu que j'utilise konsole).</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/09-23-terminal-multiplexers/#sources","title":"Sources","text":"<ul> <li>Tmux man page</li> <li>The Tao of tmux</li> </ul> <p>J'esp\u00e8re que cela aidera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/","title":"V\u00e9rifier vos fautes dans Neovim","text":"<p>Quand j'\u00e9cris du LaTeX, du Markdown ou du Python j'utilise Neovim. Neovim est un logiciel ayant refactor\u00e9 le code de Vim pour \u00eatre plus accessible et plus extensible (tout en restant compatible avec la plupart des plug-ins de Vim). Basiquement, Neovim peut \u00eatre vu comme une version am\u00e9lior\u00e9e de Vim. Allez sur la page officielle du projet pour plus de d\u00e9tails. Apr\u00e8s avoir \u00e9crit votre texte ou documentation, les correcteurs automatiques sont utiles pour vous faire gagner du temps de relecture. Dans cet article de blogue, je vous partage ma configuration Neovim pour faire cette relecture automatique.</p>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#correction-dorthographe","title":"Correction d'orthographe","text":"<p>Pour avoir une v\u00e9rification orthographique dans neovim, vous avez \u00e0 ajouter dans le fichier <code>~/.config/nvim/init.vim</code> les lignes suivantes:</p> <pre><code>\" spell languages\nset spelllang=fr\nnnoremap &lt;silent&gt; &lt;C-s&gt; :set spell!&lt;cr&gt;\ninoremap &lt;silent&gt; &lt;C-s&gt; &lt;C-O&gt;:set spell!&lt;cr&gt;\n</code></pre>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#utilisation","title":"Utilisation","text":"<p>Une fois dans Neovim, appuyer sur Ctrl+S pour mettre en \u00e9vidence les mots mal orthographi\u00e9s. Il vous suffit de rappuyer sur Ctrl+S pour d\u00e9sactiver cette fonctionnalit\u00e9. Pour avoir acc\u00e8s \u00e0 une suggestion de mots, il vous faut appuyer Ctrl+X puis S lorsque vous \u00eates en mode insertion. En mode normal, taper <code>z=</code> lorsque le curseur est sur un mot surlign\u00e9 pour afficher la suggestion de mots</p> <p>Note</p> <p>Je sais que ma configuration inhibe le signal utilisateur Ctrl+S et c'est justement mon objectif, car je veux \u00e9viter toute interruption lorsque je suis dans Neovim.</p>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#correction-de-grammaire","title":"Correction de grammaire","text":"<p>Pour faire une premi\u00e8re relecture, j'utilise la suite LanguageTool. Elle est disponible gratuitement en version hors-ligne et en ligne. Le code de l'outil de correction hors-ligne est ouvert (c'est la raison principale de mon utilisation). Dans cet article, je vais vous aider \u00e0 l'installer et vous aider \u00e0 l'utiliser avec Neovim.</p>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#installation-de-languagetool","title":"Installation de LanguageTool","text":"<p>LanguageTool n\u00e9cessite un environnement Java pour fonctionner. Ici je choisis OpenJDK comme environnement Java. Puis j'installe curl pour t\u00e9l\u00e9charger LanguageTool.</p>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#pour-fedora","title":"Pour Fedora","text":"<pre><code>sudo dnf -y install java curl\n</code></pre>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#pour-ubuntu","title":"Pour Ubuntu","text":"<pre><code>sudo apt -y install default-jre curl\n</code></pre>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#ensuite-sur-les-deux-distributions","title":"Ensuite, sur les deux distributions","text":"<p>Les commandes qui suivent t\u00e9l\u00e9chargent LanguageTool et l'installe dans <code>/usr/local</code>:</p> <pre><code>curl https://languagetool.org/download/LanguageTool-stable.zip &gt; /tmp/LanguageTool-stable.zip\nsudo unzip /tmp/LanguageTool-stable.zip -d /usr/local/LanguageTool\nsudo mv /usr/local/LanguageTool/LanguageTool-*/* /usr/local/LanguageTool/\nrm /tmp/LanguageTool-stable.zip\n</code></pre>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#configuration-de-neovim","title":"Configuration de Neovim","text":"<p>J'utilise le manager de plug-in vim-plug pour Neovim. C'est pourquoi je vais utiliser ce manager pour installer le plug-in de vigoux (l'utilisation d'autre manager est possible). Avant de continuer, je vous recommande d'utiliser une version r\u00e9cente de Neovim (\u2265 0.4.2) pour utiliser ce plug-in (puisqu'il utilise des changements r\u00e9cents faits dans Neovim).</p> <p>Dans votre <code>~/.config/nvim/init.vim</code> entre l'appel <code>call plug#begin(&lt;plugging_folderpath&gt;)</code> et l'appel <code>call plug#end()</code> vous devez rajouter les lignes suivantes:</p> <pre><code>\" Language tool integration\nPlug 'vigoux/LanguageTool.nvim'\n</code></pre> <p>Ensuite, en dehors des appels \u00e0 vim-plug vous devez ajouter les lignes suivantes:</p> <pre><code>\" grammar checking\nautocmd Filetype markdown LanguageToolSetUp\nautocmd Filetype tex LanguageToolSetUp\nlet g:languagetool_server_jar='/usr/local/LanguageTool/languagetool-server.jar'\n</code></pre>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-08-check-your-writing-in-nvim/#utilisation_1","title":"Utilisation","text":"<p>Pour utiliser ce plug-in, vous devez faire un appel \u00e0 lui dans vos documents en mode normal gr\u00e2ce \u00e0 la commande <code>:LanguageToolSetUp</code> (cette commande est lanc\u00e9e automatiquement pour les fichiers tex et markdown).</p> <p>Ensuite, le plug-in propose les commandes suivantes:</p> <ul> <li><code>:LanguageToolCheck</code>: pour surligner les erreurs d\u00e9tect\u00e9es, vous devez r\u00e9utiliser cette commande lorsque vous changer votre code/texte.</li> <li><code>:LanguageToolSummary</code>: pour afficher des d\u00e9tails sur les mots surlign\u00e9s dans une sous-fen\u00eatre.</li> <li><code>:LanguageToolClear</code>: pour enlever les affichages de LanguageTool.</li> </ul> <p>Pour plus d'options allez sur la page du plug-in.</p> <p>Note</p> <p>J\u2019ai souvent altern\u00e9 entre l\u2019utilisation de LanguageTool et Antidote. Finalement, j\u2019ai choisi LanguageTool car il s\u2019int\u00e8gre parfaitement \u00e0 mon environnement de d\u00e9veloppement et \u00e0 mes workflows.</p> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous. Si vous avez une meilleure configuration ou voulez am\u00e9liorer la mienne, n'h\u00e9sitez pas \u00e0 contribuer.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["D\u00e9veloppement","\u00c9criture"]},{"location":"fr/blog/2019/10-12-first-year-review-as-a-phd-student/","title":"Revue de ma premi\u00e8re ann\u00e9e de th\u00e8se","text":"<p>Apr\u00e8s presque une ann\u00e9e (11 mois), je vais partager dans cet article mon exp\u00e9rience en tant qu'\u00e9tudiant en th\u00e8se. Dans cet article de blogue, je vais parler du sujet de ma th\u00e8se, la direction prise, des travaux futurs et des changements dans ma vie (li\u00e9s \u00e0 la th\u00e8se).</p> <p>Avant de continuer, je voudrais remercier mes encadrants<sup>1</sup> pour leur aide et leurs conseils, La R\u00e9gion Occitanie et l'Universit\u00e9 F\u00e9d\u00e9rale de Toulouse pour leur financement, l'Universit\u00e9 Paul Sabatier et le Laboratoire IRIT pour leur support. Je souhaite \u00e9galement rappeler que c'est un article de blogue personnel (il n'engage que moi-m\u00eame).</p>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2019/10-12-first-year-review-as-a-phd-student/#sujet","title":"Sujet","text":"<p>Ma th\u00e8se consiste \u00e0 cr\u00e9er un Syst\u00e8me Automatique de Mesure de l'Intelligibilit\u00e9 (SAMI). En traitement de la parole, l'intelligibilit\u00e9 peut correspondre \u00e0 plusieurs choses. Avant de d\u00e9finir ce terme, regardons un petit sch\u00e9ma g\u00e9n\u00e9ral correspondant au syst\u00e8me d'\u00e9coute chez l'humain:</p> <p>Dans la communaut\u00e9 de la parole, l'intelligibilit\u00e9 d\u00e9signe g\u00e9n\u00e9ralement que le signal \u00e9cout\u00e9 est 100% clair. Cette mesure sert \u00e0 identifier les perturbations li\u00e9es aux bruits environnementaux. Comme vous l'avez surement devin\u00e9, ce n'est absolument pas la mesure que je cherche durant cette th\u00e8se.</p> <p>Pour cette th\u00e8se, nous nous int\u00e9ressons \u00e0 la clart\u00e9 de l'orateur ind\u00e9pendamment des bruits environnementaux et de toutes connaissances de l'auditeur.</p> <p>Ainsi, par intelligibilit\u00e9 je parle de celle de l'orateur.</p> <p>Ce genre de mesure a plusieurs applications:</p> <ul> <li>l'\u00e9valuation de la prononciation de personnes apprenant des langues \u00e9trang\u00e8res.</li> <li>le raffinement des informations pour les dispositifs auditifs afin aider \u00e0 leur am\u00e9lioration.</li> <li>l'\u00e9valuation des dommages d'une ou plusieurs maladie(s) (tel que Parkinson ou les cancers buccaux) pour aider au processus de r\u00e9\u00e9ducation.</li> </ul> <p>Le dernier point correspond au focus de ma th\u00e8se. Par cons\u00e9quent, mon syst\u00e8me doit fonctionner sur de la parole pathologique et non pathologique.</p> <p>Pour atteindre cet objectif, j'ai acc\u00e8s au corpus C2SI. Il contient environ 2 heures d'enregistrements de plusieurs t\u00e2ches (lecture, parole spontan\u00e9e, A maintenue...). Tous les sujets sont des Fran\u00e7ais et parlent couramment le Fran\u00e7ais. Le corpus C2SI est compos\u00e9 d'enregistrements de patients (souffrant de cancers buccaux) et d'un groupe contr\u00f4le (participants n'ayant pas de trouble de la parole) Plus important encore, dans ce corpus, nous avons acc\u00e8s \u00e0 une \u00e9valuation subjective de l'intelligibilit\u00e9 de chaque participant. Ces scores ont \u00e9t\u00e9 r\u00e9alis\u00e9s par un jury de 5 experts nous donnant un score compris entre 0 et 10. Ce corpus est donc le point de d\u00e9part de ma th\u00e8se.</p>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2019/10-12-first-year-review-as-a-phd-student/#direction-prise-pour-la-these","title":"Direction prise pour la th\u00e8se","text":"<p>Je suis un scientifique de la donn\u00e9e et j'aime particuli\u00e8rement laisser la machine apprendre les concepts pour moi. C'est pour \u00e7a que les encadrants et moi-m\u00eame avons d\u00e9cid\u00e9 de faire apprendre le concept d'intelligibilit\u00e9 \u00e0 l'aide de technique d'apprentissage par machine avec des syst\u00e8mes de bout en bout (bas\u00e9s sur de l'apprentissage profond).</p> <p>Apr\u00e8s quelques exp\u00e9riences sur le corpus C2SI, j'en tire les le\u00e7ons suivantes:</p> <ul> <li>Utiliser les techniques d'apprentissage de l'\u00e9tat de l'art en reconnaissance de la parole (architectures tr\u00e8s profondes et larges) est impossible sur la quantit\u00e9 de donn\u00e9es que j'ai \u00e0 disposition</li> <li>Utiliser des mod\u00e8les avec moins de param\u00e8tres n\u00e9cessaires (avec des techniques comme SincNet et l'utilisation de petites architectures) ne solutionne pas suffisamment bien le probl\u00e8me sur de la parole pathologique.</li> <li>L'apprentissage par transfert (utilisant des mod\u00e8les appris sur de la parole consid\u00e9r\u00e9e non pathologique) r\u00e9sulte en des performances plus faibles que les mod\u00e8les avec moins de param\u00e8tres.</li> </ul> <p>Pour le moment, je ne peux obtenir plus de donn\u00e9es (parce que cela peut \u00eatre douloureux pour les patients) pour obtenir de meilleurs r\u00e9sultats. L'augmentation de donn\u00e9es n'a pas aid\u00e9 \u00e0 obtenir des r\u00e9sultats suffisants. Apr\u00e8s avoir \u00e9cout\u00e9 l'enregistrement de plusieurs participants, je suis maintenant capable de deviner l'intervalle de valeur de leur score obtenu. Cela m'a rappel\u00e9 certaines de mes lectures parlant de techniques qui essayaient d'\u00eatre aussi efficientes que les enfants pour apprendre de nouveaux concepts (en termes de quantit\u00e9 de donn\u00e9es): les techniques dites de few-shot. Comme cela correspond \u00e0 mon probl\u00e8me, j'ai commenc\u00e9 \u00e0 faire une revue de ce domaine. Pour partager cette expertise acquise, j'ai commenc\u00e9 une revue de litt\u00e9rature sur les techniques de few-shot pour le traitement de la parole.</p>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2019/10-12-first-year-review-as-a-phd-student/#a-venir","title":"\u00c0 venir","text":"<p>Mes publications pr\u00e9vues (et futurs articles de blogue) sont les suivants:</p> <ol> <li> <p>Une revue de litt\u00e9rature sur les techniques de few-shot appliqu\u00e9es pour le traitement de la parole.</p> </li> <li> <p>Des exp\u00e9riences utilisant les techniques de few-shot sur le corpus du C2SI.</p> </li> </ol>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2019/10-12-first-year-review-as-a-phd-student/#ressenti-et-impact-sur-mon-mode-de-vie","title":"Ressenti et impact sur mon mode de vie","text":"<p>J'ai arr\u00eat\u00e9 le dessin digital parce que cela me n\u00e9cessitait des sessions de 2 h. J'apprends tout depuis le d\u00e9but dans ce domaine (logiciels, techniques de dessin...). C'est un exercice dur pour moi et qui me prend beaucoup de ressources mentales. \u00c0 la place, j'ai commenc\u00e9 mon site internet et ce blogue vu que j'aime partager. De plus, cela me semble plus simple et plus amusant que je ne l'avais anticip\u00e9 (c'est surtout d\u00fb \u00e0 la simplicit\u00e9 de Jekyll) Je reprendrai surement le dessin digital, mais apr\u00e8s ma th\u00e8se.</p> <p>Actuellement, j'apprends le rock acrobatique avec mon amour (ma Krikri). Je pense qu'avoir trop d'activit\u00e9 physique est n\u00e9faste pour le corps, mais aussi pour le moral. Par cons\u00e9quent, j'ai arr\u00eat\u00e9 de courir.</p> <p>En tant qu'informaticien, je suis souvent derri\u00e8re un bureau. Ainsi, je suis peu expos\u00e9 au soleil et produis ainsi moins de vitamine D que les personnes qui travaillent dehors. Pour compenser, je prends de la vitamine D3. Je vous conseille de regarder ici ou l\u00e0 pour comprendre ma d\u00e9marche. Dans tous les cas, demandez conseil \u00e0 un professionnel. Une autre solution consiste \u00e0 s'exposer plus au soleil (tout en profitant pour se relaxer/m\u00e9diter). Ainsi, j'\u00e9vite un maximum le m\u00e9tro en utilisant ma trottinette (une non \u00e9lectrique \ud83d\ude04).</p> <p>Lorsque je ressens moins de motivation que d'habitude, j'\u00e9coute les sons suivants (cela pourrait vous remonter le moral):</p> <ul> <li>Into the Abyss, by Hilltop Hoods.</li> <li>Here, by Briggs.</li> <li>Fight back, by  Neffex.</li> </ul> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p> <ol> <li> <p>Julien Pinquier, J\u00e9r\u00f4me Farinas et Virginie Woisard \u21a9</p> </li> </ol>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/","title":"Utiliser Zotero pour g\u00e9n\u00e9rer des fichiers BibTeX pour vos papiers","text":""},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/#quest-ce-que-zotero","title":"Qu'est-ce que Zotero?","text":"<p>Zotero est un logiciel permettant de collecter, organiser, partager et synchroniser des papiers de recherche (ou d'autres contenus de recherche). Je pr\u00e9f\u00e8re Zotero \u00e0 d'autres solutions, car mes besoins sont remplis. De plus, le code de Zotero est ouvert et est d\u00e9velopp\u00e9 par une organisation ind\u00e9pendante, \u00e0 but non lucratif et respectant la vie priv\u00e9e.</p>"},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/#installation","title":"Installation","text":"<p>Zotero n'est pas disponible dans les d\u00e9p\u00f4ts d'Ubuntu. Pour installer Zotero, j'utilise Flatpak. Cela permet la mise \u00e0 jour automatique d'applications \u00e0 l'origine non disponible sous Ubuntu.</p> <p>J'utilise le d\u00e9p\u00f4t Flathub avec flatpak, pour le mettre en place vous devez taper:</p> <pre><code>sudo apt install flatpak\nflatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\n</code></pre> <p>Maintenant, installons Zotero:</p> <pre><code>flatpak install flathub org.zotero.Zotero\n</code></pre> <p>Zotero est maintenant install\u00e9 et un raccourci est disponible dans votre environnement de travail.</p>"},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/#extensions","title":"Extensions","text":"<p>Pour \u00eatre capable d'utiliser toutes les fonctionnalit\u00e9s pr\u00e9sent\u00e9es dans cet article de blogue, vous devez installer les extensions suivantes:</p> <ul> <li>L'extension Zotero pour votre navigateur web.</li> </ul> <p>Pour l'installer, allez sur la page d'extension pour Chrome (compatible avec les navigateurs bas\u00e9s sur chromium) ou pour Firefox.</p> <ul> <li>Zotero poss\u00e8de un syst\u00e8me d'extensions, j'en utilise une en particulier: Better BibTeX.</li> </ul> <p>Pour l'installer il vous faut:   1. T\u00e9l\u00e9charger le dernier fichier xpi.   2. Ouvrir Zotero, aller dans <code>Tools</code> -&gt; <code>Add-ons</code>, cliquer sur la roue d'engrenage -&gt; <code>Install Add-on From File...</code> et s\u00e9lectionner le fichier d'extension que vous avez pr\u00e9c\u00e9demment s\u00e9lectionn\u00e9.</p>"},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/#ajouter-un-papier-dans-votre-collection","title":"Ajouter un papier dans votre collection","text":"<p>Avant tout, je vous recommande de cr\u00e9er un compte sur le site de Zotero. Cela vous aidera \u00e0 synchroniser votre librairie sur tous vos appareils (dans mon cas mon ordinateur personnel et professionnel). Pour vous connecter dans l'application, aller dans <code>Edit</code> -&gt; <code>Preferences</code> -&gt; l'onglet <code>Sync</code>.</p> <p>Maintenant, vous pouvez utiliser votre navigateur internet pour chercher vos papiers sur le net (sur arXiv par exemple). Lorsque vous voulez ajouter un papier dans votre collection, cliquer sur l'ic\u00f4ne du connecteur Zotero dans votre navigateur web pendant que vous visualisez le PDF d\u00e9sir\u00e9. Cela va automatiquement t\u00e9l\u00e9charger le papier pour une utilisation hors-ligne et extraire toutes les informations requises pour citer ce papier (Titre, auteurs, date...). Le t\u00e9l\u00e9chargement d'informations peut \u00eatre modifi\u00e9 \u00e0 tout moment si quelque chose ne convient pas.</p>"},{"location":"fr/blog/2019/11-06-using-zotero-to-generate-bibtex/#generer-votre-fichier-bibtex-en-utilisant-zotero","title":"G\u00e9n\u00e9rer votre fichier BibTeX en utilisant Zotero","text":"<p>Avant de g\u00e9n\u00e9rer votre fichier BibTeX, vous pouvez personnaliser le format des cl\u00e9s de citations que vous utiliserez dans votre fichier TeX. Pour effectuer cette personnalisation, allez dans le menu BibTeX: Edit -&gt; Preferences -&gt; l'onglet Better BibTeX. J'utilise le format par d\u00e9faut des cl\u00e9s (car il me convient): <code>[auth:lower][shorttitle3_3][year]</code>. Cela correspond au premier auteur (en minuscule), suivis du titre de l'article (tronqu\u00e9) et l'ann\u00e9e de publication. Si vous voulez modifier ce format, je vous conseille de faire un tour sur la documentation officielle se trouvant ici.</p> <p>Apr\u00e8s avoir fait cela, n'oubliez pas de rafraichir les cl\u00e9s de citations pour chaque \u00e9l\u00e9ment de votre collection (ce n'est pas fait automatiquement, mais la combinaison de touches Ctrl+A est votre amie). M\u00eame si vous voulez garder le format par d\u00e9faut des cl\u00e9s de citations de Better BibTeX, vous devez rafraichir ces cl\u00e9s. Du moins une fois apr\u00e8s l'installation de l'extension, sinon Zotero gardera son format par d\u00e9faut avec des underscores (qu'il vaut mieux \u00e9viter avec LaTeX).</p> <p>Pour g\u00e9n\u00e9rer le fichier BibTeX:</p> <ol> <li> <p>Clic droit sur la collection (ou libraire) contenant tous les papiers que vous voulez dans votre fichier Bib puis cliquez sur <code>Export Collection</code>, comme suivant:      </p> </li> <li> <p>Dans le menu contextuel, s\u00e9lectionnez le format \"Better BibTeX\" (qui utilise un format plus \u00e9pur\u00e9 que le format par d\u00e9faut) et cochez la case \"Keep updated\" pour laisser Zotero mettre \u00e0 jour le fichier r\u00e9sultant lorsqu'une modification/ajout est faite sur votre librairie.</p> <p> </p> </li> </ol> <p>Vous avez maintenant votre fichier Bib que vous pouvez utiliser pour vos papiers avec un format de citation vous convenant.</p> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>"},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/","title":"Am\u00e9liorer votre exp\u00e9rience bash","text":"<p>Cet article de blogue est focalis\u00e9 sur la configuration de bash. J'utilise bash, car c'est l'interpr\u00e9teur de commandes par d\u00e9faut dans la plupart des distributions Linux. Je l'utilise quotidiennement et apr\u00e8s quelques r\u00e9glages il me convient mieux. Dans cet article de blogue, je vais d\u00e9crire mes habitudes et comment je configure mon bash.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#utiliser-le-bashrc-par-defaut-dubuntu","title":"Utiliser le bashrc par d\u00e9faut d'Ubuntu","text":"<p>La configuration par d\u00e9faut d'Ubuntu est adapt\u00e9e \u00e0 l'usage quotidien et je me sers cette configuration comme base pour la mienne. Avant faisons la revue de la configuration de base d'Ubuntu.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#completion-par-defaut","title":"Compl\u00e9tion par d\u00e9faut","text":"<p>La compl\u00e9tion dans le terminal est essentielle pour gagner en productivit\u00e9. L'interpr\u00e9teur de commandes par d\u00e9faut d'Ubuntu est bash. Sous cet interpr\u00e9teur, la compl\u00e9tion s'active par l'appui de la touche tabulation. Cette compl\u00e9tion est activ\u00e9e par d\u00e9faut sur une multitude d'applications, comme la compl\u00e9tion de commandes (<code>apt ins&lt;Tab&gt;</code> donne <code>apt install</code>) ou de dossiers/fichiers. N\u00e9anmoins, le comportement par d\u00e9faut de compl\u00e9tion me semble non naturel. Si vous appuyez de multiples fois sur la touche tabulation, vous finissez toujours avec la m\u00eame liste de possibilit\u00e9s r\u00e9p\u00e9t\u00e9es le nombre de fois que vous appuyez sur la touche. Comme dans l'exemple suivant:</p> <p>Dans la prochaine section nous verrons comment configurer bash pour qu'il soit plus consistent et utile.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#la-recherche-recursive","title":"La recherche r\u00e9cursive","text":"<p>Pour acc\u00e9der aux commandes pr\u00e9c\u00e9demment \u00e9crites dans le terminal, vous pouvez utiliser la touche fl\u00e8che vers le haut. Si vous connaissez une partie de la commande que vous voulez r\u00e9utiliser (et que cette commande n'a pas \u00e9t\u00e9 \u00e9crite r\u00e9cemment) vous pouvez utiliser le mode r\u00e9cursif. Pour entrer dans le mode r\u00e9cursif, vous devez taper les touches Ctrl+R et \u00e9crire les premi\u00e8res lettres de votre choix (<code>ssh</code> par exemple). Le r\u00e9sultat est la premi\u00e8re commande correspondant \u00e0 votre recherche de votre historique bash. Si vous voulez les r\u00e9sultats suivants issus de votre historique bash, vous avez juste \u00e0 presser les touches Ctrl+R. J'appr\u00e9cie ce comportement, ainsi je le laisse tel quel.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#les-alertes","title":"Les alertes","text":"<p>C'est un alias permettant d'envoyer des notifications sous Ubuntu. Pour utiliser cet alias, il faut d'abord installer la librairie libnotify:</p> <pre><code>sudo apt install libnotify-bin\n</code></pre> <p>Cette commande envoie une notification \u00e0 votre environnement de bureau lorsqu'elle est ex\u00e9cut\u00e9e. Je l'utilise souvent avec un et logique pendant que je fais du prototypage de mod\u00e8les l\u00e9ger. Voici un exemple que vous pouvez essayer chez vous pour mieux comprendre son utilit\u00e9:</p> <pre><code>sleep 3 &amp;&amp; alert\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#personnaliser-votre-bashrc","title":"Personnaliser votre bashrc","text":"<p>Avant de commencer, je vous conseille de faire une sauvegarde de votre bashrc.</p> <pre><code>cp ~/.bashrc ~/.bashrc_back\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#personnaliser-linvite-de-commandes","title":"Personnaliser l'invit\u00e9 de commandes","text":"<p>C'est la partie cosm\u00e9tique, mais comme vous allez voir l'invit\u00e9 de commandes \u00e0 chaque ligne que vous \u00e9crivez autant qu'il vous convienne. Pour le personnaliser, il vous faut changer la variable PS1. La mienne est comme suit:</p> <pre><code>PS1=\"\\[\\033[38;5;11m\\]\\u\\[$(tput sgr0)\\]\\[\\033[38;5;15m\\]@\\H:\\[$(tput sgr0)\\]\\[\\033[38;5;32m\\]\\w\\[$(tput sgr0)\\]\\[\\033[38;5;15m\\]\\\\$ \\[$(tput sgr0)\\]\"\n</code></pre> <p>Elle devrait se trouver sous la ligne suivante de votre fichier ~/.bashrc:</p> <pre><code>if [ \"$color_prompt\" = yes ]; then\n</code></pre> <p>Ainsi, remplacez la valeur par d\u00e9faut par la v\u00f4tre.</p> <p>Cr\u00e9er votre variable PS1 peut \u00eatre long et fastidieux vu la syntaxe n\u00e9cessaire. Si vous \u00eates fain\u00e9ant comme moi, servez-vous d'ezprompt pour vous faciliter la vie. C'est un site internet interactif pour vous aider \u00e0 cr\u00e9er facilement votre variable PS1.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#modifications-de-la-variable-path","title":"Modifications de la variable PATH","text":"<p>J'aime mettre des scripts dans mon r\u00e9pertoire home (pour ne pas compromettre les r\u00e9pertoires de mon syst\u00e8me d'exploitation). Pour cela j'ai cr\u00e9\u00e9 un r\u00e9pertoire bin dans mon r\u00e9pertoire home et j'ajoute la ligne suivante dans mon bashrc:</p> <pre><code>PATH=~/bin:$PATH\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#completion","title":"Compl\u00e9tion","text":"<p>Comme vu dans la pr\u00e9c\u00e9dente section, le comportement par d\u00e9faut de compl\u00e9tion n'est pas incroyable. Par d\u00e9faut, la touche tabulation est li\u00e9e \u00e0 la commande complete. Heureusement, bash est livr\u00e9 avec une alternative: menu-complete. menu-complete fait que la touche de tabulation, mais liste les suggestions sous forme de cycle. Voici le m\u00eame exemple que pr\u00e9c\u00e9demment, mais utilisant menu-complete:</p> <p>Pour avoir ce comportement, vous devez ajouter les lignes suivantes dans votre fichier .bashrc:</p> <pre><code># tab cycle through commands after listing\nbind '\"\\t\":menu-complete'\n# completion results configuration\nbind \"set show-all-if-ambiguous on\"\nbind \"set completion-ignore-case on\"\nbind \"set menu-complete-display-prefix on\"\n# colored completion, for folders and others\nbind 'set colored-stats on'\n</code></pre> <p>De plus, menu-complete permet d'aller dans le sens inverse du cycle de suggestion (dans le cas o\u00f9 vous avez saut\u00e9 la suggestion d\u00e9sir\u00e9e). J'ai li\u00e9 ce comportement \u00e0 l'appui de la touche \u00b2 (car elle est au-dessus de la touche de tabulation d'un clavier AZERTY). Pour cela il faut ajouter les lignes suivantes:</p> <pre><code># reverse tab cycle\nbind '\"^[[Z\":menu-complete-backward' # Shift+Tab, but it is not working in Konsole.\nbind '\"\u00b2\":menu-complete-backward' # for azerty keyboard\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#mon-fichier-de-configuration-bashrc","title":"Mon fichier de configuration bashrc","text":"<p>Vous trouverez mon fichier bashrc ici. Il contient plus de personnalisation que pr\u00e9sent\u00e9, comme la configuration d'environnement gem et l'initialisation de miniconda. J'ai test\u00e9 cette configuration sous l'\u00e9mulateur de terminaux Konsole sous Ubuntu 18.04 (et KDE Neon) avec deux noms d'utilisateurs diff\u00e9rents. N\u00e9anmoins, soyez vigilants sur ce que vous faites avec ce script. Je ne fournis aucune garantie.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-28-improve-your-bash-navigation/#sources","title":"Sources","text":"<ul> <li>Les pages du manuel de Bash</li> </ul> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/01-31-format-a-bootable-usb-key/","title":"Formater une cl\u00e9 USB d'amor\u00e7age","text":"<p>Cet article de blogue est surtout un rappel pour moi, car je lutte \u00e0 chaque fois pour arriver \u00e0 cette solution \ud83d\ude1c. Apr\u00e8s avoir cr\u00e9\u00e9 une cl\u00e9 d'amor\u00e7age avec ma distribution pr\u00e9f\u00e9r\u00e9e (KDE Neon, si vous n'aviez pas devin\u00e9), je re\u00e7ois un message d'erreur lorsque je veux la formater en utilisant une interface graphique (test\u00e9 avec gnome-disk, kde partition manager et gparted).</p> <p>Voici les \u00e9tapes que je dois faire pour \u00eatre capable de modifier la cl\u00e9 USB avec une interface graphique de formatage.</p>","tags":["Astuces"]},{"location":"fr/blog/2020/01-31-format-a-bootable-usb-key/#identifier-le-nom-de-cle-usb-dans-votre-systeme","title":"Identifier le nom de cl\u00e9 USB dans votre syst\u00e8me","text":"<p>D'abord, tapez la commande suivante:</p> <pre><code>lsblk\n</code></pre> <p>Cela vous donne une sortie comme suit:</p> <pre><code>NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda           8:0    1  28,7G  0 disk\n\u2514\u2500sda1        8:1    1  28,6G  0 part /media/vincent/Kde Neon\nnvme0n1     259:0    0 126,9G  0 disk\n\u251c\u2500nvme0n1p1 259:1    0   512M  0 part /boot/efi\n\u2514\u2500nvme0n1p2 259:2    0 126,4G  0 part /\n</code></pre> <p>Ici, ma cl\u00e9 d'amor\u00e7age a le nom sda.</p>","tags":["Astuces"]},{"location":"fr/blog/2020/01-31-format-a-bootable-usb-key/#remplir-la-cle-de-zeros","title":"Remplir la cl\u00e9 de z\u00e9ros","text":"<p>Soyez vigilants dans cette partie, vous pouvez endommager votre syst\u00e8me d'exploitation en cas d'erreurs! Maintenant, il vous faut taper les commandes suivantes et remplacer <code>/dev/sda</code> par <code>/dev/&lt;le nom de votre cl\u00e9&gt;</code>.</p> <pre><code>sudo dd if=/dev/zero of=/dev/sda bs=4k conv=fsync &amp;&amp; alert\n</code></pre> <p>Et c'est fini. Maintenant, vous pouvez utiliser votre interface favorite pour formater votre cl\u00e9 usb. La commande <code>&amp;&amp; alert</code> n'est pas n\u00e9cessaire. Si vous voulez comprendre son int\u00e9r\u00eat, je vous sugg\u00e8re de regarder mon article de blog sur la configuration bash.</p> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous, cela m'aide certainement \ud83d\ude04.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Astuces"]},{"location":"fr/blog/2020/03-11-survey/","title":"\u00c9tat de l'art sur le few-shot pour des corpus de parole avec peu de donn\u00e9es","text":"<p>Mon nouvel \u00e9tat de l'art est disponible. Il fait une revue des techniques automatiques de traitement de la parole en comparant les techniques utilisant beaucoup de donn\u00e9es vs les techniques utilisant peu de donn\u00e9es. Ainsi, il inclut une partie sur les techniques dites de few-shot (pour utiliser peu de donn\u00e9es en apprentissage) et montre les premiers r\u00e9sultats encourageants de l'\u00e9tat de l'art sur l'audio. N'h\u00e9sitez pas \u00e0 y jeter un coup d'\u0153il. Malheureusement, c'est en anglais et la traduction viendra dans mon manuscrit de th\u00e8se.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","PhD"]},{"location":"fr/blog/2020/03-18-work-at-home/","title":"Configurations pour travailler depuis la maison","text":"<p>Cet article de blogue pr\u00e9sente toutes les configurations que j'ai effectu\u00e9es pour travailler depuis chez moi. C'est particuli\u00e8rement utile lorsque vous \u00eates en quarantaine (ou allez \u00eatre, car la progression du COVID-19 est alarmante). Je vais d\u00e9crire toutes les \u00e9tapes pour les utilisateurs Ubuntu avec un environnement KDE, les manipulations pour les utilisateurs de GNOME \u00e9tant similaire je n'irai pas en d\u00e9tail pour ce cas.</p>","tags":["Automatisation","Astuces","D\u00e9veloppment"]},{"location":"fr/blog/2020/03-18-work-at-home/#connecter-votre-ordinateur-personnel-au-vpn-de-votre-laboratoirecompagnie-en-utilisant-openvpn","title":"Connecter votre ordinateur personnel au VPN de votre laboratoire/compagnie en utilisant OpenVPN","text":"<p>C'est une premi\u00e8re \u00e9tape requise si vous voulez avoir acc\u00e8s \u00e0 l'Intranet de votre compagnie/laboratoire. Mon laboratoire supporte OpenVPN, je vais d\u00e9crire les \u00e9tapes requises pour utiliser cet outil.</p> <p>En premier, il faut installer OpenVPN:</p> <pre><code>sudo apt install openvpn\n</code></pre> <p>Ensuite, installez le connecteur de votre environnement de travail.</p> <pre><code># pour les utilisateurs de KDE\nsudo apt install network-manager-openvpn\n# pour les utilisateurs de GNOME\nsudo apt install network-manager-openvpn-gnome\n</code></pre> <p>Ensuite, vous pouvez utiliser l'interface graphique de votre environnement de travail pour vous connecter au VPN d\u00e9sir\u00e9. Pour les utilisateurs de KDE, aller dans le widget de r\u00e9seaux:</p> <p>Ensuite, cliquez sur l'ic\u00f4ne \"Configure network connections...\"  (entour\u00e9 de bleue sur l'image pr\u00e9c\u00e9dente). Cela ouvre la fen\u00eatre de configuration suivante:</p> <p>Pour ajouter une connexion VPN, cliquez sur l'ic\u00f4ne \"Add new connection\". Cela vous ouvrira le menu suivant:</p> <p>Ici, cliquez sur OpenVPN, puis cliquez sur \"Create\" et suivez les instructions de votre compagnie/laboratoire. Si votre laboratoire/compagnie fournit un fichier OpenVPN, c'est encore plus simple. Il vous faut cliquer sur \"Import VPN connection...\" et s\u00e9lectionnez le fichier qui vous est fourni.</p>","tags":["Automatisation","Astuces","D\u00e9veloppment"]},{"location":"fr/blog/2020/03-18-work-at-home/#se-connecter-a-un-serveur-distant","title":"Se connecter \u00e0 un serveur distant","text":"<p>J'utilise une connexion ssh pour acc\u00e9der aux machines de mon travail depuis chez moi. Pour rappel, on se connecte comme suit \u00e0 un serveur distant via ssh:</p> <pre><code>ssh &lt;login&gt;@&lt;addresss_du_serveur&gt; -p &lt;num\u00e9ro_de_port&gt;\n</code></pre> <p>Si vous voulez cr\u00e9er un serveur ssh ou avoir plus de d\u00e9tails, je vous recommande de regarder la documentation officielle d'Ubuntu.</p> <p>Apr\u00e8s m'\u00eatre connect\u00e9, je commence toujours (ou attache) une session tmux. Si vous ne connaissez pas ce qu'est tmux, allez regarder mon article de blog ici. Pour automatiser ce comportement, j'ajoute le code suivant dans mon fichier <code>~/.bashrc</code> c\u00f4t\u00e9 serveur:</p> <pre><code># Ouvre tmux automatiquement lorsque la machine est acc\u00e9d\u00e9 par ssh et sans serveur X\nif [ ! -z \"$SSH_CLIENT\" ] &amp;&amp; [ -z \"$DESKTOP_SESSION\" -a -z \"$TMUX\" ] ; then\n    tmux attach -t \"ssh\" 2&gt; /dev/null || tmux new -s \"ssh\"\nfi\n</code></pre> <p>Maintenant, \u00e0 chaque connexion via ssh, je me trouve dans une session tmux nomm\u00e9e <code>ssh</code>.</p>","tags":["Automatisation","Astuces","D\u00e9veloppment"]},{"location":"fr/blog/2020/03-18-work-at-home/#travailler-avec-des-notebooks-de-jupyter-lab","title":"Travailler avec des Notebooks de Jupyter Lab","text":"<p>Si vous utilisez Jupyter Lab, vous voulez certainement utiliser votre navigateur local pour acc\u00e9der \u00e0 vos notebooks. Il y a une multitude de solutions disponible sur internet. Ici je vous donne ma solution:</p> <ol> <li> <p>Cr\u00e9er un mot de passe pour les notebooks de jupyter (c\u00f4t\u00e9 serveur):</p> <pre><code>jupyter notebook password\n</code></pre> </li> <li> <p>Lancer jupyter lab en utilisant un port d\u00e9finit (ici 8887):</p> <pre><code>jupyter lab --port=8887 --no-browser\n</code></pre> </li> <li> <p>Sur votre terminal local, il faut transmettre le port serveur 8887 vers un port local de votre machine (disons le port 8888).     Pour cela j'utilise les tunnels ssh:</p> <pre><code>ssh -N -f -L 8888:localhost:8887 &lt;login&gt;@&lt;adresse_serveur&gt;\n</code></pre> </li> <li> <p>Utiliser votre navigateur local et pour aller sur localhost:8888/.     Ensuite, entrez votre mot de passe cr\u00e9\u00e9 en \u00e9tape 1.</p> </li> <li> <p>Profitez</p> </li> </ol> <p>Lorsque vous avez fini, vous voulez certainement arr\u00eater le tunnel ssh sur votre machine locale. Pour faire cela, il faut identifier le PID de la commande utilis\u00e9e en \u00e9tape 3 en utilisant la commande:</p> <pre><code>ps -ax | grep \"ssh -N -f -L\"\n</code></pre> <p>Maintenant que vous avez identifi\u00e9 le PID, il vous faut utiliser la commande <code>kill</code> pour arr\u00eater le tunnel:</p> <pre><code>kill -9 &lt;PID_identifi\u00e9&gt;\n</code></pre> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Automatisation","Astuces","D\u00e9veloppment"]},{"location":"fr/blog/2020/03-27-audio-loader/","title":"Audio loader","text":"<p>Note</p> <p>Ce projet a commenc\u00e9 comme une initiative parall\u00e8le \u00e0 ma th\u00e8se. Cependant, avec l'apparition de <code>torchaudio</code> et <code>speechbrain</code>, je n'ai plus vu l'utilit\u00e9 de le poursuivre.</p> <p>J'ai publi\u00e9 une version anticip\u00e9e d'une des librairies sur lesquelles je travaille. Elle est appel\u00e9e Audio Loader et est con\u00e7ue pour charger des batchs d'audios (avec caract\u00e9ristiques et v\u00e9rit\u00e9 terrain) pour des librairies de Deep Learning comme PyTorch ou TensorFlow.</p> <p>Pour l'instant, c'est les premiers travaux et seulement des m\u00e9canismes basiques sont fournis, tel que:</p> <ul> <li>Des liens vers PyTorch (basiques qui remplissent toutes les donn\u00e9es en RAM et con\u00e7us pour des sampleurs fen\u00eatr\u00e9s)</li> <li>Des chargeurs de v\u00e9rit\u00e9 terrain g\u00e9n\u00e9riques, avec un exemple sur TIMIT et le corpus C2SI.</li> <li>Des sampleurs g\u00e9n\u00e9riques (chargement utilisant des fen\u00eatres ou utilisant les fichiers en entier).</li> <li>Calculs de caract\u00e9ristiques basique utilisant la librairie librosa (seulement les MFCC et l'audio brut sont impl\u00e9ment\u00e9s).</li> <li>D\u00e9tection d'activit\u00e9s vocales basiques pour filtrer les signaux utilis\u00e9s.</li> <li>La plupart des fonctionnalit\u00e9s sont test\u00e9es \u00e0 l'aide de pytest.</li> </ul> <p>Ma liste de t\u00e2ches pour ce projet (non exhaustive):</p> <ol> <li>Ajouter un site internet pour la documentation.</li> <li>Ajouter d'autres caract\u00e9ristiques audios (spectrogrammes et autres).</li> <li>Avoir un syst\u00e8me \u00e9l\u00e9gant de cache, comme alternative \u00e0 remplir tout en RAM.</li> <li>Liens vers TensorFlow.</li> </ol> <p>Pour plus de d\u00e9tails, allez dans le r\u00e9pertoire GitHub (tout est en anglais). Si vous voulez contribuer, n'h\u00e9sitez pas \u00e0 me contacter ou \u00e0 proposer vos pull request \ud83d\ude04.</p> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/","title":"Astuces pour acc\u00e9l\u00e9rer le Deep Learning sur les serveurs","text":"<p>Lancer l'apprentissage d'un R\u00e9seau de neurones profonds peut prendre \u00e9norm\u00e9ment de temps. En effet, il n'est pas impossible de n\u00e9cessiter un mois pour apprendre un mod\u00e8le de l'\u00e9tat de l'art (tout d\u00e9pend du mat\u00e9riel \u00e0 votre disposition). Derni\u00e8rement, j'ai cherch\u00e9 \u00e0 acc\u00e9l\u00e9rer les temps d'apprentissage de mod\u00e8les bas\u00e9 sur des r\u00e9seaux profonds vu que m\u00eame 1% d'am\u00e9lioration est toujours \u00e0 prendre. Dans cet article de blogue, je vais partager les configurations que j'effectue sur toutes les machines mises \u00e0 ma disposition (bas\u00e9es sur Ubuntu). Ces configurations me permettent de gagner du temps et les voici.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#monter-le-disque-de-donnees-avec-loption-noatime","title":"Monter le disque de donn\u00e9es avec l'option noatime","text":"<p>Cette astuce est la plus importante que j'ai trouv\u00e9e. Par d\u00e9faut, le syst\u00e8me de fichier Linux utilise l'option atime. Cette option consiste \u00e0 \u00e9crire le dernier temps d'acc\u00e8s dans chaque fichier lu. Cela induit un haut usage d'entr\u00e9es/sorties disque lorsque l'on apprend un mod\u00e8le sur de larges bases de donn\u00e9es avec une multitude de fichiers. Lors de mes exp\u00e9riences, je gagne environ 5% de temps de calcul sur un ensemble d'entrainement d'environ 30000 fichiers audios (durant moins de 10s chacun). Plus de fichiers r\u00e9sulteraient \u00e0 de meilleurs gains. Gardez \u00e0 l'esprit qu'utiliser un manager de donn\u00e9es de haute performance (tel que hdf5) est une bonne alternative \u00e0 cette solution (et peut potentiellement obtenir de meilleures performances).</p> <p>Dans les prochaines sous-sections, je vais expliquer comment mettre l'option noatime sur un disque pour d\u00e9sactiver le comportement atime.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#determiner-lidentifiant-dun-disque","title":"D\u00e9terminer l'identifiant d'un disque","text":"<p>L'identifiant d'un disque se nomme UUID. Identifier l'UUID d'un disque peut se faire facilement en utilisant la commande suivante:</p> <pre><code>sudo lsblk -fm\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#verifier-si-le-disque-est-deja-monte","title":"V\u00e9rifier si le disque est d\u00e9j\u00e0 mont\u00e9","text":"<p>Maintenant que nous avons l'UUID de notre disque, nous devons nous assurer qu'il ne soit pas d\u00e9j\u00e0 mont\u00e9. Pour cela, tapez la commande suivante:</p> <pre><code>df -h\n</code></pre> <p>Si le disque est d\u00e9j\u00e0 mont\u00e9, vous devez le d\u00e9monter:</p> <pre><code>umount &lt;point d acc\u00e8s&gt;\n</code></pre> <p>Note</p> <p>Si vous voulez appliquer l'option noatime sur le disque de votre syst\u00e8me d'exploitation, vous pouvez toujours modifier le fichier <code>/etc/fstab</code> et cela sera appliqu\u00e9 au prochain red\u00e9marrage de votre ordinateur.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#creer-un-point-dacces","title":"Cr\u00e9er un point d'acc\u00e8s","text":"<p>Avant de monter votre disque, vous devez cr\u00e9er dossier servant de point d'acc\u00e8s (ou utiliser un dossier d\u00e9j\u00e0 existant). Pour cr\u00e9er un dossier, tapez:</p> <pre><code>mkdir &lt;votre point d acc\u00e8s&gt;\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#creer-une-entree-fstab-pour-votre-disque","title":"Cr\u00e9er une entr\u00e9e fstab pour votre disque","text":"<p>Maintenant que nous avons l'UUID de votre disque de donn\u00e9es (qui est non mont\u00e9) et un point d'acc\u00e8s, nous allons \u00e9diter le fichier <code>/etc/fstab</code>. En faisant comme ceci, votre disque de donn\u00e9es se montera automatiquement lors des prochains red\u00e9marrages. Pour ce faire, ajoutez la ligne suivante dans le fichier <code>/etc/fstab</code>:</p> <pre><code>UUID=&lt;UUID-identifi\u00e9&gt; &lt;chemin absolu du point d acc\u00e8s&gt; ext4 errors=remount-ro,noatime  0 0\n</code></pre> <p>Notes</p> <p>L'astuce ici est d'ajouter l'option noatime. Elle peut \u00eatre ajout\u00e9e sur les partitions syst\u00e8me (en modifiant les lignes du fichier <code>/etc/fstab</code>).</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#monter-le-disque-de-donnees","title":"Monter le disque de donn\u00e9es","text":"<p>Nous venons de configurer le fichier <code>fstab</code>, ce qui montera votre disque au prochain red\u00e9marrage. Si vous ne voulez pas red\u00e9marrer votre machine, vous pouvez monter votre disque de donn\u00e9es gr\u00e2ce \u00e0 la commande suivante:</p> <pre><code>sudo mount &lt;chemin absolu du point d acc\u00e8s&gt;\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#avoir-les-derniers-pilotes","title":"Avoir les derniers pilotes","text":"<p>Une autre astuce que j'utilise est de toujours avoir les pilotes de la carte graphique \u00e0 jour. Ainsi, je peux b\u00e9n\u00e9ficier des derni\u00e8res optimisations. Depuis juillet 2019, Ubuntu offre l'acc\u00e8s aux derniers pilotes Nvidia pour leurs utilisateurs de version LTS (support \u00e0 long terme).</p> <p>Pour lister les pilotes disponibles, vous devez taper la commande suivante:</p> <pre><code>ubuntu-drivers devices\n</code></pre> <p>Ensuite, vous choisissez le dernier driver (disons le 560) et utilisez la commande <code>apt</code> comme suivant:</p> <pre><code>sudo apt install nvidia-driver-560\n</code></pre>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#suivre-les-recommendations-de-votre-librairie-de-deep-learning","title":"Suivre les recommendations de votre librairie de deep learning","text":"<p>Il existe de nombreuses techniques pour optimiser le temps d'entra\u00eenement et d'inf\u00e9rence de vos mod\u00e8les. Bien qu'il existe des techniques sp\u00e9cifiques \u00e0 chaque librairie, beaucoup sont communes \u00e0 PyTorch et TensorFlow.</p> <p>Parmi les plus utiles dans ces deux frameworks, on peut citer l'utilisation de la pr\u00e9cision mixte et la compilation du r\u00e9seau pour am\u00e9liorer consid\u00e9rablement la vitesse d'apprentissage de vos mod\u00e8les. La pr\u00e9cision mixte combine des calculs en pr\u00e9cision simple (32 bits) et en demi-pr\u00e9cision (16 bits), ce qui acc\u00e9l\u00e8re l'entra\u00eenement tout en r\u00e9duisant la consommation de m\u00e9moire, sans compromettre la pr\u00e9cision des r\u00e9sultats. La compilation, quant \u00e0 elle, optimise le calcul des graphes pour les rendre plus efficaces. Cela permet une ex\u00e9cution plus rapide sur les CPU et GPU, en minimisant les calculs redondants et en maximisant le parall\u00e9lisme.</p> <p>Pour aller plus loin, vous pouvez explorer la quantization (r\u00e9duction du nombre de bits pour encoder les param\u00e8tres) et le prunage de mod\u00e8les (suppression de param\u00e8tres tout en conservant la performance).</p> <p>Retrouvez plus de d\u00e9tails sur la mise en \u0153uvre de ces techniques ici pour TensorFlow et ici pour PyTorch.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/06-24-tweaks-to-speedup-AI-servers/#ameliorer-les-temps-de-compilation","title":"Am\u00e9liorer les temps de compilation","text":"<p>Pour am\u00e9liorer les temps de compilation, vous pouvez utiliser la RAM au lieu d'un disque physique (encore plus utile si vous n'avez pas de SSD). Pour ce faire, vous devez ajouter la ligne suivante dans votre fichier <code>/etc/fstab</code>:</p> <pre><code>none    /tmp/    tmpfs    noatime,size=10%    0    0\n</code></pre> <p>Notes</p> <p>Ceci limite la taille maximale de <code>/tmp</code> (de 10% de la quantit\u00e9 maximale de RAM) et peut bloquer les usages n\u00e9cessitant un large cache (comme construire des images singularity). N'oubliez pas de changer le cache utilis\u00e9 pour ces cas (ou augmentez la taille de <code>/tmp</code> dans la ligne au-dessus et en fonction de votre quantit\u00e9 de RAM disponible).</p> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous. Si vous avez des conseils ou d'autres astuces, n'h\u00e9sitez pas \u00e0 partager votre savoir \ud83d\ude04.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["D\u00e9veloppement"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/","title":"Automatisation en utilisant des cl\u00e9s SSH","text":"<p>J'ai commenc\u00e9 mon blogue avec un article de blogue sur comment sauver les identifiants git pour les sites internet ne supportant pas les cl\u00e9s SSH (c'est par ici pour ceux qui sont int\u00e9ress\u00e9s). \u00c0 ce moment, j'\u00e9tais dans l'impossibilit\u00e9 d'utiliser les cl\u00e9s SSH sur les serveurs d'Overleaf (et c'est toujours le cas au moment o\u00f9 j'\u00e9cris). Cela m'a aid\u00e9 pour ce cas particulier. N\u00e9anmoins, cette astuce fonctionne uniquement pour les serveurs git. De plus, ce n'est pas le meilleur moyen pour automatiser ses connexions (au moins de mon point de vue). Aujourd'hui, nous allons voir comment utiliser les cl\u00e9s SSH pour automatiser plusieurs \u00e9tapes d'identifications.</p> <p>Dans cet article de blogue, nous allons voir deux cas d'utilisation:</p> <ul> <li>Identification automatique \u00e0 des serveurs utilisant le protocole SSH.</li> <li>Identification automatique lors de commandes push/push sur des serveurs git (comme GitHub ou GitLab).</li> </ul> <p>Les cl\u00e9s SSH sont compos\u00e9es d'une cl\u00e9 publique pour encoder les messages (destin\u00e9 aux serveurs) et une cl\u00e9 priv\u00e9e afin de lire ces messages (destin\u00e9 pour le client du serveur). Si vous voulez plus d'information sur ce protocole, allez voir ici.</p> <p>La distribution utilis\u00e9e (et test\u00e9e) pour ce tutoriel est Kubuntu 20.04 LTS (ma nouvelle distribution, mais c'est pour un futur article de blogue).</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#preparer-le-cote-client","title":"Pr\u00e9parer le c\u00f4t\u00e9 client","text":"<p>Dans cette section, nous allons pr\u00e9parer le client qu'il puisse s'identifier automatiquement sur les serveurs. D'abord, cr\u00e9ons une paire de cl\u00e9s publique et priv\u00e9e. Enfin, nous d\u00e9marrerons l'agent de cl\u00e9s SSH pour le configurer avec la cl\u00e9 priv\u00e9e.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#verifiez-la-presence-dune-paire-de-cles-ssh","title":"V\u00e9rifiez la pr\u00e9sence d'une paire de cl\u00e9s SSH","text":"<p>Pour cela il faut regarder dans le dossier <code>~/.ssh/</code> pour voir si vous avez d\u00e9j\u00e0 une cl\u00e9 publique SSH. Par d\u00e9faut, le nom de fichier d'une cl\u00e9 publique est <code>id_rsa.pub</code>. Vous pouvez utiliser une paire diff\u00e9rente de cl\u00e9s par serveur si vous voulez. Je pr\u00e9f\u00e8re garder une paire de cl\u00e9s par machine (et en changer r\u00e9guli\u00e8rement).</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#generer-une-paire-de-cles-ssh","title":"G\u00e9n\u00e9rer une paire de cl\u00e9s SSH","text":"<p>Pour g\u00e9n\u00e9rer une paire de cl\u00e9s li\u00e9es \u00e0 une adresse email (pour mieux identifier l'utilisateur connect\u00e9) vous devez \u00e9crire la ligne suivante:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <p>Ensuite, vous devez suivre les instructions. Vous pouvez laisser le chemin des cl\u00e9s par d\u00e9faut si vous n'avez pas d\u00e9j\u00e0 une paire de cl\u00e9s. Vous allez \u00e9galement d\u00e9finir un mot de passe pour d\u00e9verrouiller votre cl\u00e9 priv\u00e9e, soyez certain de vous en rappeler. Faites \u00e9galement attention de ne pas r\u00e9v\u00e9ler votre cl\u00e9 priv\u00e9e (le fichier <code>id_rsa</code>) en toutes circonstances.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#demarrer-lagent-de-cle-ssh-et-ajouter-votre-cle-privee","title":"D\u00e9marrer l'agent de cl\u00e9 SSH et ajouter votre cl\u00e9 priv\u00e9e","text":"<p>Pour laisser votre syst\u00e8me se souvenir de votre cl\u00e9 priv\u00e9e durant votre session, vous pouvez utiliser l'agent SSH comme suit:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>Maintenant, nous avons une paire de cl\u00e9s priv\u00e9e et publique ainsi que l'agent de cl\u00e9 SSH de configur\u00e9s. Nous sommes pr\u00eats pour automatiser les identifications SSH vers des serveurs SSH et/ou serveur git.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#automatisez-votre-connexion-sur-les-serveurs-en-utilisant-ssh","title":"Automatisez votre connexion sur les serveurs en utilisant SSH","text":"<p>Pour automatiser l'\u00e9tape de saisie de votre mot de passe, vous pouvez ajouter votre cl\u00e9 publique dans la liste des cl\u00e9s autoris\u00e9es de vos serveurs. Avant \u00e7a, assurez-vous que votre espace utilisateur distant contienne le dossier <code>~/ssh</code>:</p> <pre><code>ssh &lt;identifiant&gt;@&lt;adresse_serveur&gt; mkdir -p .ssh\n</code></pre> <p>Ensuite, vous devez ajouter votre cl\u00e9 publique dans le fichier des cl\u00e9s autoris\u00e9es de vos serveurs:</p> <pre><code>cat ~/.ssh/id_rsa.pub | ssh &lt;identifiant&gt;@&lt;adresse_serveur&gt; 'cat &gt;&gt; .ssh/authorized_keys'\n</code></pre> <p>Maintenant, votre serveur est configur\u00e9 avec votre paire de cl\u00e9s SSH. Vous pouvez r\u00e9p\u00e9ter ces deux \u00e9tapes pour chaque serveur dont vous avez acc\u00e8s.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#automatiser-vos-authentifications-sur-les-serveurs-git","title":"Automatiser vos authentifications sur les serveurs Git","text":"<p>Pour automatiser vos authentifications (\u00e9crire votre identifiant et votre mot de passe) apr\u00e8s l'utilisation d'une commande <code>push</code>/<code>pull</code>, vous devez ajouter votre cl\u00e9 publique dans le serveur git (en utilisant l'interface web du serveur git). Pour copier votre cl\u00e9 publique sur un site internet (tel que GitHub ou GitLab) vous pouvez ajouter votre cl\u00e9 dans le presse-papier (pour utiliser Ctrl+V \u00e0 l'int\u00e9rieur de votre navigateur internet).</p> <p>\u00c0 cette fin, vous devez installer <code>xclip</code>:</p> <pre><code>sudo apt install xclip\n</code></pre> <p>ou si vous \u00eates sur manjaro:</p> <pre><code>sudo pamac install xclip\n</code></pre> <p>Ensuite, c'est aussi simple que suit:</p> <pre><code>xclip -sel clip &lt; ~/.ssh/id_rsa.pub\n</code></pre> <p>Apr\u00e8s, suivez les \u00e9tapes de ce lien (en anglais, mais tr\u00e8s bien illustr\u00e9) pour GitHub et de cet autre lien (en anglais, mais la documentation officielle n'est pas traduite en fran\u00e7ais) pour GitLab.</p> <p>Maintenant, votre configuration est pr\u00eate pour votre serveur Git. La prochaine sous-section expliquera comment tester cette configuration sans avoir \u00e0 modifier l'un de vos r\u00e9pertoires.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#tester-votre-configuration","title":"Tester votre configuration","text":"<p>Pour tester votre nouvelle configuration sur vos serveurs git, vous pouvez utiliser le protocole SSH. Cela vous \u00e9vitera de modifier un de vos r\u00e9pertoires distants. Les prochaines sous-sections vous montrent comment faire avec les serveurs les plus populaires.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#garder-les-identites-de-lagent-ssh-sur-kubuntu-2204-apres-un-redemarrage","title":"Garder les identit\u00e9s de l'agent SSH sur Kubuntu 22.04 apr\u00e8s un red\u00e9marrage","text":"<p>Avec les pr\u00e9c\u00e9dentes instructions, vous devez (du moins dans Kubuntu) reconfigurer votre agent SSH apr\u00e8s chaque red\u00e9marrage ou d\u00e9connexion. Dans cette sous-section, nous allons utiliser kwallet pour outrepasser cette limitation.</p> <p>D'abord, nous devons installer le paquet <code>ssh-askpass</code>:</p> <pre><code>sudo apt install ssh-askpass\n</code></pre> <p>Ensuite, nous devons cr\u00e9er un script qui va automatiquement d\u00e9verrouiller votre cl\u00e9 priv\u00e9e lorsque vous vous connectez. Ceci est fait par les lignes suivantes:</p> <pre><code>mkdir -p ~/.config/autostart-scripts\necho '#!/bin/sh' &gt; ~/.config/autostart-scripts/ssh-add.sh\necho 'export SSH_ASKPASS=/usr/bin/ksshaskpass' &gt;&gt; ~/.config/autostart-scripts/ssh-add.sh\necho 'ssh-add &lt; /dev/null' &gt;&gt; ~/.config/autostart-scripts/ssh-add.sh\nchmod +x ~/.config/autostart-scripts/ssh-add.sh\n</code></pre> <p>L'\u00e9tape suivante consiste \u00e0 taper la commande suivante et de cocher la case pour se souvenir du mot de passe:</p> <pre><code>~/.config/autostart-scripts/ssh-add.sh\n</code></pre> <p>Cela va permettre \u00e0 kwallet (portefeuille de cl\u00e9s de KDE) de retenir le mot de passe pour votre cl\u00e9 priv\u00e9e et la d\u00e9bloquer apr\u00e8s chaque connexion.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#garder-les-identites-de-lagent-ssh-sur-manjaro-apres-un-redemarrage","title":"Garder les identit\u00e9s de l'agent SSH sur Manjaro apr\u00e8s un red\u00e9marrage","text":"<p>Apr\u00e8s mon passage sur Manjaro linux (voir mon post sur ma migration), je me suis aper\u00e7u que la m\u00e9thode pour Kubuntu ne fonctionnait pas (et risque de ne pas fonctionner sur les prochaines LTS). Voici ma solution, qui est une combinaison de deux approches que vous pouvez retrouver dans mes sources.</p> <p>Tout d'abord, assurez-vous d'avoir le n\u00e9cessaire\u00a0:</p> <pre><code>sudo pamac install kwallet ksshaskpass kwalletmanager\n</code></pre> <p>Ensuite, configurons notre syst\u00e8me et zsh pour utiliser les voix de communications adapt\u00e9es \u00e0 l'agent ssh:</p> <pre><code>sudo echo '#!/bin/sh' &gt; /etc/profile.d/ssh-askpass.sh\nsudo echo 'export SSH_ASKPASS=/usr/bin/ksshaskpass' &gt;&gt; /etc/profile.d/ssh-askpass.sh\necho 'export SSH_AUTH_SOCK=\"$XDG_RUNTIME_DIR\"/ssh-agent.socket' &gt;&gt; ~/.zshrc\n</code></pre> <p>Ensuite, nous cr\u00e9ons le r\u00e9pertoire utilisateur pour systemd:</p> <pre><code>mkdir -p ~/.config/systemd/user\n</code></pre> <p>Ensuite, cr\u00e9ez le fichier <code>~/.config/systemd/user/ssh-agent.service</code> et remplissez-le avec le contenu de l'encadr\u00e9 suivant\u00a0:</p> <pre><code>[Unit]\nDescription=SSH agent (ssh-agent)\n\n[Service]\nType=simple\nEnvironment=SSH_AUTH_SOCK=%t/ssh-agent.socket\nEnvironment=DISPLAY=:0\nEnvironment=KEY_FILE=/home/%u/.ssh/id_rsa\nExecStart=ssh-agent -D -a $SSH_AUTH_SOCK\nExecStartPost=/bin/sleep 3\nExecStartPost=/usr/bin/ssh-add $KEY_FILE\nExecStop=kill -15 $MAINPID\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Ce service lance l'agent ssh \u00e0 chaque authentification et ajoute la cl\u00e9 priv\u00e9e que vous avez cr\u00e9\u00e9e en d\u00e9but de cet article. Nous allons maintenant l'activer et le lancer\u00a0:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user enable ssh-agent.service\n</code></pre> <p>Maintenant vous pouvez red\u00e9marrer votre machine et toute est bon \ud83d\ude04. En esp\u00e9rant que cela vous a \u00e9t\u00e9 utile \ud83d\ude09.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/08-02-automation-using-ssh-keys/#sources-et-inspirations","title":"Sources et inspirations","text":"<ul> <li>Instructions officielles de GitHub pour les cl\u00e9s SSH</li> <li>Instructions officielles de GitLab pour les cl\u00e9s SSH</li> <li>Configurer l'agent de cl\u00e9 SSH d'Ubuntu</li> <li>Kubuntu et l'agent de cl\u00e9 SSH</li> <li>Manjaro et l'agent de cl\u00e9 SSH (1/2)</li> <li>Manjaro et l'agent de cl\u00e9 SSH (2/2)</li> </ul> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Automatisation"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/","title":"Cr\u00e9er votre documentation sphinx sur GitHub","text":"<p>Note</p> <p>J'utilise d\u00e9sormais <code>mkdocs</code> pour tous mes projets de documentation, y compris ce site web.</p> <p>Lors d'un pr\u00e9c\u00e9dent article de blogue, j'ai annonc\u00e9 l'aper\u00e7u de ma librairie Audio Loader. Elle pr\u00e9pare des batchs d'audios pour des librairies de r\u00e9seaux de neurones (La librairie se trouve ici). Pour cette librairie j'avais planifi\u00e9 de cr\u00e9er un site internet de documentation. Une version pr\u00e9liminaire est maintenant disponible ici. Comme Audio Loader est une librairie Python, j'ai choisi Sphinx pour g\u00e9n\u00e9rer la documentation de cette librairie. Sphinx utilise le format de fichier reStructuredText pour cr\u00e9er la documentation. N\u00e9anmoins, je pr\u00e9f\u00e8re la syntaxe du markdown qui est plus simple et que j'utilise plus souvent. Ainsi, j'utilise autant de fichiers markdown que possible pour cr\u00e9er mes documentations avec Sphinx. Ensuite, pour h\u00e9berger la documentation j'utilise GitHub Pages vu que je l'utilise d\u00e9j\u00e0 pour ce blogue.</p> <p>Dans cet article de blogue, je vais vous expliquer comment cr\u00e9er une documentation en utilisant Sphinx et autant de fichiers markdown que possible. Enfin, j'expliquerai comment mettre en place votre d\u00e9p\u00f4t GitHub pour h\u00e9berger cette documentation.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#creer-sa-premiere-documentation","title":"Cr\u00e9er sa premi\u00e8re documentation","text":"<p>Dans cette partie, nous allons voir comment cr\u00e9er une documentation en utilisant Sphinx.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#installer-sphinx","title":"Installer Sphinx","text":"<p>Avant tout, il faut installer sphinx:</p> <pre><code>pip install sphinx\n</code></pre> <p>Maintenant, nous avons tout le n\u00e9cessaire pour cr\u00e9er notre documentation \ud83d\ude04.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#creer-les-fichiers-initiaux","title":"Cr\u00e9er les fichiers initiaux","text":"<p>Sphinx organise la documentation en 3 \u00e9l\u00e9ments:</p> <ul> <li>un fichier de compilation pour g\u00e9n\u00e9rer la documentation</li> <li>les fichiers sources contenant les instructions pour g\u00e9n\u00e9rer la documentation</li> <li>la documentation r\u00e9sultante (fichier pdf, pages html, ...)</li> </ul> <p>Maintenant, cr\u00e9ons cette organisation en utilisant l'outil <code>quickstart</code> de sphinx:</p> <pre><code>mkdir docs\ncd docs\nsphinx-quickstart\n</code></pre> <p>Dans les instructions de quickstart, choisissez de s\u00e9parer les dossiers sources des dossiers de build/r\u00e9sultats (ceci facilitera les prochaines \u00e9tapes). Remplissez le reste comme bon vous semble.</p> <p>Apr\u00e8s cela, Sphinx cr\u00e9\u00e9 deux dossiers (<code>docs/sources</code> and <code>docs/build</code>) et un fichier de compilation (<code>docs/Makefile</code>).</p> <p>Les fichiers sources sphinx se trouvent dans <code>docs/sources</code> et le fichier <code>index.rst</code> repr\u00e9sente la page de pr\u00e9sentation de votre documentation (l'\u00e9quivalent de <code>index.html</code> pour un site internet). Pour compiler votre documentation en un site internet ou un fichier pdf, vous pouvez utiliser le fichier de compilation (en utilisant <code>make html</code> ou <code>make pdf</code>). La documentation r\u00e9sultante se trouvera dans <code>docs/build</code>.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#inclurelier-des-fichiers-dans-un-document-rst","title":"Inclure/lier des fichiers dans un document rst","text":"<p>Ici je ne vais pas d\u00e9tailler le format rst vu que je l'\u00e9vite autant que possible. \u00c0 la place, je vais vous montrer comment ajouter des documents et/ou liens vers d'autres fichiers au sein d'un fichier rst. N\u00e9anmoins, si vous voulez apprendre le format je vous conseille d'aller ici.</p> <p>Je pr\u00e9f\u00e8re \u00e9viter un maximum le format rst vu que je peux utiliser des fichiers markdown la plupart du temps. N\u00e9anmoins pour les cas o\u00f9 les fichiers rst sont n\u00e9cessaires (comme <code>index.rst</code>), il est utile de savoir inclure ou faire un lien \u00e0 des fichiers. Par cons\u00e9quent, c'est ce que je vais expliquer dans cette partie.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#lier-un-fichier-rst-a-laide-du-doctree","title":"Lier un fichier rst \u00e0 l'aide du doctree","text":"<p>Pour rajouter un lien vers un fichier (disons <code>extra_document.rst</code>) il vous faut ajouter <code>extra_document</code> dans le doctre comme suit:</p> <pre><code>.. toctree::\n    :maxdepth: 2\n\n    extra_document\n</code></pre> <p>Attention:</p> <p>L'indentation de <code>extra_document</code> doit \u00eatre comme l'indentation de la ligne contenant <code>:maxdepth: 2</code>. Si ce n'est pas le cas, vous aurez un avertissement vous disant que sphinx ne trouve pas le fichier (cela m'a pris des heures pour m'en rendre compte).</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#inclure-un-fichier-rst-dans-un-autre-fichier-rst","title":"Inclure un fichier rst dans un autre fichier rst","text":"<p>Pour ajouter le contenu d'un fichier <code>rst</code> \u00e0 l'int\u00e9rieur d'un autre fichier <code>rst</code>, vous devez ajouter la ligne suivante:</p> <pre><code>.. include:: my_file.rst\n</code></pre> <p>Le document contenant cette ligne sera rempli du contenu de <code>my_file.rst</code>.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#ecrire-une-documentation-sphinx-a-laide-de-fichiers-markdown-et-dautodoc","title":"\u00c9crire une documentation sphinx \u00e0 l'aide de fichiers markdown et d'autodoc","text":"","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#configurer-sphinx-pour-le-markdown","title":"Configurer sphinx pour le markdown","text":"<p>Pour ajouter le support du markdown dans sphinx, j'utilise <code>m2r</code> au lieu de <code>recommonmark</code> (officiellement recommand\u00e9 par sphinx). La raison est simple: <code>recommonmark</code> ne supporte pas la commande <code>mdinclude</code> pour inclure les documents markdown dans des fichiers <code>rst</code>. Maintenant, installons <code>m2r</code>:</p> <pre><code>pip install m2r\n</code></pre> <p>Ensuite, nous devons modifier le fichier <code>source/config.py</code> pour lui  ajouter cette ligne:</p> <pre><code>extensions.append(\"m2r\")\n</code></pre> <p>Maintenant, vous pouvez utiliser des fichiers markdown pour votre documentation. Vous pouvez ajoutez ces derniers dans le doctree (comme pour les fichiers <code>rst</code>) ou les inclure en utilisant <code>mdinclude</code>. Un petit exemple avec <code>mdinclude</code>:</p> <pre><code>.. mdinclude:: my_file.md\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#autodoc","title":"Autodoc","text":"<p>Pendant la cr\u00e9ation d'une documentation, il est agr\u00e9able d'utiliser la docstring des fichiers sources pour remplir la documentation. Ici je vais expliquer comment vous pouvez utiliser l'extension <code>autodoc</code>.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#installation","title":"Installation","text":"<p>Pour activer l'extension, il faut ajouter la ligne suivante dans le fichier <code>source/config.py</code>:</p> <pre><code>extensions.append('sphinx.ext.autodoc')\n</code></pre> <p>Il faut \u00e9galement ajouter les lignes suivantes en d\u00e9but du fichier <code>source/config.py</code>:</p> <pre><code>import os\nimport sys\nsys.path.insert(0, os.path.abspath('../..'))\n</code></pre> <p>Pour utiliser le format numpy dans vos docstring vous devez \u00e9galement ajouter l'extension napoleon dans le fichier <code>source/config.py</code>:</p> <pre><code>extensions.append('sphinx.ext.napoleon')\n</code></pre> <p>Nous sommes maintenant pr\u00eats pour utiliser autodoc.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#montrer-des-elements-de-la-docstring","title":"Montrer des \u00e9l\u00e9ments de la docstring","text":"<p>Toutes les commandes qui vont suivre fonctionnent au sein de fichiers <code>rst</code> (je cherche toujours un moyen simple pour utiliser ces commandes dans un fichier markdown, n'h\u00e9sitez pas \u00e0 partager vos conseils en commentaire \ud83d\ude04). Pour plus de d\u00e9tails, suivez la documentation officielle.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#utiliser-la-docstring-dun-module","title":"Utiliser la docstring d'un module","text":"<pre><code>.. automodule:: project_folder.module\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#utiliser-toutes-les-docstring-dun-module","title":"Utiliser toutes les docstring d'un module","text":"<pre><code>.. automodule:: project_folder.module\n    :members:\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#utiliser-la-docstring-dune-classe","title":"Utiliser la docstring d'une classe","text":"<pre><code>.. autoclass:: project_folder.module.class\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#theme-sphinx","title":"Th\u00e8me sphinx","text":"<p>Maintenant, pensons esth\u00e9tique \ud83d\ude04. Nous allons utiliser les th\u00e8mes se trouvant ici.</p> <p>Sur le site internet, choisissez votre style d\u00e9sir\u00e9, puis cliquez sur l'hyperlink pypi pour installer le th\u00e8me en utilisant la commande <code>pip</code>. Pour appliquer votre th\u00e8me, cliquez sur l'hyperlink <code>conf.py</code> associ\u00e9 et regardez la variable <code>html_theme</code>. Ensuite copiez le code correspondant pour modifier votre variable <code>html_theme</code> dans votre fichier <code>source/config.py</code>.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#heberger-votre-documentation-sur-github","title":"H\u00e9berger votre documentation sur GitHub","text":"<p>Maintenant que nous savons comment cr\u00e9er une documentation, il serait bien de l'h\u00e9berger sur un serveur. Ici, je vais expliquer comment j'ai fait avec les serveurs GitHub.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#preparer-la-structure-de-votre-projet","title":"Pr\u00e9parer la structure de votre projet","text":"<p>Pour mes projets, je pense que la structure la plus adapt\u00e9e est la suivante:</p> <pre><code>my_project\n|-  my_project_code      --&gt; dossier contenant le code source et les sources de la documentation\n|-  my_project_gh_pages  --&gt; dossier contenant votre site web ou pdf\n|   |-  html             --&gt; dossier contenant votre documentation (synchronis\u00e9 avec la branche gh_pages)\n</code></pre> <p>Maintenant, cr\u00e9ons cette structure. En premier, cr\u00e9ons un r\u00e9pertoire pour votre projet:</p> <pre><code>mkdir my_project\ncd my_project\n</code></pre> <p>Puis, cr\u00e9ons les dossiers <code>my_project/my_project_code</code> et <code>my_project/my_project_gh_pages</code>:</p> <pre><code>mkdir my_project_gh_pages\ngit clone https://github.com/username/my_project\nmv my_project my_project_code\n</code></pre> <p>Maintenant, cr\u00e9ons et pr\u00e9parons le dossier <code>html</code>:</p> <pre><code>git clone https://github.com/username/my_project\nmv my_project html\ncd html\ngit branch gh-pages\n\ngit symbolic-ref HEAD refs/heads/gh-pages  # auto-switches branches to gh-pages\n# supprime l'index pour la branche gh-pages\nrm .git/index\ngit clean -fdx\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#modifier-le-makefile-de-sphinx","title":"Modifier le makefile de sphinx","text":"<p>Maintenant, nous devons modifier le makefile pour utiliser la branche gh-branch et le dossier <code>my_project_gh_pages</code>.</p> <p>Pour cela, \u00e9ditez le fichier <code>my_project/my_project_code/docs/Makefile</code> tel que la variable <code>BUILDDIR</code> soit comme tel:</p> <pre><code>BUILDDIR      = ../../my_project_gh_pages/\n</code></pre> <p>Vous pouvez maintenant g\u00e9n\u00e9rer votre documentation avec la commande suivante:</p> <pre><code>make html\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#creer-vos-fichiers-html-pour-github","title":"Cr\u00e9er vos fichiers html pour GitHub","text":"<p>Votre documentation est maintenant dans le r\u00e9pertoire <code>my_project/my_project_gh_pages/html</code>. Maintenant il vous reste \u00e0 faire un <code>commit</code> et <code>push</code> comme suit:</p> <pre><code>cd my_project_gh_pages/\ngit add html\ngit commit -m \"First documentation commit\"\ngit push --set-upstream origin gh-pages\n</code></pre> <p>La documentation est maintenant disponible sous <code>https://username.github.io/my_project</code>, \u00e0 moins que vous n'ayez fait de redirection pour votre GitHub Pages (comme pour ce blogue).</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#forcer-la-desactivation-de-jekyll","title":"Forcer la d\u00e9sactivation de jekyll","text":"<p>Les serveurs de GitHub utilisent jekyll et cela peut provoquer des erreurs d'interpr\u00e9tations de fichiers html g\u00e9n\u00e9r\u00e9s par sphinx. Heureusement, nous pouvons d\u00e9sactiver jekyll.</p> <p>\u00c0 l'int\u00e9rieur du dossier <code>my_project/my_project_gh_pages/html</code> tapez les commandes suivantes:</p> <pre><code>touch .nojekyll\ngit add .nojekyll\ngit commit -m \"added .nojekyll\"\ngit push origin gh-pages\n</code></pre>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#construire-la-documentation-et-la-commiter-automatiquement","title":"Construire la documentation et la commiter automatiquement","text":"<p>Vous pouvez ajouter une commande dans votre Makefile:</p> <pre><code>    pushhtml: html\n\n    cd $(BUILDDIR)/html; git add . ; git commit -m \"rebuilt docs\"; git push origin gh-pages\n</code></pre> <p>Maintenant, en tapant <code>make pushhtml</code> vous pouvez construire votre documentation, la commiter et la pousser automatiquement sur GitHub. Elle reste au c\u00f4t\u00e9 de la commande <code>make html</code> qui peut \u00eatre utilis\u00e9e pour tester vos documentations dans les pousser sur GitHub.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2020/11-20-create-a-documentation-with-sphinx-on-github/#sourcesinspirations","title":"Sources/Inspirations","text":"<ul> <li> <p>La documentation Official de sphinx</p> </li> <li> <p>Getting started with sphinx</p> </li> <li> <p>Getting started with autodoc</p> </li> <li> <p>Publishing sphinx documentation on GitHub</p> </li> </ul> <p>J\u2019esp\u00e8re que cela aidera certains d\u2019entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","D\u00e9veloppement"]},{"location":"fr/blog/2021/09-12-badaboom-asteroids/","title":"BadaBoom - Ast\u00e9ro\u00efdes","text":"<p>Dans ce nouvel article de blog, je vais vous pr\u00e9senter un petit projet que j'ai r\u00e9alis\u00e9 derni\u00e8rement. \u00c9tant passionn\u00e9 d'espace, j'ai cherch\u00e9 une base de donn\u00e9es simple qui me permettrait d'appr\u00e9hender l'immensit\u00e9 du monde qui nous entoure. Je suis finalement tomb\u00e9 sur la base 'Asteroids - NeoWs' fournie par la NASA (pour plus d'informations, c'est par ici). Cette base est focalis\u00e9e sur les ast\u00e9ro\u00efdes passant pr\u00e8s de la plan\u00e8te Terre. Dans cette base, le point le plus proche d'un ast\u00e9ro\u00efde passant aupr\u00e8s de la Terre repr\u00e9sente un \u00e9v\u00e8nement.</p> <p>J'ai donc r\u00e9alis\u00e9 un d\u00e9p\u00f4t nomm\u00e9 badaboom. Ce dernier contient un parseur (extracteur de donn\u00e9es) et un programme permettant de r\u00e9aliser quelques figures et statistiques. Dans cet article de blog, je vais d\u00e9tailler les premiers r\u00e9sultats que j'ai obtenus (et mettrais \u00e0 jour ce post en fonction de mes avanc\u00e9es).</p> <p>Note</p> <p>Vous pouvez recr\u00e9er tous les graphiques avec des informations mises \u00e0 jour. Pour cela, vous pouvez consulter le projet sur GitHub \u00e0 badaboom. J'utilise \u00e9galement ce d\u00e9p\u00f4t comme mod\u00e8le pour mes projets Python. N'h\u00e9sitez pas \u00e0 y jeter un \u0153il et \u00e0 me faire part de vos suggestions pour l'am\u00e9liorer !</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2021/09-12-badaboom-asteroids/#resultats","title":"R\u00e9sultats","text":"<p>Tous les r\u00e9sultats qui suivront sont calcul\u00e9s pour les ann\u00e9es comprises entre 1980 et 2030. Sachant que les donn\u00e9es \u00e9taient r\u00e9cup\u00e9r\u00e9es le 10 septembre 2021, toutes les donn\u00e9es de cette date \u00e0 fin 2030 sont les projections de la NASA.</p> <p>Avant de commencer, voici les 10 plus grands ast\u00e9ro\u00efdes pour les ann\u00e9es entre 1980 et 2030:</p> Identifiant de l'ast\u00e9ro\u00efde Nom de l'ast\u00e9ro\u00efde Diam\u00e8tre minimum estim\u00e9 (km) Diam\u00e8tre maximum estim\u00e9 (km) Amplitude absolue h Lien d'information de l'ast\u00e9ro\u00efde 2001036 1036 Ganymed (A924 UB) 37.5452 83.9537 9.25 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001036 2000433 433 Eros (A898 PA) 21.8049 48.7573 10.43 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2000433 2001866 1866 Sisyphus (1972 XA) 8.64082 19.3215 12.44 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001866 2004954 4954 Eric (1990 SQ) 8.06408 18.0318 12.59 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2004954 2001627 1627 Ivar (1929 SH) 7.7724 17.3796 12.67 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001627 2003552 3552 Don Quixote (1983 SA) 6.80072 15.2069 12.96 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2003552 2002212 2212 Hephaistos (1978 SB) 5.73528 12.8245 13.33 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2002212 2025916 25916 (2001 CP44) 4.88152 10.9154 13.68 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2025916 2001980 1980 Tezcatlipoca (1950 LA) 4.61907 10.3286 13.8 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2001980 2005587 5587 (1990 SB) 4.57673 10.2339 13.82 https://ssd.jpl.nasa.gov/sbdb.cgi?sstr=2005587 <p>Ainsi, on s'aper\u00e7oit que des ast\u00e9ro\u00efdes gigantesques passent au-dessus notre t\u00eate sans m\u00eame que l'on ne s'en rende compte ! Certains d'entre vous remarqueront que certains noms d'ast\u00e9ro\u00efdes leur sont familiers (surtout si vous regardez la s\u00e9rie The Expanse).</p> <p>Maintenant regardons le nombre d'ast\u00e9ro\u00efdes nous visitant par mois (la figure est interactive n'h\u00e9sitez pas \u00e0 passer votre souris sur les barres):</p> Bokeh Plot <p>En voyant ce graphique on peut en tirer les points suivants:</p> <ul> <li>Plus les ann\u00e9es passent et plus on observe d'ast\u00e9ro\u00efdes (surement li\u00e9 \u00e0 une am\u00e9lioration de nos observatoires, m\u00eame si plus d'ast\u00e9ro\u00efdes peuvent nous croiser ces derni\u00e8res ann\u00e9es).</li> <li>Le nombre de passages pr\u00e9vus d'ast\u00e9ro\u00efdes est plus faible que ces 5 derni\u00e8res ann\u00e9es. Cela est peut-\u00eatre d\u00fb au fait que les efforts sont mis sur les ast\u00e9ro\u00efdes les plus gros (voir la figure suivante qui peut laisser penser cette ceci).</li> <li>Les ast\u00e9ro\u00efdes ne font en g\u00e9n\u00e9ral pas plus d'un passage par an.</li> </ul> <p>Allons maintenant plus en d\u00e9tail avec la figure suivante (elle aussi interactive):</p> Bokeh Plot <p>Les cat\u00e9gories d'ast\u00e9ro\u00efdes sont compl\u00e8tements arbitraires. C'est surtout que j'arrive \u00e0 me visualiser ces diff\u00e9rentes tailles avec ces cat\u00e9gories. On remarque que l'on d\u00e9tecte de plus en plus d'ast\u00e9ro\u00efdes dont la taille est inf\u00e9rieure \u00e0 500m. Tandis que lorsque l'on regarde les pr\u00e9visions on s'aper\u00e7oit que le nombre pr\u00e9vu est bien moindre. Surement parce que les plus gros ast\u00e9ro\u00efdes sont prioritaires pour la surveillance autour de notre plan\u00e8te.</p> <p>En addition \u00e0 ce graphique, j'ai calcul\u00e9 les nombres suivants:</p> <ul> <li>Ast\u00e9ro\u00efde unique rencontr\u00e9 et observ\u00e9 de 1980 \u00e0 2021-09-10 : 25810</li> <li>Ast\u00e9ro\u00efde unique dont la rencontre est pr\u00e9vue entre 2021-09-10 et 2030 : 11211</li> </ul> <p>Il y a malgr\u00e9 tout pas mal de visites de pr\u00e9vues, en esp\u00e9rant que les plus gros n'entrent pas en contact (pour les plus anxieux, ce n'est pas au programme).</p> <p>Voil\u00e0, c'est tout pour aujourd'hui, j'ai r\u00e9alis\u00e9 ce projet pour ma curiosit\u00e9 et je me suis dit que je pouvais vous le partager.</p> <p>J\u2019esp\u00e8re que cela aidera/inspirera certains d\u2019entre vous. Dans tous les cas, j'esp\u00e8re que cela a \u00e9t\u00e9 int\u00e9ressant.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2021/09-19-hackaviz/","title":"Hackaviz 2021","text":"<p>Dans ce nouvel article de blog, je vais vous pr\u00e9senter ma participation \u00e0 l'Hackaviz 2021. L'Hackaviz est un concours organis\u00e9 par l'association de visualisation de donn\u00e9es nomm\u00e9e Toulouse dataviz. Elle consiste \u00e0 raconter une histoire en utilisant des donn\u00e9es Toulousaines mises \u00e0 disposition.</p> <p>M'amusant \u00e0 faire des visualisations de donn\u00e9es sur mon temps libre, j'y ai donc particip\u00e9 en solo. Le code que j'ai utilis\u00e9 est disponible ici. Dans cet article de blog, je vais vous r\u00e9v\u00e9ler l'histoire que j'ai souhait\u00e9 raconter \u00e0 travers mon travail.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2021/09-19-hackaviz/#resultats","title":"R\u00e9sultats","text":"<p>La r\u00e9gion Occitanie est une r\u00e9gion bien ensoleill\u00e9e avec son chef-lieu \u00e9tant \u00e0 Toulouse. On s'attend donc \u00e0 ce que la ville soit la plus dynamique en termes de transactions fonci\u00e8res entre les ann\u00e9es 2016 et 2020.</p> <p>Voyons cela au travers de ma r\u00e9alisation\u00a0:</p> Bokeh Plot <p>\u00c9tant une figure interactive n'h\u00e9sitez pas \u00e0 passer vos souris sur les r\u00e9gions color\u00e9es pour avoir plus de d\u00e9tails (le nombre de transactions par type et la valeur moyenne de ces transactions).</p> <p>Ainsi, Toulouse n'est pas la ville (parmi les villes dont les donn\u00e9es sont disponibles) ayant le plus de transactions fonci\u00e8res. Montpellier (chef-lieu de l'ancienne r\u00e9gion Languedoc-Roussillon) et N\u00eemes sont l\u00e9g\u00e8rement devant (2332 pour Toulouse vs 2481 pour Montpellier vs 2729 pour N\u00eemes). Tandis que B\u00e9ziers avec ses 4251 transactions est largement devant les autres villes. En regardant plus en d\u00e9tail ce graphique, on s'aper\u00e7oit qu'except\u00e9 Toulouse, les villes les plus dynamiques fonci\u00e8rement sont les villes proches de la mer M\u00e9diterran\u00e9e. La valeur moyenne des transactions sur cette zone est globalement plus \u00e9lev\u00e9 que pour les villes moins dynamiques fonci\u00e8rement.</p> <p>Ainsi, le bassin m\u00e9diterran\u00e9en est une zone d'attraction pour les activit\u00e9s fonci\u00e8res.</p> <p>J\u2019esp\u00e8re que cela aidera/inspirera certains d\u2019entre vous. Dans tous les cas, j'esp\u00e8re que cela a \u00e9t\u00e9 int\u00e9ressant.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2021/12-08-badaboom-fireballs/","title":"Badaboom - Les boules de feu","text":"<p>Dans ce nouvel article de blog, je vais vous pr\u00e9senter la suite de mes travaux sur ma librairie badaboom). Cet article fait suite \u00e0 mon pr\u00e9c\u00e9dent article sur mon projet badaboom (vous pouvez retrouver cet article ici). Pour ce deuxi\u00e8me volet, j'ai r\u00e9alis\u00e9 l'analyse des boules de feu (fireballs en anglais). Les boules de feu sont des termes astronomiques d\u00e9signant des m\u00e9t\u00e9ores exceptionnellement brillants et suffisamment spectaculaires pour \u00eatre vus dans une zone tr\u00e8s \u00e9tendue.</p> <p>Pour ce travail, j'ai utilis\u00e9 la base de donn\u00e9es du CNEOS (appartenant \u00e0 la NASA). Je vais pour cet article, commencer par vous pr\u00e9senter les \u00e9l\u00e9ments pr\u00e9sents dans cette base de donn\u00e9es et finir par vous montrer les premiers r\u00e9sultats que j'ai obtenus (je mettrais \u00e0 jour ce post compte tenu de mes avanc\u00e9es).</p> <p>Note</p> <p>Vous pouvez recr\u00e9er tous les graphiques avec des informations mises \u00e0 jour. Pour cela, vous pouvez consulter le projet sur GitHub \u00e0 badaboom. J'utilise \u00e9galement ce d\u00e9p\u00f4t comme mod\u00e8le pour mes projets Python. N'h\u00e9sitez pas \u00e0 y jeter un \u0153il et \u00e0 me faire part de vos suggestions pour l'am\u00e9liorer !</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2021/12-08-badaboom-fireballs/#resultats","title":"R\u00e9sultats","text":"<p>Vu que cette base recense les impacts boules de feux, je me suis d'abord tourn\u00e9 vers les \u00e9nergies d\u00e9missions lumineuses et les \u00e9nergies d'impacts. Il est \u00e0 noter que j'ai mis ces \u00e9nergies \u00e0 l'\u00e9chelle logarithmique, car l'\u00e9chelle des valeurs et tellement vari\u00e9 que j'ai trouv\u00e9 plus simple et plus lisible de faire ainsi.</p> Bokeh Plot Bokeh Plot <p>On s'aper\u00e7oit que la majorit\u00e9 des boules de feu recens\u00e9es sont dans le m\u00eame ordre de grandeur avec quelques exceptions pouvant \u00e9mettre jusqu'\u00e0 375000 GJ d'\u00e9nergie lumineuse (avec une \u00e9nergie d'impact de 440kt). Ces valeurs sont difficiles \u00e0 appr\u00e9hender sans r\u00e9f\u00e9rentiels et me laisse r\u00eaveur sur l'effet que l'on doit ressentir si on a la chance d'observer en r\u00e9el un tel ph\u00e9nom\u00e8ne (tout en \u00e9tant \u00e0 une distance permettant d'\u00eatre en s\u00e9curit\u00e9 bien s\u00fbr \ud83d\ude1c).</p> <p>Avant de vous montrer la carte des impacts enregistr\u00e9s par le CNEOS, il faut noter que sur leur base de donn\u00e9es il manque des localisations pour certaines boules de feu. C'est pourquoi j'ai r\u00e9alis\u00e9 l'histogramme qui suit:</p> Bokeh Plot <p>On peut n\u00e9anmoins noter qu'il y a de moins en moins de d\u00e9tection r\u00e9alis\u00e9e sans localisation. C'est donc \u00e0 partir des donn\u00e9es poss\u00e9dant une g\u00e9olocalisation que j'ai r\u00e9alis\u00e9 la carte qui suit. Cette carte est interactive, en passant la souris dessus les bulles vous pouvez voir les d\u00e9tails pour chaque impact. Vous pouvez \u00e9galement zoomer sur la carte pour voir plus pr\u00e9cis\u00e9ment certaines r\u00e9gions du globe. La taille des bulles correspondant \u00e0 l'\u00e9nergie d'impact de la boule de feu.</p> <p>Note</p> <p>La carte ci-dessous a perdu son arri\u00e8re-plan car l'API pr\u00e9c\u00e9dente n'est plus disponible. J'ai mis \u00e0 jour l'API dans badaboom en utilisant Plotly avec Mapbox, rempla\u00e7ant ainsi la solution bas\u00e9e sur Bokeh. Vous pouvez recr\u00e9er la carte vous-m\u00eame. Je n'ai pas h\u00e9berg\u00e9 le r\u00e9sultat, car l'API devient payante apr\u00e8s un certain nombre de vues.</p> Bokeh Plot <p>Pour rappel tout le code n\u00e9cessaire pour r\u00e9aliser ces figures est disponible sur ma librairie badaboom).</p> <p>J\u2019esp\u00e8re que cela aidera/inspireront certains d\u2019entre vous. Dans tous les cas, j'esp\u00e8re que cela a \u00e9t\u00e9 int\u00e9ressant.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2022/04-03-hackaviz/","title":"Hackaviz 2022","text":"<p>Dans ce nouvel article de blog, je vais vous pr\u00e9senter ma participation \u00e0 l'Hackaviz 2022. L'Hackaviz est un concours organis\u00e9 par l'association de visualisation de donn\u00e9es nomm\u00e9e Toulouse dataviz. Elle consiste \u00e0 raconter une histoire en utilisant des donn\u00e9es mises \u00e0 disposition.</p> <p>M'amusant \u00e0 faire des visualisations de donn\u00e9es sur mon temps libre, j'y ai donc particip\u00e9 en solo. Vous trouverez ma participation \u00e0 l'ann\u00e9e pr\u00e9c\u00e9dente ici. Le code que j'ai utilis\u00e9 sera disponible d'ici peu.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2022/04-03-hackaviz/#resultats","title":"R\u00e9sultats","text":"<p>Cette ann\u00e9e, une base de donn\u00e9es des financements collect\u00e9s et distribu\u00e9s par les organismes de gestion collective des droits d\u2019auteur, au titre de la r\u00e9mun\u00e9ration pour copie priv\u00e9e est \u00e0 l'honneur.</p> <p>J'ai consacr\u00e9 mon travail sur le type d'aides accord\u00e9es, toutes organisations confondues. Ainsi j'ai r\u00e9alis\u00e9 un premier graphique (il est interactif vous pouvez utiliser votre souris ou doigt) montrant l'\u00e9volution du nombre d'aides accord\u00e9es par type:</p> Bokeh Plot <p>On remarque ici l'effet de la covid car en 2020 il y a eu une baisse du nombre d'aides accord\u00e9es (qui \u00e9taient \u00e0 la hausse pour tous les types chaque ann\u00e9e jusque l\u00e0). Avec les spectacles vivants souffrent le plus, ceci est certainement li\u00e9 aux confinements. Suite \u00e0 ce premier constat, j'ai voulu observer la r\u00e9partition du nombre d'aides avec le graphique suivant (lui aussi interactif):</p> Bokeh Plot <p>Ainsi avec ce graphique on se rend compte que la r\u00e9partition des financements est rest\u00e9e stable de 2016 \u00e0 2019 (ind\u00e9pendamment de l'augmentation du nombre d'aides allou\u00e9es). N\u00e9anmoins pour l'ann\u00e9e 2020 cette r\u00e9partition a chang\u00e9 en la d\u00e9faveur des diffusions de spectacle vivant et en faveur des financements de cr\u00e9ations. Tandis que la proportion de financements pour l'\u00e9ducation artistique et culturelle et la formation des artistes est rest\u00e9e stable.</p> <p>J\u2019esp\u00e8re que cela aidera/inspirera certains d\u2019entre vous. Dans tous les cas, j'esp\u00e8re que cela a \u00e9t\u00e9 int\u00e9ressant.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Visualisations"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/","title":"Mon passage \u00e0 Manjaro Linux","text":"<p>Je ne recommande pas cette distribution</p> <p>Apr\u00e8s deux ans d'utilisation, je ne recommande plus Manjaro en raison de probl\u00e8mes r\u00e9currents de s\u00e9curit\u00e9 et de stabilit\u00e9. Les utilisateurs rencontrent souvent des difficult\u00e9s avec les paquets de l'Arch User Repository (AUR), et les mises \u00e0 jour peuvent rendre votre syst\u00e8me instable. De plus, il existe des probl\u00e8mes persistants avec les certificats SSL, ce qui peut affecter la gestion des paquets et les connexions s\u00e9curis\u00e9es. Pour plus d'informations, vous pouvez consulter cette vid\u00e9o.</p> <p>Je laisse ce post en tant qu'archive, car il propose des solutions \u00e0 certains des probl\u00e8mes que les utilisateurs de Manjaro peuvent rencontrer.</p> <p>J'ai commenc\u00e9 mon aventure KDE avec Fedora (pendant 6 mois) o\u00f9 l'interface graphique \u00e9tait plus bugu\u00e9e que KDE Neon o\u00f9 je suis rest\u00e9 durant un an environ et j'ai fini dans Kubuntu LTS avec des applications flatpaks. Cela fait trois ans que j'utilise Kubuntu, bien que j'appr\u00e9cie certains de ses aspects (LTS, \u00e9tabli sur les paquets Ubuntu, facile \u00e0 configurer et avec de nombreuses ressources) mon exp\u00e9rience quotidienne \u00e9tait peu commode. Chaque fois que j'actualisais mes pilotes NVIDIA, je craignais d'avoir une interface graphique inutilisable (\u00e9cran totalement noir avec seulement des terminaux tty disponibles) car m\u00eame si tous les paquets n\u00e9cessaires pour le pilote NVIDIA ne sont pas dans les d\u00e9p\u00f4ts, l'installation est faite par apt. Pire encore, ces fichiers manquants peuvent prendre une semaine pour arriver dans les d\u00e9p\u00f4ts. Moins casse-t\u00eate, mais \u00e9galement ennuyeux, snap est forc\u00e9 via apt dans la distribution Ubuntu (comme <code>apt install firefox</code> ou <code>apt install chromium</code> r\u00e9sultent en une installation via snap). Enfin, avoir un environnement KDE d\u00e9pass\u00e9 de plusieurs versions n'est pas l'id\u00e9al si vous souhaitez b\u00e9n\u00e9ficier des derni\u00e8res corrections de bogues de l'environnement (leur \u00e9quipe fait de gros efforts pour corriger de nombreux bogues et am\u00e9liorer l'exp\u00e9rience utilisateur, vous pouvez consulter le blog de Nate pour avoir un aper\u00e7u de leurs efforts).</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#mes-besoins","title":"Mes besoins","text":"<p>Avant de vous dire pourquoi j'ai choisi Manjaro KDE, je vais vous exposer mes besoins :</p> <ul> <li> <p>une distribution stable qui dispose des applications n\u00e9cessaires \u00e0 tous mes besoins (je peux utiliser des flatpaks pour les applications manquantes), qui ne casse pas lorsque j'actualise mes pilotes et qui n\u00e9cessite un minimum de maintenance manuelle, \u00a0</p> </li> <li> <p>un moyen facile d'installer mon syst\u00e8me,</p> </li> <li>une grande communaut\u00e9 avec de la documentation et des wikis,</li> <li>Je ne veux pas r\u00e9installer mon syst\u00e8me tous les 6 mois (c'est pour cela que j'\u00e9tais sur Kubuntu LTS),</li> <li>un environnement KDE \u00e0 jour, mais pas forc\u00e9ment la derni\u00e8re version, sans ruptures ni bugs.</li> </ul> <p>Compte tenu de ces exigences, Manjaro et openSUSE semblent \u00eatre d'excellentes solutions. J'ai d\u00e9j\u00e0 utilis\u00e9 la documentation et les wikis d'Arch/Manjaro pour r\u00e9soudre certains de mes probl\u00e8mes avec Kubuntu (c'est une ironie car il n'y avait rien sur les forums d'Ubuntu/Kubuntu). De plus, en 2021 sur distrowatch, Manjaro semble \u00eatre une solution plus populaire (donc peut-\u00eatre plus de personnes pour aider en cas de probl\u00e8mes).</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#mon-experience","title":"Mon exp\u00e9rience","text":"<p>Comme aucune distribution n'est parfaite, il y a de bonnes choses \u00e0 prendre et de mauvaises \u00e0 surmonter. Tout d'abord le processus d'installation, ce n'est pas la meilleure interface utilisateur qui existe, mais je n'ai eu aucun probl\u00e8me ni aucune difficult\u00e9, tout \u00e9tait fluide. Ensuite vient le premier logiciel \u00e0 installer, en combinant les paquets officiels avec flatpak et AUR, je n'ai pas install\u00e9 un seul paquet manuellement. C'est une premi\u00e8re pour moi, le gestionnaire de logiciels recherche les mises \u00e0 jour de toutes mes applications (une chose tr\u00e8s pratique).</p> <p>Viennent ensuite les premiers soucis pour finaliser mon installation, que j'ai r\u00e9solus avec quelques recherches sur les forums/wikis Arch et Manjaro (je les ai list\u00e9s \u00e0 la fin de ce post pour les curieux). Enfin, viennent les routines et quelques changements d'habitudes.</p> <p>Dans l'ensemble, cela a \u00e9t\u00e9 une excellente exp\u00e9rience et je n'ai pas regrett\u00e9 mon changement. J'ai m\u00eame install\u00e9 Manjaro sur tous mes ordinateurs. Ce n'est s\u00fbrement pas une distribution pour d\u00e9butants (car vous devez faire peu de maintenance), mais une fois mis en place l'exp\u00e9rience utilisateur est excellente compar\u00e9e \u00e0 Kubuntu LTS (en termes de performance et de stabilit\u00e9 du syst\u00e8me). Si vous ne vous souciez pas de l'environnement syst\u00e8me, alors PopOS est une excellente alternative (adapt\u00e9e aux d\u00e9butants et aux experts). De plus, je n'ai pas \u00e0 m'inqui\u00e9ter des mises \u00e0 jour majeures entre les LTS comme avec Kubuntu, car Manjaro est une rolling release (l'inconv\u00e9nient est la maintenance pour certaines mises \u00e0 jour). Enfin, je n'ai pas peur de mettre \u00e0 jour mon syst\u00e8me, ce qui \u00e9tait ma principale raison de quitter Kubuntu !</p> <p>Dans les deux sous-sections restantes, j'expose un retour plus d\u00e9taill\u00e9 en listant tous les avantages et inconv\u00e9nients que j'ai dans mon utilisation quotidienne. Ensuite, j'\u00e9num\u00e8re les probl\u00e8mes que j'ai rencontr\u00e9s et comment je les ai r\u00e9solus (suivi par les sources que j'ai utilis\u00e9es pour r\u00e9soudre ces probl\u00e8mes).</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#utilisation-quotidienne","title":"Utilisation quotidienne","text":"","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#les-avantages","title":"Les avantages","text":"<ul> <li>M\u00eame interface que dans mon Kubuntu, mais avec un plasma plus r\u00e9cent (ce qui m'a permis de b\u00e9n\u00e9ficier de fonctionnalit\u00e9s plus r\u00e9centes).</li> <li>Vous pouvez s\u00e9lectionner votre noyau Linux et le changer facilement, avec un acc\u00e8s aux derniers noyaux (LTS et non LTS, qui peuvent contenir des pilotes pour les derni\u00e8res pi\u00e8ces de mat\u00e9riel).</li> <li>Pas de version plasma de pointe \u00e0 moins qu'elle ne soit pas stable (compar\u00e9 \u00e0 KDE Neon), c'est une victoire claire pour mes besoins.</li> <li>Mon syst\u00e8me fonctionne de mani\u00e8re beaucoup plus fluide sur mon ordinateur portable par rapport \u00e0 Kubuntu avec la m\u00eame autonomie de batterie !</li> <li>Le gestionnaire de paquets a une interface similaire \u00e0 <code>apt</code> : <code>pamac</code> (et non le gestionnaire <code>pacman</code>). Par cons\u00e9quent, il m'aide beaucoup pour ma transition, de plus <code>pamac</code> sugg\u00e8re des paquets optionnels \u00e0 installer (ce qui peut vous faire gagner du temps si vous manquez quelque chose).</li> <li>L'interpr\u00e9teur de commandes par d\u00e9faut de Manjaro, zsh, est pratique et utile pour moi (j'ai m\u00eame bascul\u00e9 de bash vers lui).</li> <li>Un large panel de logiciels et des int\u00e9grations pr\u00eates \u00e0 l'emploi (comme l'int\u00e9gration de languagetool + texstudio si vous installez les deux).</li> <li>Les mises \u00e0 jour NVIDIA se sont bien pass\u00e9es (m\u00eame apr\u00e8s 10 mois d'utilisation).</li> </ul>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#inconvenients","title":"Inconv\u00e9nients","text":"<ul> <li>Les paquets AUR peuvent \u00eatre cass\u00e9s lors de certaines mises \u00e0 jour. Cela arrive car ce n'est pas une source officielle de paquets support\u00e9s par l'\u00e9quipe Manjaro (mais y avoir acc\u00e8s est tr\u00e8s pratique). Comme j'utilise d'abord les paquets officiels, puis les applications flatpak et en dernier recours les paquets AUR, j'ai peu d'applications AUR, et ce n'est pas un probl\u00e8me pour moi. N\u00e9anmoins, les solutions pour r\u00e9soudre ce probl\u00e8me peuvent \u00eatre :</li> <li>Retarder la mise \u00e0 jour d'un paquet AUR pour conformer ses d\u00e9pendances dans les d\u00e9p\u00f4ts de Manjaro (car elles arrivent plus tard dans Manjaro que dans Arch). Je n'ai pas eu besoin de faire cela dans mon utilisation.</li> <li> <p>Vous pouvez avoir besoin de lancer manuellement une reconstruction d'un paquet AUR (<code>pamac remove &lt;paquet&gt;</code> puis <code>pamac build &lt;paquet&gt;</code>) apr\u00e8s une mise \u00e0 jour des d\u00e9pendances depuis les d\u00e9p\u00f4ts de Manjaro. Cela m'est arriv\u00e9 une fois jusqu'\u00e0 maintenant.</p> </li> <li> <p>Certaines mises \u00e0 jour peuvent casser certains de vos fichiers de configuration (car le format peut changer avec les nouvelles versions majeures). Cela peut se produire entre chaque mise \u00e0 jour de Kubuntu LTS, mais maintenant que je suis sur une distribution bas\u00e9e sur Arch, cela peut se produire \u00e0 chaque mise \u00e0 jour (pour chaque configuration sp\u00e9cifique que vous d\u00e9finissez sur votre syst\u00e8me). En pratique, cela ne m'est arriv\u00e9 qu'une fois \u00e0 cause d'une mise \u00e0 jour de gnome-keyring (dont j'esp\u00e8re pouvoir me d\u00e9barrasser avec plasma 5.26, car kwallet impl\u00e9mentera les protocoles manquants g\u00e9r\u00e9s par gnome-keyring). Une bonne habitude pour r\u00e9soudre ce genre de probl\u00e8mes est de suivre le flux officiel des versions de Manjaro (c'est un flux RSS). L\u00e0, ils d\u00e9crivent les mises \u00e0 jour avec les probl\u00e8mes possibles et la plupart du temps des solutions pour les r\u00e9soudre. Comme chaque version est associ\u00e9e \u00e0 un post sur le forum de Manjaro, vous pouvez demander de l'aide \u00e0 la communaut\u00e9 (s'il n'y a pas encore de solution \ud83d\ude04).</p> </li> <li> <p>Vous ne pouvez pas utiliser discover pour installer et mettre \u00e0 jour pour les nouvelles applications. J'\u00e9tais habitu\u00e9 \u00e0 cet outil, et cela s'int\u00e8gre bien dans l'environnement KDE. Mais sur les syst\u00e8mes bas\u00e9s sur Arch, son utilisation peut casser votre syst\u00e8me. Heureusement, il existe un gestionnaire similaire sur Manjaro qui r\u00e9pond \u00e0 tous mes besoins (recherche d'applications, de paquets et gestion des flatpaks). Sauf qu'il utilise une interface utilisateur gtk au lieu d'une interface qt, donc c'est un probl\u00e8me mineur.</p> </li> </ul>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#problemes-rencontres-et-solutions","title":"Probl\u00e8mes rencontr\u00e9s et solutions","text":"","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#activer-les-peripheriques-bluetooth-sur-lecran-de-connexion","title":"Activer les p\u00e9riph\u00e9riques Bluetooth sur l'\u00e9cran de connexion","text":"<p>Cette fonctionnalit\u00e9 est tr\u00e8s utile pour les utilisateurs de souris et/ou claviers Bluetooth (pour saisir les identifiants et s\u00e9lectionner les utilisateurs). Je pense qu'elle devrait \u00eatre activ\u00e9e par d\u00e9faut, mais vous pouvez l'activer rapidement :</p> <pre><code>sudo sed -i.back 's/#AutoEnable=false/AutoEnable=true/g' /etc/bluetooth/main.conf\n</code></pre>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#haut-parleurecouteur-bluetooth-gresillant-avec-une-mauvaise-qualite","title":"Haut-parleur/\u00e9couteur Bluetooth gr\u00e9sillant avec une mauvaise qualit\u00e9","text":"<p>Pour r\u00e9soudre ce probl\u00e8me, d\u00e9sactivez d'abord toutes les options d'\u00e9conomie d'\u00e9nergie sur les p\u00e9riph\u00e9riques Bluetooth (uniquement pour les utilisateurs de tlp). Pour faire cela facilement, vous pouvez installer <code>tlpui</code> :</p> <pre><code>pamac install tlpui\n</code></pre> <p>Ensuite, toujours dans tlpui -&gt; section audio, d\u00e9sactivez l'option \"sound power save controller\". Cela devrait am\u00e9liorer certains craquements, mais ce n'\u00e9tait pas suffisant de mon c\u00f4t\u00e9. Vous pouvez aussi regarder dans USB_DENYLIST pour ajouter votre r\u00e9cepteur Bluetooth (m\u00eame si vous utilisez un ordinateur portable ou que le Bluetooth est int\u00e9gr\u00e9 \u00e0 votre carte m\u00e8re).</p> <p>J'ai combin\u00e9 cette solution avec le remplacement de pulseaudio par pipewire (qui fournit des connexions \u00e0 faible latence, impl\u00e9mente plus de codecs Bluetooth et g\u00e8re tous les appels pulseaudio car pipewire est compatible avec presque toutes les API pulseaudio). Plus d'informations sur pipewire ici.</p> <p>Pour le faire sous Manjaro, vous devez taper les commandes suivantes :</p> <pre><code>pamac remove pulseaudio pulseaudio-jack pulseaudio-lirc pulseaudio-rtp pulseaudio-zeroconf pulseaudio-bluetooth pulseaudio-alsa pulseaudio-ctl manjaro-pulse plasma-pa\npamac install manjaro-pipewire\npamac install plasma-pa\n</code></pre> <p>Au moment o\u00f9 j'\u00e9cris ces lignes, nous devons d'abord supprimer <code>plasma-pa</code> pour pouvoir supprimer pulseaudio. Nous le r\u00e9installons une fois que pipewire est install\u00e9 pour pouvoir contr\u00f4ler l'audio via l'interface plasma de KDE.</p> <p>Puis red\u00e9marrez votre ordinateur et c'est fait. Toutes les applications utilisant pr\u00e9c\u00e9demment pulseaudio devraient fonctionner avec une meilleure latence et en utilisant de meilleurs codecs \ud83d\ude04.</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#adresser-les-lags-des-peripheriques-bluetooth-tel-que-les-lags-de-souris-bluetooth","title":"Adresser les lags des p\u00e9riph\u00e9riques Bluetooth tel que les lags de souris Bluetooth","text":"<p>Tout d'abord, si vous utilisez TLP, d\u00e9sactivez la fonction powersave pour votre p\u00e9riph\u00e9rique \u00e0 l'aide de tlpui (regardez la liste USB_DENYLIST pour ajouter votre r\u00e9cepteur Bluetooth). Ensuite, pour r\u00e9soudre ce probl\u00e8me, nous allons d\u00e9finir un service qui r\u00e9duit la latence Bluetooth au niveau du noyau Linux pour tous les p\u00e9riph\u00e9riques. Enfin, nous aborderons les codecs Bluetooth pour les haut-parleurs et leurs interf\u00e9rences avec les autres p\u00e9riph\u00e9riques Bluetooth.</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#definir-un-service-pour-forcer-une-faible-latence","title":"D\u00e9finir un service pour forcer une faible latence","text":"<p>Pour ce faire, nous devons ajouter le script suivant dans <code>/etc/systemd/system/fix-mouse-lag.service</code> pour cr\u00e9er notre service :</p> <pre><code>[Unit\u00e9]\nDescription=ex\u00e9cuter le script racine au d\u00e9marrage/r\u00e9veil pour corriger le d\u00e9calage de la souris\nAvant=bluetooth.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/sleep 2\nExecStart=/usr/local/bin/fix-mouse-lag.sh\n\n[Installer]\nWantedBy=bluetooth.service\n</code></pre> <p>Ensuite, ajoutez le script suivant dans <code>/usr/local/bin/fix-mouse-lag.sh</code> qui sera ex\u00e9cut\u00e9 par notre service \u00e0 chaque d\u00e9marrage/r\u00e9veil de veille :</p> <pre><code>#!/bin/sh\n\necho 0 &gt; /sys/kernel/debug/bluetooth/hci0/conn_latency\necho 6 &gt; /sys/kernel/debug/bluetooth/hci0/conn_min_intervalle\necho 7 &gt; /sys/kernel/debug/bluetooth/hci0/conn_max_interval\n</code></pre> <p>Enfin, activons et d\u00e9marrons notre service :</p> <pre><code>sudo chown root:root /etc/systemd/system/fix-mouse-lag.service\nsudo chmod a+rx /usr/local/bin/fix-mouse-lag.sh\nsudo systemctl enable fix-mouse-lag.service --now\n</code></pre>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#selection-dun-codec-bluetooth-pour-mon-casque-audio","title":"S\u00e9lection d'un codec Bluetooth pour mon casque audio","text":"<p>Je poss\u00e8de un Bose QC 35 II, et sur le papier il ne peut \u00eatre utilis\u00e9 qu'avec les codecs HSP/HFC, AAC et SBC. Le premier \u00e9tant un codec de basse qualit\u00e9 qui permet d'utiliser le microphone du casque audio, et les derniers sont r\u00e9serv\u00e9s \u00e0 une sortie audio de haute qualit\u00e9. Avec mes tests, j'ai d\u00e9couvert qu'il peut aussi g\u00e9rer le codec SBC XQ (qui est un meilleur codec compar\u00e9 \u00e0 AAC et SBC, voir ce lien pour plus de d\u00e9tails).</p> <p>J'ai test\u00e9 ces trois codecs de haute qualit\u00e9 avec une Logitech MX Vertical et j'ai constat\u00e9 que si les codecs SBC et SBC XQ ne perturbent pas la connexion de ma souris, le codec AAC, lui, le fait. Pipewire utilise le codec AAC par d\u00e9faut, car Bose le recommande pour ce casque<sup>1</sup>. Pour modifier ce comportement, nous devons \u00e9diter le fichier <code>/usr/share/pipewire/media-session.d/bluez-monitor.conf</code>. J'y ai d\u00e9comment\u00e9 la ligne <code>bluez5.enable-sbc-xq = true</code> et sp\u00e9cifi\u00e9 les <code>bluez5.codecs</code> comme suit :</p> <p><code>bluez5.codecs = [ sbc_xq ldac aptx aptx_hd aptx_ll aptx_ll_duplex faststream faststream_duplex ]</code></p> <p>J'ai donc supprim\u00e9 la capacit\u00e9 SBC et AAC de Pipewire pour \u00eatre s\u00fbr qu'il n'utilisera pas ces codecs avec mes appareils (le premier n'est pas un probl\u00e8me, mais je pr\u00e9f\u00e8re une qualit\u00e9 sup\u00e9rieure pour mon casque \ud83d\ude1b).</p>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#corriger-le-bug-des-themes-ne-fonctionnant-pas-avec-les-applications-flatpak","title":"Corriger le bug des th\u00e8mes ne fonctionnant pas avec les applications flatpak","text":"<p>La plupart des applications flatpak n'auront pas leur th\u00e8me suivant celui du syst\u00e8me sur Manjaro KDE. En particulier les applications gtk et electron. Si ce probl\u00e8me n'est pas trait\u00e9 en amont sur Arch, les d\u00e9veloppeurs de Manjaro ne le corrigeront pas (voir mes sources pour plus de d\u00e9tails). Pour r\u00e9soudre ce probl\u00e8me, nous devons taper les commandes suivantes :</p> <pre><code>flatpak override --filesystem=xdg-config/gtk-3.0:ro\nflatpak override --filesystem=xdg-config/gtk-4.0:ro\n</code></pre> <p>Il ajoute aux applications flatpak les permissions minimales dont elles ont besoin pour voir le th\u00e8me utilis\u00e9 dans l'environnement KDE. Ensuite, vous devez installer le th\u00e8me flatpak GTK correspondant \u00e0 celui que vous utilisez sous votre syst\u00e8me. Par exemple, si vous utilisez le th\u00e8me breeze-dark sous KDE, vous devez installer la version du th\u00e8me flatpak pour les applications gtk comme ceci :</p> <pre><code>flatpak -y install org.gtk.Gtk3theme.Breeze-Dark\n</code></pre>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/09-01-my-switch-to-manjaro/#mes-sources","title":"Mes sources","text":"<ul> <li> <p>Utilisation de AUR sous Manjaro</p> </li> <li> <p>Activer le Bluetooth au d\u00e9marrage</p> </li> <li> <p>R\u00e9duire les d\u00e9lais de saisie de la souris</p> </li> <li> <p>R\u00e9soudre le probl\u00e8me de th\u00e9matisation de flatpak (par votre serviteur)</p> </li> </ul> <ol> <li> <p>Notez que je ne parle que pour le cas pipewire. Je ne l'ai pas test\u00e9 pour pulseaudio car j'ai des probl\u00e8mes de cr\u00e9pitement avec ce dernier.\u00a0\u21a9</p> </li> </ol>","tags":["Archives","Linux"]},{"location":"fr/blog/2022/10-30-my-phd-experience/","title":"L'exp\u00e9rience de mon doctorat","text":"<p>Cela fait un moment, pas de bilan de deuxi\u00e8me ann\u00e9e en raison de sentiments mitig\u00e9s pendant le covid. Mais gr\u00e2ce \u00e0 mes encadrants, j'ai r\u00e9ussi \u00e0 surmonter mes pens\u00e9es n\u00e9gatives. Je suis maintenant fier d'\u00eatre un docteur en informatique \ud83c\udf89.</p> <p>Pour rappel, cette th\u00e8se a \u00e9t\u00e9 encadr\u00e9 par Julien Pinquier, J\u00e9r\u00f4me Farinas et Virginie Woisard. Elle a \u00e9t\u00e9 r\u00e9alis\u00e9 \u00e0 l'IRIT (Toulouse, France).</p> <p>Dans ce post, je vais partager avec vous un r\u00e9sum\u00e9 de mon travail, quelques sentiments et conseils que j'ai \u00e0 partager.</p>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2022/10-30-my-phd-experience/#resume-de-ma-these","title":"R\u00e9sum\u00e9 de ma th\u00e8se","text":"<p>Les personnes atteintes de cancers des voies a\u00e9rodigestives sup\u00e9rieures pr\u00e9sentent des difficult\u00e9s de prononciation apr\u00e8s des chirurgies ou des radioth\u00e9rapies. Il est important pour le praticien de pouvoir disposer d\u2019une mesure refl\u00e9tant la s\u00e9v\u00e9rit\u00e9 de la parole. Pour produire cette mesure, il est commun\u00e9ment pratiqu\u00e9 une \u00e9tude perceptive qui rassemble un groupe de cinq \u00e0 six experts cliniques. Ce proc\u00e9d\u00e9 limite l'usage de cette \u00e9valuation en pratique. Ainsi, la cr\u00e9ation d'une mesure automatique, semblable \u00e0 l'indice de s\u00e9v\u00e9rit\u00e9, permettrait un meilleur suivi des patients en facilitant son obtention.</p> <p>Pour r\u00e9aliser une telle mesure, nous nous sommes appuy\u00e9s sur une t\u00e2che de lecture, classiquement r\u00e9alis\u00e9e. Nous avons utilis\u00e9 les enregistrements du corpus cancer qui rassemble plus de 100 personnes <sup>1</sup>. Ce corpus repr\u00e9sente environ une heure d'enregistrement pour mod\u00e9liser l'indice de s\u00e9v\u00e9rit\u00e9.</p> <p>Dans ce travail de doctorat, une revue des m\u00e9thodes de l'\u00e9tat de l'art sur la reconnaissance de la parole, des \u00e9motions et du locuteur utilisant peu de donn\u00e9es a \u00e9t\u00e9 entreprise. Nous avons ensuite essay\u00e9 de mod\u00e9liser la s\u00e9v\u00e9rit\u00e9 \u00e0 l'aide d'apprentissage par transfert et par apprentissage profond. Les r\u00e9sultats \u00e9tant non utilisables, nous nous sommes tourn\u00e9 sur les techniques dites &lt;&lt;~few shot~&gt;&gt; (apprentissage \u00e0 partir de quelques exemples seulement). Ainsi, apr\u00e8s de premiers essais prometteurs sur la reconnaissance de phon\u00e8mes <sup>2</sup>, nous avons obtenu des r\u00e9sultats prometteurs pour cat\u00e9goriser la s\u00e9v\u00e9rit\u00e9 des patients. N\u00e9anmoins, l'exploitation de ces r\u00e9sultats pour une application m\u00e9dicale demanderait des am\u00e9liorations.</p> <p>Nous avons donc r\u00e9alis\u00e9 des projections des donn\u00e9es de notre corpus. Comme certaines tranches de scores \u00e9taient s\u00e9parables \u00e0 l'aide de param\u00e8tres acoustiques, nous avons propos\u00e9 une nouvelle m\u00e9thode <sup>3</sup>. Celle-ci est fond\u00e9e sur des repr\u00e9sentations de la parole autoapprise sur le corpus Librispeech : le mod\u00e8le PASE+, qui est inspir\u00e9 de l\u2019Inception Score (g\u00e9n\u00e9ralement utilis\u00e9 en image pour \u00e9valuer la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es par les mod\u00e8les). Notre m\u00e9thode nous permet de produire un score semblable \u00e0 l'indice de s\u00e9v\u00e9rit\u00e9 avec une corr\u00e9lation de Spearman de 0,87 sur la t\u00e2che de lecture du corpus cancer. L'avantage de notre approche est qu'elle ne n\u00e9cessite pas des donn\u00e9es du corpus cancer pour l'apprentissage. Ainsi, nous pouvons utiliser l'enti\u00e8ret\u00e9 du corpus pour l'\u00e9valuation de notre syst\u00e8me. La qualit\u00e9 de nos r\u00e9sultats nous a permis d\u2019envisager une utilisation en milieu clinique \u00e0 travers une application sur tablette : des tests sont d'ailleurs en cours \u00e0 l'h\u00f4pital Larrey de Toulouse.</p> <p>Mots cl\u00e9s: Parole pathologique, indice de s\u00e9v\u00e9rit\u00e9, trouble de la parole, cancer ORL, apprentissage profond, apprentissage avec quelques exemples, auto supervis\u00e9, mesure entropique, few-shot, peu de donn\u00e9es, quantit\u00e9 de donn\u00e9es limit\u00e9s, traitement automatique de la parole.</p> <p>Pour mon manuscrit, j'ai utilis\u00e9 Latex (comme tous mes articles). Pour collaborer avec d'autres personnes (notamment mes encadrants) sur celui-ci, j'ai utilis\u00e9 overleaf pour avoir des commentaires collaboratif (plus d'informations ici), un historique des modifications et pour synchroniser mon travail en cours sur GitHub (par s\u00e9curit\u00e9 car je ne voulais pas r\u00e9\u00e9crire ce manuscrit depuis le d\u00e9but). Une fois mon manuscrit mis en ligne, j'ajouterai un lien vers ce dernier ici pour les personnes int\u00e9ress\u00e9es.</p>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2022/10-30-my-phd-experience/#la-presentation-de-ma-these","title":"La pr\u00e9sentation de ma th\u00e8se","text":"<p>Comme pour mon manuscrit, j'ai utilis\u00e9 Latex avec beamer pour ma pr\u00e9sentation! Ce n'est pas un choix courant (je n'ai pas vu de pr\u00e9sentation de th\u00e8se utilisant Latex) mais si on est habitu\u00e9, on gagne tellement de temps pour se concentrer sur le fond plut\u00f4t que sur la forme. J'ai cr\u00e9\u00e9 mon propre th\u00e8me (pas \u00e0 partir de z\u00e9ro je ne suis pas un fou \ud83e\udd23) et il \u00e9tait presque termin\u00e9 avant de travailler sur ma pr\u00e9sentation de th\u00e8se (car j'utilisais beamer pour mes pr\u00e9sentations hebdomadaires avec mes encadrants).</p> <p>Maintenant parlons un peu de mon ressenti, j'\u00e9tais stress\u00e9 c'est s\u00fbr, ce moment m'intimidait... C'est tellement formel, surtout le temps pour r\u00e9aliser la pr\u00e9sentation (45 min dans mon cas). D'habitude quand je pr\u00e9sente des choses, si je prends 5/10 de minutes de plus ou de moins que pr\u00e9vu, c'est OK. Mais l\u00e0, je n'avais pas cette marge d'erreur. Cela m'a stress\u00e9 et m\u00eame si j'ai beaucoup r\u00e9p\u00e9t\u00e9 pour cette pr\u00e9sentation, plus je me pr\u00e9parais, plus je me sentais stress\u00e9. Ce qui est amusant, c'est que lorsqu'il y avait des probl\u00e8mes (des gens qui arrivaient alors que j'avais commenc\u00e9, des diapositives manquantes, des diapositives qui n'\u00e9taient pas partag\u00e9es au d\u00e9but et ainsi de suite), cela me faisait oublier les aspects stressants de l'exp\u00e9rience. Pour ceux qui n'\u00e9taient pas l\u00e0, vous pouvez voir ma pr\u00e9sentation/\"performance\":</p> <p>Je pense que j'aurais pu r\u00e9duire mon stress si j'avais fait une r\u00e9p\u00e9tition dans l'amphith\u00e9\u00e2tre et utilis\u00e9 un chronom\u00e8tre num\u00e9rique (avec un grand \u00e9cran) lors de ma r\u00e9p\u00e9tition et le jour de la soutenance.</p> <p>J'\u00e9tais plus \u00e0 l'aise avec les questions, l\u00e0 je n'\u00e9tais pas limit\u00e9 par le temps pour r\u00e9pondre au jury. De plus, le fait que ce soit interactif r\u00e9duit l'aspect formel de l'\u00e9preuve. Puis, apr\u00e8s la d\u00e9lib\u00e9ration du jury, j'ai re\u00e7u mon titre de docteur ! Quel sentiment incroyable, un m\u00e9lange de joie, de stress et de soulagement. Tout cela m'a fait pleurer \u00e0 la fin, quel grand moment !</p> <p>J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p> <ol> <li> <p>I participated in the analysis of the dataset in hal-02921918 \u21a9</p> </li> <li> <p>I conducted the review of state-of-the-art techniques and the experiments that lead to this paper:  DOI:10.1186/s13636-022-00251-w \u21a9</p> </li> <li> <p>This work led to the following publication (yet to be published): Roger, V., Farinas, J., Woisard, V., and Pinquier, J. (2022b). Cr\u00e9ation d\u2019une mesure entropique de la parole pour \u00e9valuer l\u2019intelligibilit\u00e9 de patients atteints de cancers des voies a\u00e9rodigestives sup\u00e9rieures. In 34e Journ\u00e9es d\u2019\u00c9tudes sur la Parole (JEP2022).\u00a0\u21a9</p> </li> </ol>","tags":["Archives","Th\u00e8se"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/","title":"Mes visualisations du tournoi Red Bull Wololo legacy","text":"<p>Pendant mon temps libre, je joue \u00e0 Age of Empires 2 (le jeu vid\u00e9o de mon enfance). J'aime aussi regarder des tournois avec des joueurs professionnels. R\u00e9cemment, il y a eu le plus grand tournoi d'Age of Empires 2 organis\u00e9 par Red Bull : Red Bull Wololo Legacy. Ce tournoi comprenait une cagnotte de 200 000 $ ! Plut\u00f4t bien pour un jeu vieux de 25 ans.</p> <p>Comme vous pouvez le voir sur mon blog (\u00e9galement sur Twitter et Reddit), j'aime publier des visualisations de donn\u00e9es. Aussi, j'ai int\u00e9gr\u00e9 une association de data visualisation (Toulouse dataviz). Pour me perfectionner en data visualisation, pourquoi ne pas fusionner ces deux passe-temps ?</p> <p>Nous y voil\u00e0, dans ce post, je montrerai toutes les visualisations de donn\u00e9es que j'ai faites pour ce Tournoi. Dans ce post, vous avez les graphiques am\u00e9lior\u00e9s (principalement des changements esth\u00e9tiques) compar\u00e9s \u00e0 ceux que j'ai publi\u00e9s sur Twitter et Reddit. Pour r\u00e9aliser ces graphiques, j'ai utilis\u00e9 les donn\u00e9es de Liquipedia, g\u00e9n\u00e9r\u00e9 des graphiques avec Plotly et les ai affin\u00e9s (avec des logos, des fonds sur les figures...) avec Inkscape. J'ai choisi d'utiliser les deux couleurs principales du logo Red Bull pour mes cr\u00e9ations, car \u00e0 l'\u00e9poque, je pensais que cela pourrait \u00eatre int\u00e9ressant de cr\u00e9er une sorte de s\u00e9rie.</p> <p>Je suis heureux que cela ait si bien fonctionn\u00e9, ce billet est donc le d\u00e9but d'une s\u00e9rie de billets sur les dataviz pour les tournois Age of Empires 2 !</p> <p>Assez parl\u00e9, regardons maintenant mon travail \ud83d\ude03.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#realisations-des-joueurs","title":"R\u00e9alisations des joueurs","text":"<p>Ma premi\u00e8re dataviz \u00e9tait de montrer les r\u00e9alisations de chaque joueur concernant leurs classements ainsi que le nombre de parties qu'ils ont gagn\u00e9es, perdues. Elle inclut \u00e9galement le prix qu'ils ont gagn\u00e9. Je suis assez content de celle-ci, car elle parait agr\u00e9able et poss\u00e8de de multiples informations.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#matchs-de-civilisations","title":"Matchs de civilisations","text":"<p>Ensuite, nous avons la dataviz avec le plus de retours (positifs et n\u00e9gatifs). Dans ce travail, j'ai cr\u00e9\u00e9 une matrice de confusion et comme elles sont sym\u00e9triques, j'ai supprim\u00e9 la redondance (une habitude comme gars de l'apprentissage automatique \ud83d\ude05). Je trouve le r\u00e9sultat agr\u00e9able, mais je ne suis pas convaincu de la lisibilit\u00e9 (j'ai re\u00e7u quelques commentaires sur ce point). Comme il a n\u00e9cessit\u00e9 un effort important dans Inkscape, il est possible que je n'inclue pas celui-ci (tel quel ou pas du tout) pour les prochains tournois.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#quelle-civilisation-a-ete-jouee-sur-chaque-carte","title":"Quelle civilisation a \u00e9t\u00e9 jou\u00e9e sur chaque carte ?","text":"<p>Ensuite, nous avons une matrice de quelle civilisation a \u00e9t\u00e9 jou\u00e9e pour chaque carte. Simple et efficace, j'aime le contenu, mais peu l'esth\u00e9tique. Je vais essayer d'am\u00e9liorer cela pour les prochains tournois.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#le-ratio-killdeath-de-chaque-civilisation-jouee","title":"Le ratio Kill/death de chaque civilisation jou\u00e9e","text":"<p>Celui qui a cr\u00e9\u00e9 de nombreuses discussions sur les strat\u00e9gies et les \u00e9quilibres des civilisations. J'adore discuter et voir les explications des autres, \u00e7a m'aide comme joueur, et c'est toujours amusant \ud83d\ude04.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#la-roue-de-la-victoire","title":"La roue de la victoire","text":"<p>Enfin, mon dernier travail, une roue des civilisations victorieuses sur chaque carte. Je l'ai fait sous forme de roue pour \u00e9viter les blancs d'un \u00e9quivalent utilisant des matrices. Le r\u00e9sultat est une cr\u00e9ation esth\u00e9tique (je suis partial, car je suis content de l'aspect) et informative. Elle comprend les proportions de parties sur chaque carte et les proportions de victoire des civilisations sur chaque carte.</p> <p>Ce graphique demandera du travail pour les prochains tournois (pour que les personnes voient les chiffres de m\u00eame que les proportions), mais c'\u00e9tait amusant \u00e0 faire.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-08-redbull-wololo-legacy/#quelle-est-la-suite","title":"Quelle est la suite ?","text":"<p>Pour cet \u00e9v\u00e9nement, j'ai publi\u00e9 simultan\u00e9ment sur Twitter et Reddit. Mes cr\u00e9ations ont obtenu plus de vues et de r\u00e9actions sur Reddit. \u00c0 titre de comparaison, ma premi\u00e8re visualisation a \u00e9t\u00e9 vue 30 fois sur Twitter (sans interactions) et plus de 35 000 sur Reddit ! Sur Reddit, ces dataviz ont g\u00e9n\u00e9r\u00e9 des discussions sur le tournoi. J'ai aussi re\u00e7u des retours avec de bons conseils pour am\u00e9liorer mon travail (ce qui sera vu pour les prochains tournois).</p> <p>Si vous ne l'avez pas encore vu, le prochain tournoi que je couvrirai sera Warlords.</p> <p>J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-13-warlords/","title":"Mes visualisations du tournoi Warlords","text":"<p>Ce billet est le second \u00e9pisode de ma s\u00e9rie de tournois Age of Empires 2 (jetez un coup d'\u0153il \u00e0 mon premier si vous l'avez manqu\u00e9). Tout le travail effectu\u00e9 ici concerne l'\u00e9v\u00e9nement principal (c'est ce qui m'int\u00e9resse le plus dans les tournois).</p> <p>Ici, les deux couleurs principales que j'ai choisies dans le logo donnent des visualisations plus agr\u00e9ables (du moins \u00e0 mon avis). Cette fois, j'ai am\u00e9lior\u00e9 mes visualisations sur les aspects suivants :</p> <ul> <li>utiliser une meilleure police de caract\u00e8res (ici Nimbus Sans) ;</li> <li>\u00e9viter les titres dans mes graphiques, comme nous les avons dans le post Reddit et sur ce blog ;</li> <li>int\u00e9grer le logo du tournoi dans chaque visualisation :</li> <li>int\u00e9grer des informations sur les donn\u00e9es utilis\u00e9es (ici je n'ai utilis\u00e9 que les donn\u00e9es de Liquipedia, mais je pourrais ajouter d'autres sources dans les tournois futurs) ;</li> <li>augmenter la taille de la police pour \u00eatre plus facile \u00e0 lire (peut-\u00eatre pas assez pour les smartphones, cependant).</li> </ul> <p>Assez parl\u00e9, regardons maintenant mon travail \ud83d\ude03.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-13-warlords/#realisations-des-joueurs","title":"R\u00e9alisations des joueurs","text":"<p>Ma premi\u00e8re visualisation \u00e9tait celle qui montrait les r\u00e9alisations de chaque joueur concernant leur rang \u00e0 c\u00f4t\u00e9 du nombre de matchs qu'ils ont gagn\u00e9, perdu. Ici, Classic Pro a d\u00fb d\u00e9clarer forfait et Jordan a eu une victoire automatique. J'ai essay\u00e9 des choses pour l'illustrer, mais je n'\u00e9tais pas satisfait du r\u00e9sultat, alors j'ai fini avec le suivant :</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2022/12-13-warlords/#quelle-civilisation-a-ete-jouee-sur-chaque-carte","title":"Quelle civilisation a \u00e9t\u00e9 jou\u00e9e sur chaque carte ?","text":"<p>Ensuite, nous avons la matrice de quelle civilisation a \u00e9t\u00e9 jou\u00e9e pour chaque carte. Dans ce tournoi, les civilisations qui n'ont pas \u00e9t\u00e9 jou\u00e9es sont : Goths, Slaves, Vietnamiens, Huns, Celtes et Siciliens. Ici, nous avons une \u00e9norme matrice, car nous avons plus de cartes par rapport au Red Bull Wololo Legacy. Cette matrice est maintenant plus simple \u00e0 lire, mais l'esth\u00e9tique n'est pas suffisante pour moi (peut-\u00eatre que la prochaine fois sera meilleure).</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/","title":"Mes visualisations du tournoi The Grand Melee","text":"<p>Ce billet est le troisi\u00e8me \u00e9pisode de ma s\u00e9rie de tournois Age of Empires 2 (jetez un coup d'\u0153il \u00e0 mon premier et \u00e0 mon second si vous les avez manqu\u00e9s). Tout le travail effectu\u00e9 ici concerne l'\u00e9v\u00e9nement principal (c'est ce qui m'int\u00e9resse le plus dans les tournois). Pour rappel, je publie la majorit\u00e9 de ces visualisations sur Reddit avant de r\u00e9aliser ce post. Ce dernier est un r\u00e9sum\u00e9 du tournoi et des retours que j'ai pu avoir.</p> <p>Cette fois, j'ai am\u00e9lior\u00e9 mes visualisations gr\u00e2ce \u00e0 des donn\u00e9es suppl\u00e9mentaires venant d'Age of Empires Captains Mode lorsque c'\u00e9tait utile et en ajoutant un QR code me servant de signature pour mes \u0153uvres.</p> <p>Regardons maintenant ce travail \ud83d\ude03.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/#realisations-des-joueurs","title":"R\u00e9alisations des joueurs","text":"<p>Ma premi\u00e8re visualisation est celle montrant chaque rang obtenu des joueurs avec le nombre de matchs qu'ils ont gagn\u00e9, perdu. Ici, il y avait moins de joueurs pour l'\u00e9v\u00e8nement principal, mais malgr\u00e9 tout nous avons pu assister \u00e0 de beaux matchs.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/#quelle-civilisation-a-ete-jouee-sur-chaque-carte","title":"Quelle civilisation a \u00e9t\u00e9 jou\u00e9e sur chaque carte ?","text":"<p>Ensuite, nous avons un sankey sur quelle civilisation a \u00e9t\u00e9 jou\u00e9e pour chaque carte. Dans ce tournoi, les civilisations qui n'ont pas \u00e9t\u00e9 jou\u00e9es sont : Celtes, Chinois, Hongrois, Malais, Maliens, Siciliens, Slaves, Espagnols, Teutons, Vietnamiens et Vikings. J'ai pr\u00e9f\u00e9r\u00e9 le sankey \u00e0 la matrice de chaleur, car la matrice poss\u00e8de beaucoup de z\u00e9ros. \u00c0 voir si les sankey sont plus lisibles sur des \u00e9v\u00e8nements avec plus de matchs \ud83d\ude09</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/#quelles-cartes-ont-ete-populaires","title":"Quelles cartes ont \u00e9t\u00e9 populaires ?","text":"<p>Ici, j'ai rajout\u00e9 les informations de ban venant des drafts des joueurs. Ceci a plu, mais j'aimerais trouver un affichage plus adapt\u00e9 pour la suite.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/#tuermourir-de-chaque-civilisation-jouee","title":"Tuer/mourir de chaque civilisation jou\u00e9e","text":"<p>Ici, j'ai rajout\u00e9 les informations de snipe et ban venant des drafts des joueurs. J'aime toujours autant les discussions que ce graphe g\u00e9n\u00e8re \ud83d\ude04. J'ai eu quelques pistes pour am\u00e9liorer ce graphe (dont avoir les snipe/bans/mort comme \u00e9tant des valeurs n\u00e9gatives), nous verrons au prochain tournois si j'obtiens de meilleures visualisations.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/01-16-the_grand_melee/#la-roue-de-la-victoire","title":"La roue de la victoire","text":"<p>Enfin, la roue de la victoire ! Comme mon pr\u00e9c\u00e9dent post, c'est une exclusivit\u00e9 pour ceux qui lisent ce post \ud83d\ude09. Vous pouvez survoler chaque segment avec votre souris pour recueillir des informations, et m\u00eame cliquer sur les cat\u00e9gories pour examiner une carte ou une cat\u00e9gorie de carte sp\u00e9cifique (cliquez sur le logo du tournoi pour revenir en arri\u00e8re). N'h\u00e9sitez pas \u00e0 me donner des conseils pour am\u00e9liorer ce travail.</p> <p>J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Age of Empires","Visualisations"]},{"location":"fr/blog/2023/12-03-tools-python-dataviz/","title":"Outils de visualisation de donn\u00e9es pour Python","text":"<p>Bonjour, cela fait un moment !</p> <p>R\u00e9cemment, j'ai fait une pr\u00e9sentation sur les biblioth\u00e8ques Python pour la visualisation de donn\u00e9es. Je les ai compar\u00e9es et j'ai pr\u00e9sent\u00e9 des exemples d'utilisation. Tout le code et les exemples sont disponibles ici. Si par hasard les diapositives ne vous conviennent pas, vous pouvez t\u00e9l\u00e9charger la version pdf (sans \u00e9l\u00e9ments anim\u00e9s).</p> <p>Vous pouvez \u00e9galement voir ma pr\u00e9sentation :</p> <p>Il s'agissait d'une pr\u00e9sentation faite pour l'association Toulouse Dataviz. J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Archives","Python","Visualisations"]},{"location":"fr/blog/2024/06-02-diffusers-unconditional-model/","title":"G\u00e9n\u00e9rer des images avec un mod\u00e8le non conditionn\u00e9","text":"<p>Ces derni\u00e8res ann\u00e9es, les mod\u00e8les de diffusion se sont impos\u00e9s comme un outil puissant dans le domaine de la mod\u00e9lisation g\u00e9n\u00e9rative, rivalisant souvent avec les GAN (Generative Adversarial Networks) dans la production d'images de haute qualit\u00e9 (voir this paper de 2021 et stable diffusion de 2022). Ces mod\u00e8les fonctionnent en simulant un processus de diffusion o\u00f9 les donn\u00e9es sont progressivement bruit\u00e9es puis d\u00e9bruit\u00e9es pour g\u00e9n\u00e9rer de nouveaux \u00e9chantillons.</p> <p>De nouveaux cours apparaissent pour expliquer le fonctionnement de ces m\u00e9thodes. L'un d'entre eux est How Diffusion Models Works de DeepLearning.ai. Il fournit d'excellents carnets de notes pour cr\u00e9er notre premier mod\u00e8le de diffusion/pipeline. Cependant, il est fastidieux de mettre en \u0153uvre une telle approche \u00e0 partir de z\u00e9ro en utilisant uniquement la biblioth\u00e8que PyTorch (mais c'est un excellent moyen d'apprendre le fonctionnement d'une telle m\u00e9thode).</p> <p>Dans cet article de blog, nous allons nous plonger dans l'impl\u00e9mentation d'un UNet inconditionnel en utilisant la biblioth\u00e8que Diffusers de deeplearning.ai. Nous allons parcourir les composants cl\u00e9s du code, expliquer comment le processus de diffusion est mod\u00e9lis\u00e9 avec la biblioth\u00e8que Diffusers. Tout le code (sous forme de nortebook) les poids du r\u00e9seau peuvent \u00eatre trouv\u00e9s ici.</p> <p>Note</p> <p>Ce billet n'explique pas le fonctionnement des mod\u00e8les de diffusion, mais comment utiliser la librairie diffusers pour se simplifier la vie.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-02-diffusers-unconditional-model/#pourquoi-la-bibliotheque-diffusers","title":"Pourquoi la biblioth\u00e8que diffusers ?","text":"<p>Cette biblioth\u00e8que est soutenue par Hugging Face et offre un cadre robuste et flexible pour travailler avec des mod\u00e8les de diffusion. Voici les principales raisons pour lesquelles vous devriez envisager d'utiliser Diffusers pour vos projets :</p> <ul> <li>Facilit\u00e9 d'utilisation avec une API de haut niveau pour concevoir des mod\u00e8les UNET.</li> <li>Documentation compl\u00e8te, m\u00eame si les tutoriels et les exemples ne couvrent pas tous les cas.</li> <li>Communaut\u00e9 active et support, avec plus de 23k \u00e9toiles sur leur d\u00e9p\u00f4t GitHub.</li> </ul>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-02-diffusers-unconditional-model/#definir-le-modele-unet-et-un-planificateur-de-bruit","title":"D\u00e9finir le mod\u00e8le UNET et un planificateur de bruit","text":"<p>Avant d'entra\u00eener un mod\u00e8le, nous devons le d\u00e9finir et d\u00e9finir l'ordonnanceur de bruit associ\u00e9 :</p> <pre><code>from diffusers import DDPMScheduler\nfrom diffusers.models import UNet2DModel\n\n# D\u00e9finir le mod\u00e8le UNet\n# Note : ce mod\u00e8le utilise moins de param\u00e8tres que le cours deeplearning.ai, car il n'est pas n\u00e9cessaire d'avoir un mod\u00e8le aussi grand pour cette t\u00e2che\nmodel = UNet2DModel(\n    sample_size=(16,16),                              # Taille de l'image d'entr\u00e9e\n    in_channels=3,                                    # Nombre de canaux d'entr\u00e9e (par exemple, 3 pour RVB)\n    out_channels=3,                                   # Nombre de canaux de sortie\n    layers_per_block=2,                               # Couches par bloc dans l'UNet\n    block_out_channels=(128, 64),                     # Canaux dans chaque bloc\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\"),  # Types de blocs descendants\n    up_block_types=(\"UpBlock2D\", \"UpBlock2D\")         # Types de blocks ascendants\n)\n\n# D\u00e9finir le planificateur DDPM\nnoise_scheduler = DDPMScheduler(num_train_timesteps=500)\n</code></pre> <p>Et c'est tout ! Le UNet2DModel peut nous faire gagner \u00e9norm\u00e9ment de temps dans la d\u00e9finition d'un tel mod\u00e8le.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-02-diffusers-unconditional-model/#entrainer-le-modele","title":"Entra\u00eener le mod\u00e8le","text":"<p>Ici, j'ai fait une fonction simple de formation comprenant les bases pour ne pas submerger les lecteurs \ud83d\ude09 :</p> <pre><code>def train(unet: UNet2DModel, noise_scheduler: DDPMScheduler, dataloader: DataLoader, num_epochs: int, lr: float) -&gt; None:\n    \"\"\"Entra\u00eene l'unet \u00e0 partir de son noise_scheduler et d'un dataloader.\n\n    Parameters\n    ----------\n    unet : UNet2DModel\n        Le mod\u00e8le unet \u00e0 entra\u00eener.\n    noise_scheduler : DDPMScheduler\n        Le planificateur de bruit \u00e0 utiliser lors de l'entra\u00eenement.\n    dataloader : DataLoader\n        Le chargeur de donn\u00e9es contenant les images \u00e0 reproduire.\n    num_epochs : int\n        Le nombre d'\u00e9poques pour entra\u00eener l'unet.\n    lr : float\n        Le taux d'apprentissage \u00e0 utiliser pour entra\u00eener l'unet.\n    \"\"\"\n    epochs = range(num_epochs)\n    losses = np.zeros(num_epochs)\n\n    optimizer = Adam(unet.parameters(), lr=lr)\n    unet.train()\n\n    for epoch in epochs:\n        epoch_loss = 0\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n\n            # En supposant que votre dataloader fournisse des images et des cibles (non utilis\u00e9 ici)\n            images, _ = batch\n            images = images.to(unet.device)\n\n            # G\u00e9n\u00e9rer un bruit al\u00e9atoire\n            noise = torch.randn(images.shape).to(unet.device)\n\n            # Forward du mod\u00e8le\n            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=unet.device).long()\n            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n            predicted_noise = unet(noisy_images, timesteps).sample\n\n            # Calcul de la perte (erreur quadratique moyenne entre le bruit r\u00e9el et le bruit pr\u00e9dit)\n            loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n\n            # Backward et optimisation\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        epoch_loss /= len(dataloader)\n        losses[epoch] = epoch_loss\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n</code></pre> <p>Cette fonction affiche la perte moyenne de chaque \u00e9poque. Pour entra\u00eener le mod\u00e8le, nous faisons le forward du mod\u00e8le sur des timeteps al\u00e9atoires avec des donn\u00e9es provenant d'un dataloader. Ensuite nous calculons la perte sur le bruit pr\u00e9dit pour optimiser les param\u00e8tres de notre mod\u00e8le. Dans le notebook, nous avons utilis\u00e9 le dataloader de deeplearning.ai, donc rien de nouveau sous le soleil \ud83d\ude1b. Comme vous pouvez le voir, la biblioth\u00e8que Diffusers n'est pas magique et n\u00e9cessite que nous impl\u00e9mentions certaines choses nous-m\u00eames.</p> <p>Pour sauvegarder notre mod\u00e8le pr\u00e9-entra\u00een\u00e9, une seule ligne est n\u00e9cessaire :</p> <pre><code>model.save_pretrained(pre_trained_model_path)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-02-diffusers-unconditional-model/#faire-des-inferences-avec-le-modele-entraine","title":"Faire des inf\u00e9rences avec le mod\u00e8le entra\u00een\u00e9","text":"<p>Maintenant que nous avons appris notre mod\u00e8le, nous pouvons le charger et l'utiliser sur un pipeline de la biblioth\u00e8que diffusers. Tout d'abord, chargeons les param\u00e8tres de notre mod\u00e8le :</p> <pre><code>model = UNet2DModel.from_pretrained(pre_trained_model)\n</code></pre> <p>C'est assez facile ! Maintenant, d\u00e9clarons notre pipeline et utilisons cuda si disponible :</p> <pre><code>from diffusers import DDPMPipeline\n\npipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\npipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <p>Et c'est tout, maintenant g\u00e9n\u00e9rer de nouvelles images est aussi simple que :</p> <pre><code>generated_image = pipeline(batch_size=16, num_inference_steps=500)\n</code></pre> <p>J'ai cr\u00e9\u00e9 un outil simple pour montrer les images g\u00e9n\u00e9r\u00e9es (vous pouvez jeter un oeil au d\u00e9p\u00f4t GitHub pour plus d'informations). Visualisons donc le r\u00e9sultat :</p> <pre><code>fig = plot_generated_images(generated_image.images, 4, 4)\nfig.show()\n</code></pre> <p></p> <p>Nous y voil\u00e0, nous avons g\u00e9n\u00e9r\u00e9 nos premiers exemples en utilisant la librairie diffusers. Dans un prochain article de blog, nous verrons comment g\u00e9n\u00e9rer des exemples \u00e0 partir d'une v\u00e9rit\u00e9 de terrain (au lieu de tutoriels n'utilisant que du texte et de suivre le cours deeplearning ai).</p> <p>J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>A bient\u00f4t, Vincent.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-16-diffusers-conditional-model/","title":"G\u00e9n\u00e9rer des images \u00e0 partir de labels avec un mod\u00e8le conditionn\u00e9","text":"<p>Suite \u00e0 mon pr\u00e9c\u00e9dent billet sur la g\u00e9n\u00e9ration non conditionnelle \u00e0 l'aide de la biblioth\u00e8que diffusers (lien ici), nous allons nous plonger dans la g\u00e9n\u00e9ration conditionnelle \u00e0 l'aide de labels. Alors que de nombreux tutoriels et exemples se concentrent sur les mod\u00e8les de diffusion guid\u00e9s par le texte, dans cet article de blog, nous allons explorer l'utilisation de labels one-hot pour conditionner notre mod\u00e8le de diffusion. Cette approche peut \u00eatre particuli\u00e8rement utile dans les sc\u00e9narios o\u00f9 nous voulons g\u00e9n\u00e9rer des images bas\u00e9es sur des cat\u00e9gories discr\u00e8tes plut\u00f4t que sur des descriptions textuelles.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-16-diffusers-conditional-model/#diffusion-conditionnelle","title":"Diffusion conditionnelle","text":"<p>Les mod\u00e8les de diffusion conditionnelle \u00e9tendent la diffusion non conditionnelle en introduisant des informations suppl\u00e9mentaires au cours du processus de g\u00e9n\u00e9ration. Ces informations suppl\u00e9mentaires, ou donn\u00e9es de conditionnement, peuvent guider le mod\u00e8le pour qu'il produise des \u00e9chantillons appartenant \u00e0 une cat\u00e9gorie sp\u00e9cifique ou poss\u00e9dant certaines caract\u00e9ristiques. Les entr\u00e9es de conditionnement les plus courantes sont les descriptions textuelles, les labels de classe ou d'autres attributs d'image.</p> <p>Dans ce tutoriel, nous utiliserons des labels one-hot comme entr\u00e9e de conditionnement. Le codage unitaire est une repr\u00e9sentation des variables cat\u00e9gorielles sous forme de vecteurs binaires. Cette m\u00e9thode est particuli\u00e8rement adapt\u00e9e \u00e0 des t\u00e2ches telles que la g\u00e9n\u00e9ration d'images, o\u00f9 chaque image correspond \u00e0 une cat\u00e9gorie discr\u00e8te. Si nous reprenons notre jeu de donn\u00e9es pr\u00e9c\u00e9dent, nous aurons 5 cat\u00e9gories : <code>[h\u00e9ros, non-h\u00e9ros, nourriture, sort, h\u00e9ros de profil]</code>.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-16-diffusers-conditional-model/#definir-le-modele-unet-le-planificateur-de-bruit-et-le-reseau-pour-integrer-nos-labels","title":"D\u00e9finir le mod\u00e8le unet, le planificateur de bruit et le r\u00e9seau pour int\u00e9grer nos labels","text":"<p>Par rapport \u00e0 la derni\u00e8re fois, nous utiliserons la classe <code>UNet2DConditionModel</code> pour notre mod\u00e8le, et nous passerons la v\u00e9rit\u00e9 terrain comme embeddings (espace latent).</p> <pre><code>from diffusers import DDPMScheduler\nfrom diffusers.models import UNet2DConditionModel\n\nclass_emb_size = 64\n\n# D\u00e9finir le mod\u00e8le UNet\nunet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,                             # Taille de l'embeddings\n    sample_size=(16, 16),                                       # Taille de l'image d'entr\u00e9e\n    in_channels=3,                                              # Nombre de canaux d'entr\u00e9e (par exemple, 3 pour RVB)\n    out_channels=3,                                             # Nombre de canaux de sortie\n    layers_per_block=2,                                         # Couches par bloc dans l'UNet\n    block_out_channels=(64, 128),                               # Canaux dans chaque bloc\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),   # Types de blocs descendants\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),         # Types de blocks ascendants\n)\n\n# D\u00e9finir le planificateur DDPM\nnoise_scheduler = DDPMScheduler(num_train_timesteps=200)\n</code></pre> <p>Pour introduire des conditions avec un tel mod\u00e8le, vous pouvez utiliser un bloc d'attention crois\u00e9e, d'o\u00f9 l'utilisation de <code>\"CrossAttnDownBlock2D\"</code> et <code>\"CrossAttnUpBlock2D\"</code> dans notre <code>unet</code>. Maintenant, nous avons besoin d'un r\u00e9seau pour int\u00e9grer nos labels dans l'espace latent d\u00e9sir\u00e9e, ici nous prendrons le m\u00eame r\u00e9seau que dans le cours de deeplearning.ai :</p> <pre><code>class UnsqueezeLayer(nn.Module):\n    \"\"\"Couche g\u00e9n\u00e9rique pour appliquer unsqueeze \u00e0 son entr\u00e9e.\"\"\"\n\n    def __init__(self, dim: int) -&gt; None:\n        super(UnsqueezeLayer, self).__init__()\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.unsqueeze(x, dim=self.dim)\n\n\n# We have to create this custom class to be able to use our sequential model inside our pipeline.\nclass CustomSequential(nn.Sequential):\n    \"\"\"H\u00e9rite de sequential pour ajouter les propri\u00e9t\u00e9s `device` et `dtype`.\n\n    Cela suppose que tous les param\u00e8tres partagent le m\u00eame `device` et utilisent le m\u00eame type.\n    \"\"\"\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def dtype(self):\n        return next(self.parameters()).dtype\n\nnum_classes = 5\n\nemb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size),\n    nn.GELU(),\n    nn.Linear(class_emb_size, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n</code></pre> <p>Ici, nous avons cr\u00e9\u00e9 une classe bas\u00e9 sur <code>nn.Sequential</code> pour avoir les propri\u00e9t\u00e9s <code>device</code> et <code>dtype</code> sur ce mod\u00e8le (pour pouvoir l'utiliser dans notre pipeline final). Comme <code>UNet2DConditionModel</code> est con\u00e7u pour avoir des text embeddings de forme <code>[batch, sequence_length, feature_dim]</code>, notre <code>emb_net</code> se termine avec un squeeze pour avoir l'embedding de forme <code>[batch_size, 1, class_emb_size]</code>. Le fait de ne pas avoir de s\u00e9quence dans nos labels, cela convient mieux \u00e0 notre cas.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-16-diffusers-conditional-model/#entrainer-le-modele","title":"Entra\u00eener le mod\u00e8le","text":"<p>Ici, j'ai modifi\u00e9 notre fonction train pour ajouter une pr\u00e9cision mixte (pour acc\u00e9l\u00e9rer l'entra\u00eenement) et notre conditionnement :</p> <pre><code>def train(\n    unet: UNet2DConditionModel,\n    emb_net: nn.Module,\n    noise_scheduler: DDPMScheduler,\n    dataloader: DataLoader,\n    num_epochs: int,\n    lr: float,\n):\n    epochs = range(num_epochs)\n\n    optimizer = Adam(chain(unet.parameters(), emb_net.parameters()), lr=lr)\n    scaler = GradScaler(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # For mixed precision\n    unet.train()\n\n    for epoch in epochs:\n        epoch_loss = 0\n\n        for batch in tqdm(dataloader):\n            optimizer.zero_grad()\n\n            # En supposant que votre dataloader fournisse les images et la cible associ\u00e9e\n            images, labels = batch\n            images = images.to(unet.device)\n            labels = labels.to(dtype=torch.float32, device=unet.device)\n\n            with autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):  # Mixed precision\n                # G\u00e9n\u00e9rer un bruit al\u00e9atoire\n                noise = torch.randn(images.shape, device=unet.device)\n\n                # Generate random timesteps and apply the noise scheduler\n                timesteps = torch.randint(\n                    0,\n                    noise_scheduler.config.num_train_timesteps,\n                    (images.shape[0],),\n                    device=unet.device,\n                )\n\n                noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n\n                # Compute the class embeddings\n                enc_labels = emb_net(labels)\n\n                # Forward pass through the model with labels embeddings\n                predicted_noise = unet(\n                    noisy_images, timesteps, enc_labels, class_labels=labels\n                ).sample\n\n                # Compute loss (mean squared error between actual and predicted noise)\n                loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n\n            # Backward pass and optimization\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        epoch_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n</code></pre> <p>Il affiche la perte moyenne de chaque \u00e9poque. Pour entra\u00eener le mod\u00e8le, nous faisons un forward sur des pas de temps al\u00e9atoires avec nos donn\u00e9es et les embeddings (espace latent repr\u00e9sentant les labels). Ensuite, nous calculons la perte sur le bruit pr\u00e9dit pour optimiser les param\u00e8tres de nos <code>unet</code> et <code>emb_net</code>. Dans le notebook, nous avons gard\u00e9 le dataloader de deeplearning.ai, donc rien de nouveau sous le soleil. Comme vous pouvez le voir, notre boucle d'apprentissage n'est pas tr\u00e8s diff\u00e9rente de la boucle d'apprentissage de notre unet inconditionnel.</p> <p>Note</p> <p>Je n'ai pas masqu\u00e9 al\u00e9atoirement les labels (comme dans le tutoriel de deeplearning.ai) car cela n'a pas am\u00e9lior\u00e9 les r\u00e9sultats. Mais nous verrons dans le prochain article pourquoi cela peut \u00eatre important (dans les bonnes circonstances).</p> <p>Maintenant, nous pouvons sauvegarder nos <code>unet</code> et <code>emb_net</code> pr\u00e9-entra\u00een\u00e9s.</p> <pre><code>unet.save_pretrained(pre_trained_unet_path)\ntorch.save(emb_net.state_dict(), pre_trained_emb_net_path)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/06-16-diffusers-conditional-model/#faire-des-inferences-avec-lunet-entraine","title":"Faire des inf\u00e9rences avec l'unet entra\u00een\u00e9","text":"<p>Maintenant que nous avons appris notre mod\u00e8le, nous pouvons le charger et l'utiliser sur un pipeline de la biblioth\u00e8que diffusers. Tout d'abord, chargeons nos mod\u00e8les :</p> <pre><code>unet = UNet2DConditionModel.from_pretrained(pre_trained_unet)\nemb_net.load_state_dict(torch.load(pre_trained_emb_net))\n</code></pre> <p>Plut\u00f4t facile ! Maintenant la partie la plus difficile, nous devons cr\u00e9er un pipeline personnalis\u00e9 pour utiliser nos mod\u00e8les. Ici, nous ne partirons pas de z\u00e9ro en \u00e9tendant le <code>DDPMPipeline</code> :</p> <pre><code>from diffusers import DDPMPipeline\n\n\nclass ConditionalDDPMPipeline(DDPMPipeline):\n    def __init__(\n        self, unet: UNet2DConditionModel, class_net: CustomSequential, scheduler: DDPMScheduler\n    ) -&gt; None:\n        super().__init__(unet=unet, scheduler=scheduler)\n        self.class_net = class_net\n        # to let the pipeline change the model device and/or type\n        self.register_modules(class_net=class_net)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        class_label: list[list[float]],\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        num_inference_steps: int = 1000,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -&gt; Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        La fonction d'appel au pipeline pour la g\u00e9n\u00e9ration.\n\n        Args :\n            class_label (list[list[float]]) :\n                liste de labels onne-hot. len(class_label) repr\u00e9sente le nombre d'exemples \u00e0 g\u00e9n\u00e9rer.\n            generator (`torch.Generator`, *optionnel*) :\n                Un [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) pour rendre la g\u00e9n\u00e9ration d\u00e9terministe.\n            num_inference_steps (`int`, *optionnel*, par d\u00e9faut 1000) :\n                Le nombre d'\u00e9tapes de d\u00e9bruitage. Plus d'\u00e9tapes de d\u00e9bruitage conduisent g\u00e9n\u00e9ralement \u00e0 une image de meilleure qualit\u00e9 au d\u00e9triment d'une inf\u00e9rence plus lente.\n            output_type (`str`, *optionnel*, par d\u00e9faut `\"pil\"`) :\n                Le format de sortie de l'image g\u00e9n\u00e9r\u00e9e. Choisissez entre `PIL.Image` et `np.array`.\n            return_dict (`bool`, *optionnel*, par d\u00e9faut `True`) :\n                Retourne ou non un [`~pipelines.ImagePipelineOutput`] au lieu d'un simple tuple.\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] ou `tuple` :\n                Si `return_dict` est `True`, [`~pipelines.ImagePipelineOutput`] est retourn\u00e9, sinon un `tuple` est retourn\u00e9, o\u00f9 le premier \u00e9l\u00e9ment est une liste avec les images g\u00e9n\u00e9r\u00e9es.\n        \"\"\"\n        batch_size = len(class_label)\n        # \u00c9chantillon de bruit gaussien pour commencer la boucle\n        if isinstance(self.unet.config.sample_size, int):\n            image_shape = (\n                batch_size,\n                self.unet.config.in_channels,\n                self.unet.config.sample_size,\n                self.unet.config.sample_size,\n            )\n        else:\n            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n\n        if self.device.type == \"mps\":\n            # randn ne fonctionne pas de mani\u00e8re reproductible sur mps\n            image = randn_tensor(image_shape, generator=generator)\n            image = image.to(self.device)\n        else:\n            image = randn_tensor(image_shape, generator=generator, device=self.device)\n\n        labels = torch.tensor(class_label, device=self.device)\n        enc_labels = self.class_net(labels)\n\n        # d\u00e9finir les valeurs des \u00e9tapes\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        for t in self.progress_bar(self.scheduler.timesteps):\n            # 1. pr\u00e9dire le bruit model_output\n            model_output = self.unet(image, t, enc_labels, class_labels=labels, return_dict=False)[\n                0\n            ]\n\n            # 2. calculer l'image pr\u00e9c\u00e9dente : x_t -&gt; x_t-1\n            image = self.scheduler.step(model_output, t, image, generator=generator).prev_sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image,)\n\n        return ImagePipelineOutput(images=image)\n</code></pre> <p>En r\u00e9sum\u00e9, c'est le m\u00eame code que le <code>DDPMPipeline</code> de la librairie diffusers sauf que nous avons ajout\u00e9 notre <code>emb_net</code> pour faire des g\u00e9n\u00e9rations conditionn\u00e9es avec des labels. Maintenant, d\u00e9clarons notre pipeline avec cuda si possible, g\u00e9n\u00e9rons quelques \u00e9chantillons et visualisons-les :</p> <pre><code>pipeline = ConditionalDDPMPipeline(unet=unet, class_net=class_net, scheduler=noise_scheduler)\npipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ngenerated_image = pipeline(\n    [\n        # h\u00e9ros, non-h\u00e9ros, nourriture, sort, h\u00e9ros de profil\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n    ],\n    num_inference_steps=200,\n)\n\nfig = plot_generated_images(generated_image.images, 5, 4)\nfig.show()\n</code></pre> <p></p> <p>Et voil\u00e0, nous avons g\u00e9n\u00e9r\u00e9 nos premiers exemples conditionn\u00e9s en utilisant la librairie de diffusers avec des labels one-hot. Le notebook complet et les poids d'entrainement sont disponibles ici.</p> <p>J'esp\u00e8re que cela aidera et/ou inspirera certains d'entre vous.</p> <p>Au plaisir de vous revoir, Vincent.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/","title":"Am\u00e9lioration de la vitesse et de la qualit\u00e9 de g\u00e9n\u00e9ration d'image","text":"<p>Bienvenue dans la troisi\u00e8me partie de notre s\u00e9rie sur la biblioth\u00e8que Diffusers. Dans cet article, nous explorerons des strat\u00e9gies pour am\u00e9liorer \u00e0 la fois la vitesse d'inf\u00e9rence et la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es. Ainsi, nous am\u00e9liorerons les techniques discut\u00e9es dans notre article pr\u00e9c\u00e9dent, mais aussi en utiliserons des nouvelles.</p> <p>Remarque sur les bonnes pratiques</p> <p>Avant d'entrer dans les d\u00e9tails techniques, il est important de noter que cet article ne suit pas toutes les bonnes pratiques g\u00e9n\u00e9ralement recommand\u00e9es pour les projets de machine learning. En particulier, nous n'utilisons pas de versionnage de jeux de donn\u00e9es, d'outils de suivi comme MLflow, ni de cadres d'optimisation d'hyperparam\u00e8tres tels que Optuna. Ces pratiques sont essentielles pour un workflow ML robuste et seront abord\u00e9es dans de futurs articles, mais pas imm\u00e9diatement. L'objectif de ce tutoriel est de pr\u00e9senter un flux de travail simplifi\u00e9, plus facile \u00e0 suivre et \u00e0 comprendre.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#analyse-des-donnees-letape-cruciale","title":"Analyse des donn\u00e9es : l'\u00e9tape cruciale","text":"<p>L'analyse des donn\u00e9es est une \u00e9tape cruciale pour am\u00e9liorer la qualit\u00e9 de la g\u00e9n\u00e9ration d'image.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#comprendre-la-distribution-des-donnees","title":"Comprendre la distribution des donn\u00e9es","text":"<p>Commen\u00e7ons par examiner la distribution de notre jeu de donn\u00e9es. Cela nous aidera \u00e0 identifier tout d\u00e9s\u00e9quilibre ou anomalie dans les donn\u00e9es.</p> <p></p> <p>Actuellement, notre jeu de donn\u00e9es n'est pas \u00e9quilibr\u00e9. Le r\u00e9\u00e9quilibrage des donn\u00e9es am\u00e9liorera la performance du mod\u00e8le en renfor\u00e7ant sa capacit\u00e9 \u00e0 g\u00e9n\u00e9raliser et \u00e0 produire des r\u00e9sultats plus pr\u00e9cis. Nous pouvons le faire avec l'ajout suivant dans notre notebook d'entra\u00eenement :</p> <pre><code>dataset = SpritesDataset(\n    \"./dlai_lib/sprites_1788_16x16.npy\",\n    \"./dlai_lib/sprite_labels_nc_1788_16x16.npy\",\n    null_context=False\n)\n\n# Cr\u00e9ation d'un sampler pour compenser le d\u00e9s\u00e9quilibre du jeu de donn\u00e9es\nlabels = dataset.slabels.argmax(axis=1)\nu_labels, class_counts = np.unique(labels, return_counts=True)\nclass_weights = 1 - class_counts / class_counts.sum()\nsample_weights = tuple(class_weights[label] for label in labels)\nnum_samples = ceil(len(labels)/batch_size) * batch_size  # num_samples est un multiple de batch_size pour \u00e9viter de recompilation\n\ndataset_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n\ndataloader = DataLoader(\n    dataset, sampler=dataset_sampler, batch_size=batch_size, num_workers=1, pin_memory=True\n)\n</code></pre> <p>Dans cette approche, nous calculons les poids pour chaque \u00e9chantillon dans notre jeu de donn\u00e9es et utilisons un \u00e9chantillonneur de donn\u00e9es al\u00e9atoire pour s\u00e9lectionner les \u00e9chantillons en fonction de ces poids. Ainsi, les \u00e9chantillons des classes les moins repr\u00e9sent\u00e9es sont s\u00e9lectionn\u00e9s plus fr\u00e9quemment que ceux des classes les plus fr\u00e9quentes.</p> <p>Malgr\u00e9 le fait que la classe non-humaine soit l'une des plus fr\u00e9quentes dans notre jeu de donn\u00e9es, le mod\u00e8le peine \u00e0 g\u00e9n\u00e9rer autre chose que des formes indistinctes, ce qui indique qu'il pourrait y avoir des probl\u00e8mes sous-jacents avec les donn\u00e9es elles-m\u00eames.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#analyse-de-la-classe-non-humaine","title":"Analyse de la classe non-humaine","text":"<p>Un d\u00e9fi majeur pour notre mod\u00e8le est la classe non-humaine, qui inclut fr\u00e9quemment des images mal d\u00e9finis. Pour illustrer ce probl\u00e8me, examinons d'abord les images non-humaines d'origine :</p> <p></p> <p>Comme on peut le voir, certains exemples sont de mauvaise qualit\u00e9 et ressemblent aux sorties non-humaines produites par notre mod\u00e8le. Pour r\u00e9soudre ce probl\u00e8me, nous allons \u00e9tudier et d\u00e9velopper une fonction pour d\u00e9tecter automatiquement de tels \u00e9chantillons probl\u00e9matiques dans la base de donn\u00e9es, car il n'est pas possible de revoir manuellement plus de 80 000 \u00e9chantillons.</p> <pre><code>def detect_wrong_images(image: np.ndarray, number_of_white: int = 26) -&gt; bool:\n    \"\"\"Fonction pour d\u00e9tecter si une image est incorrecte et doit \u00eatre exclue de l'entra\u00eenement.\n\n    Param\u00e8tres\n    ----------\n    image : np.ndarray\n        L'image en RGB avec une forme [largeur, hauteur, canal]\n    number_of_white : int, optionnel\n        Le nombre de pixels blancs qu'une image doit avoir pour \u00eatre consid\u00e9r\u00e9e comme valide, par d\u00e9faut 26\n\n    Retourne\n    -------\n    bool\n        True si l'image est incorrecte pour la g\u00e9n\u00e9ration et False si elle est correcte.\n    \"\"\"\n\n    wrong_image = False\n    # Notez que le blanc est (255, 255, 255), donc la somme est 765\n    # Fond blanc donc un minimum de pixels doit \u00eatre blanc\n    wrong_image |= np.sum(image.sum(axis=-1) == 765) &lt; number_of_white\n\n    # Les 4 coins doivent \u00eatre blancs\n    wrong_image |= image[0, 0].sum() != 765\n    wrong_image |= image[-1, 0].sum() != 765\n    wrong_image |= image[0, -1].sum() != 765\n    wrong_image |= image[-1, -1].sum() != 765\n\n    return wrong_image\n</code></pre> <p>L'objectif de cette fonction est simple : les bonnes images doivent avoir tous les coins blancs et une part significative des pixels doit \u00eatre blancs, puisque nous nous attendons \u00e0 des arri\u00e8re-plans blancs. Bien que cette approche suffise pour un jeu de donn\u00e9es relativement simple, des jeux de donn\u00e9es r\u00e9els n\u00e9cessiteront des v\u00e9rifications suppl\u00e9mentaires, telles que la nettet\u00e9 et des contraintes sur la palette de couleurs.</p> <p>Si nous tra\u00e7ons quelques mauvais \u00e9chantillons, nous obtenons ceci :</p> <p></p> <p>En utilisant cette approche, nous avons identifi\u00e9 et supprim\u00e9 30 000 \u00e9chantillons probl\u00e9matiques de notre jeu de donn\u00e9es, capturant efficacement les principaux probl\u00e8mes de la classe non-humaine.</p> <p>Et si nous montrons les 30 exemples non-humains de notre nouveau jeu de donn\u00e9es, nous obtenons ceci :</p> <p></p> <p>L'application de cette fonction pour affiner notre jeu de donn\u00e9es peut acc\u00e9l\u00e9rer l'entra\u00eenement du mod\u00e8le et am\u00e9liorer significativement les r\u00e9sultats. Toutefois, vous constaterez peut-\u00eatre que la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es est encore insuffisante et que le processus de g\u00e9n\u00e9ration est trop lent. J'ai cr\u00e9\u00e9 un jeu de donn\u00e9es personnalis\u00e9 pour permettre ce filtrage avec le param\u00e8tre <code>clean_version=True</code>.</p> <p>Remarque : Pour une explication d\u00e9taill\u00e9e de ce processus, veuillez consulter le notebook d\u00e9di\u00e9 ici : https://github.com/vroger11/diffusers-tutorials/tree/main/tutorials/03-dataset_analysis.ipynb</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ameliorations-dans-la-conception-du-reseau","title":"Am\u00e9liorations dans la conception du r\u00e9seau","text":"<p>On pense souvent qu'ajouter des couches et des param\u00e8tres suppl\u00e9mentaires am\u00e9liorera le mod\u00e8le. Bien que cela puisse \u00eatre vrai dans certains cas, ce n'est pas la solution compl\u00e8te et cela conduit souvent \u00e0 des mod\u00e8les inefficaces. Dans les sous-sections suivantes, nous explorerons des techniques pour am\u00e9liorer les r\u00e9sultats et la vitesse d'inf\u00e9rence.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#echauffement-du-reseau-conditionnel","title":"\u00c9chauffement du r\u00e9seau conditionnel","text":"<p>Pour renforcer la concordance du r\u00e9sultat conditionn\u00e9, nous utilisons une strat\u00e9gie d'\u00e9chauffement pour le r\u00e9seau conditionnel. Mais, cela entraine un sur apprentissage de l'UNet sur la v\u00e9rit\u00e9 terrain. Pour solutionner ce probl\u00e8me, apr\u00e8s avoir gel\u00e9 les param\u00e8tres du r\u00e9seau conditionnel, nous appliquons un masquage al\u00e9atoire des \u00e9tiquettes (contexte) en utilisant une distribution de Bernoulli. Cette approche stabilise le processus d'entra\u00eenement et aide \u00e0 \u00e9viter que le UNet ne surapprenne au r\u00e9seau conditionnel.</p> <p>J'ai choisi une valeur d'\u00e9chauffement arbitraire de 80 \u00e0 des fins de d\u00e9monstration dans ce tutoriel. Cela peut ne pas \u00eatre la valeur optimale pour tous les sc\u00e9narios. Dans des applications r\u00e9elles, il est conseill\u00e9 de surveiller les performances du mod\u00e8le et d'ajuster l'\u00e9tape d'\u00e9chauffement en cons\u00e9quence pour d\u00e9terminer le meilleur moment pour geler le r\u00e9seau conditionnel.</p> <p>Ces ajustements sont mis en \u0153uvre en incorporant les deux conditions suivantes dans notre boucle d'entra\u00eenement :</p> <pre><code>if epoch &gt;= warmup:\n    warmup_done = True\n    # geler toutes les couches du r\u00e9seau d'embeddings\n    for param in emb_net.parameters():\n        param.requires_grad = False\n</code></pre> <pre><code>if warmup_done:\n    # masquage al\u00e9atoire des \u00e9tiquettes (contexte)\n    context_mask = torch.bernoulli(torch.zeros(labels.shape[0]) + 0.95).to(unet.device)\n    labels = labels * context_mask.unsqueeze(-1)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ameliorations-de-larchitecture-du-reseau","title":"Am\u00e9liorations de l'architecture du r\u00e9seau","text":"","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ameliorations-de-lunet","title":"Am\u00e9liorations de l'UNet","text":"<p>L'ajout \u00e0 la fois d'un bloc descendant et d'un bloc ascendant \u00e0 l'architecture UNet s'est av\u00e9r\u00e9 efficace. Mes exp\u00e9riences ont montr\u00e9 que l'absence de ces blocs entra\u00eenait une baisse de la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es.</p> <p>Par cons\u00e9quent, nous modifions notre UNet comme suit :</p> <pre><code>unet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,\n    sample_size=16,\n    in_channels=3,\n    out_channels=3,\n    layers_per_block=2,\n    block_out_channels=(64, 128, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\"),  # Un bloc descendant suppl\u00e9mentaire\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),  # Un bloc ascendant suppl\u00e9mentaire\n)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#goulot-detranglement-du-reseau-conditionnel","title":"Goulot d'\u00e9tranglement du r\u00e9seau conditionnel","text":"<p>La mise en \u0153uvre d'un goulot d'\u00e9tranglement dans le r\u00e9seau conditionnel impose une meilleure qualit\u00e9 des embeddings, ce qui am\u00e9liore \u00e0 son tour la performance globale.</p> <p>Ainsi, notre r\u00e9seau d'embeddings devient :</p> <pre><code>emb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size//2), # goulot d'\u00e9tranglement pour imposer une meilleure qualit\u00e9 des embeddings\n    nn.GELU(),\n    nn.Linear(class_emb_size//2, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#techniques-de-regularisation","title":"Techniques de r\u00e9gularisation","text":"<p>Les techniques de r\u00e9gularisation sont essentielles pour am\u00e9liorer la g\u00e9n\u00e9ralisation du mod\u00e8le final et \u00e9viter le surapprentissage :</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#dropout","title":"Dropout","text":"<p>Nous appliquons le dropout \u00e0 la fois au UNet et au r\u00e9seau d'embeddings pour r\u00e9duire le surapprentissage et am\u00e9liorer la robustesse du mod\u00e8le.</p> <p>Cela entra\u00eene les modifications suivantes :</p> <pre><code>emb_net = CustomSequential(\n    nn.Linear(num_classes, class_emb_size//2),\n    nn.Dropout(0.1),  # ajout du dropout pour r\u00e9gulariser le mod\u00e8le\n    nn.GELU(),\n    nn.Linear(class_emb_size//2, class_emb_size),\n    UnsqueezeLayer(dim=1),\n)\n\n# D\u00e9finir le mod\u00e8le UNet\nunet = UNet2DConditionModel(\n    encoder_hid_dim=class_emb_size,\n    sample_size=16,\n    in_channels=3,\n    out_channels=3,\n    layers_per_block=2,\n    block_out_channels=(64, 128, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n    dropout=0.2,  # ajout du dropout pour r\u00e9gulariser le mod\u00e8le\n)\n</code></pre> <p>Notez que nous \u00e9vitons d'appliquer le Dropout \u00e0 la sortie de notre r\u00e9seau d'embeddings pour \u00e9viter des probl\u00e8mes, tels que l'optimisation des valeurs d'embedding nulles dues au Dropout, qui pourraient affecter le m\u00e9canisme d'attention dans notre UNet.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#optimiseur-adamw","title":"Optimiseur AdamW","text":"<p>Nous utilisons l'optimiseur AdamW, qui int\u00e8gre la d\u00e9gradation des poids. Cette technique favorise une meilleure r\u00e9gularisation en p\u00e9nalisant les poids importants.</p> <p>Pour configurer cela, nous devons modifier une importation :</p> <pre><code>from torch.optim import AdamW\n</code></pre> <p>De plus, il est n\u00e9cessaire de changez la cr\u00e9ation de l'optimiseur en utilisant un <code>weight_decay</code>. Ici, j'ai choisi une valeur arbitraire qui fonctionne bien dans la plupart des cas mais qui m\u00e9rite d'\u00eatre s\u00e9lectionn\u00e9e plus soigneusement si vous avez les capacit\u00e9s de calcul :</p> <pre><code>optimizer = AdamW(chain(unet.parameters(), emb_net.parameters()), lr=lr, weight_decay=1e-4)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ajustements-du-scheduler-et-de-la-fonction-de-perte","title":"Ajustements du scheduler et de la fonction de perte","text":"","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ddim-vs-ddpm","title":"DDIM vs. DDPM","text":"<p>Le DDIM (Denoising Diffusion Implicit Models) pr\u00e9sente plusieurs avantages par rapport au DDPM (Denoising Diffusion Probabilistic Models). Contrairement au DDPM, qui repose sur une cha\u00eene de Markov pour d\u00e9bruiter s\u00e9quentiellement les \u00e9chantillons, le DDIM contourne le processus de Markov explicite en utilisant une approche non-Markovienne qui permet moins d'\u00e9tapes de diffusion. Cela se traduit par un \u00e9chantillonnage plus rapide et plus efficace, tout en maintenant ou en am\u00e9liorant la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es.</p> <p>Le passage du DDPM au DDIM est simple ; nous devons simplement commencer par remplacer la cr\u00e9ation du scheduler de bruit :</p> <pre><code># Utilisation du DDIMScheduler au lieu du DDPMScheduler\nnoise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n</code></pre> <p>Comme l'API du <code>DDIMScheduler</code> est similaire \u00e0 celle du <code>DDPMScheduler</code>, il n'est pas n\u00e9cessaire de modifier le code autour du <code>noise_scheduler</code>. Cependant, nous devons r\u00e9\u00e9crire notre pipeline, bien qu'il ressemble beaucoup \u00e0 notre <code>DDPMPipeline</code> personnalis\u00e9.</p> <pre><code>class ConditionalDDIMPipeline(DDIMPipeline):\n    def __init__(\n        self, unet: UNet2DConditionModel, class_net: CustomSequential, scheduler: DDIMScheduler\n    ) -&gt; None:\n        super().__init__(unet=unet, scheduler=scheduler)\n        self.class_net = class_net\n        self.class_net.eval()\n        self.register_modules(class_net=class_net)\n        self.unet.eval()\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        class_label: list[list[float]],\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        eta: float = 0.0,\n        num_inference_steps: int = 1000,\n        use_clipped_model_output: Optional[bool] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ) -&gt; Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Fonction d'appel du pipeline pour la g\u00e9n\u00e9ration.\n\n        Arguments:\n            class_label (list[list[float]]):\n                Liste d'exemples one-hot. len(class_label) repr\u00e9sente le nombre d'exemples \u00e0 g\u00e9n\u00e9rer.\n            generator (`torch.Generator`, *optionnel*):\n                Un [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) pour rendre\n                la g\u00e9n\u00e9ration d\u00e9terministe.\n            eta (`float`, *optionnel*, par d\u00e9faut 0.0):\n                Correspond au param\u00e8tre eta (\u03b7) du papier [DDIM](https://arxiv.org/abs/2010.02502). Ne s'applique\n                qu'au [`~schedulers.DDIMScheduler`], et est ignor\u00e9 dans les autres schedulers. Une valeur de `0`\n                correspond \u00e0 DDIM et `1` correspond \u00e0 DDPM.\n            num_inference_steps (`int`, *optionnel*, par d\u00e9faut 1000):\n                Le nombre d'\u00e9tapes de d\u00e9bruitage. Plus d'\u00e9tapes de d\u00e9bruitage conduisent g\u00e9n\u00e9ralement \u00e0 une image de\n                meilleure qualit\u00e9, au d\u00e9triment d'une inf\u00e9rence plus lente.\n            use_clipped_model_output (`bool`, *optionnel*, par d\u00e9faut `None`):\n                Si `True` ou `False`, voir la documentation de [`DDIMScheduler.step`]. Si `None`, rien n'est pass\u00e9\n                en aval au scheduler (utiliser `None` pour les schedulers qui ne prennent pas en charge cet argument).\n            output_type (`str`, *optionnel*, par d\u00e9faut `\"pil\"`):\n                Le format de sortie de l'image g\u00e9n\u00e9r\u00e9e. Choisissez entre `PIL.Image` ou `np.array`.\n            return_dict (`bool`, *optionnel*, par d\u00e9faut `True`):\n                Si `True`, retourne un [`~pipelines.ImagePipelineOutput`] ; sinon, retourne un tuple simple.\n\n        Retourne:\n            [`~pipelines.ImagePipelineOutput`] ou `tuple`:\n                Si `return_dict` est `True`, un [`~pipelines.ImagePipelineOutput`] est retourn\u00e9, sinon un tuple\n                est retourn\u00e9 dont le premier \u00e9l\u00e9ment est une liste des images g\u00e9n\u00e9r\u00e9es.\n        \"\"\"\n        batch_size = len(class_label)\n        # \u00c9chantillonnage du bruit gaussien pour d\u00e9buter la boucle\n        if isinstance(self.unet.config.sample_size, int):\n            image_shape = (\n                batch_size,\n                self.unet.config.in_channels,\n                self.unet.config.sample_size,\n                self.unet.config.sample_size,\n            )\n        else:\n            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n\n        image = randn_tensor(image_shape, generator=generator, device=self._execution_device, dtype=self.unet.dtype)\n\n        # obtention du ground truth encod\u00e9\n        labels = torch.tensor(class_label, device=self.device)\n        with autocast(str(self.unet.device)):\n            enc_labels = self.class_net(labels)\n\n            # d\u00e9finition des valeurs de step\n            self.scheduler.set_timesteps(num_inference_steps)\n\n            for t in self.progress_bar(self.scheduler.timesteps):\n                # 1. pr\u00e9dire le bruit model_output\n                model_output = self.unet(image, t, enc_labels, class_labels=labels, return_dict=False)[\n                    0\n                ]\n\n                # 2. pr\u00e9dire la moyenne pr\u00e9c\u00e9dente de l'image x_t-1 et ajouter la variance en fonction de eta\n                # eta correspond \u00e0 \u03b7 dans l'article et doit \u00eatre compris entre [0, 1]\n                # passer de x_t \u00e0 x_t-1\n                image = self.scheduler.step(\n                    model_output, t, image, eta=eta, use_clipped_model_output=use_clipped_model_output, generator=generator\n                ).prev_sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image,)\n\n        return ImagePipelineOutput(images=image)\n</code></pre> <p>Nous avons modifi\u00e9 l'h\u00e9ritage et ajout\u00e9 le param\u00e8tre <code>eta</code> pour assurer la compatibilit\u00e9 avec le <code>DDPMScheduler</code>. Avec ce scheduler, j'ai pu g\u00e9n\u00e9rer des \u00e9chantillons de haute qualit\u00e9 en seulement 10 \u00e9tapes, au lieu de 100, am\u00e9liorant ainsi la vitesse d'inf\u00e9rence de notre mod\u00e8le final de plus de 10 fois, car nous utilisons \u00e9galement une approche non-Markovienne.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#fonction-de-perte","title":"Fonction de perte","text":"<p>Nous passons de la perte L2 \u00e0 la perte L1. La perte L1, ou erreur absolue moyenne (MAE), est moins sensible aux valeurs aberrantes par rapport \u00e0 la perte L2 (erreur quadratique moyenne), ce qui la rend plus robuste et stable pendant l'entra\u00eenement. Cette robustesse aide \u00e0 g\u00e9n\u00e9rer des images plus claires et plus coh\u00e9rentes visuellement en r\u00e9duisant l'influence des valeurs extr\u00eames et en assurant des mises \u00e0 jour de gradients stables.</p> <p>Dans notre code pr\u00e9c\u00e9dent, cela n\u00e9cessite de modifier le calcul de la fonction de perte :</p> <pre><code># Calculer la perte l1 au lieu de mse, elle offre une meilleure robustesse aux bruits\n# ce qui est adapt\u00e9 aux mod\u00e8les de diffusion, mais l'inconv\u00e9nient est un entra\u00eenement plus lent\nloss = torch.nn.functional.l1_loss(predicted_noise, noise)\n</code></pre>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#ameliorations-de-lentrainement","title":"Am\u00e9liorations de l'entra\u00eenement","text":"","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#augmentation-des-epoques","title":"Augmentation des \u00e9poques","text":"<p>Avec les modifications mentionn\u00e9es, nous avons augment\u00e9 le nombre d'\u00e9poques d'entra\u00eenement \u00e0 100. Cela garantit que le mod\u00e8le soit bien entra\u00een\u00e9 et de mieux tirer parti de nos ajustements pr\u00e9c\u00e9dents.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#torch-compile","title":"Torch compile","text":"<p>Pour acc\u00e9l\u00e9rer l'entra\u00eenement, nous utilisions d\u00e9j\u00e0 la pr\u00e9cision mixet. N\u00e9anmoin, nous pouvons faire mieux en utilisant <code>torch.compile</code>. Ceci nous permet de r\u00e9duire de 30 % le temps d'entra\u00eenement. Notez que <code>torch.compile</code> n'a pas \u00e9t\u00e9 utilis\u00e9 pour le pipeline final, car nous avons n\u00e9cessit\u00e9 un seul forward pour l'\u00e9valuation. Cependant, <code>torch.compile</code> offre des avantages significatifs en mati\u00e8re d'efficacit\u00e9 pour la mise en production des mod\u00e8les.</p> <p>Pour compiler nos r\u00e9seaux, nous devons ajouter les lignes suivantes :</p> <pre><code># compiler les r\u00e9seaux pour des inf\u00e9rences plus rapides m\u00eame pour l'entra\u00eenement\nunet = torch.compile(unet, fullgraph=True)\nemb_net = torch.compile(emb_net, fullgraph=True)\n</code></pre> <p>Pour ceux qui sont int\u00e9ress\u00e9s par des r\u00e9sultats encore plus rapides gr\u00e2ce \u00e0 des techniques telles que la quantification et la distillation, je recommande de consulter le tutoriel de Hugging Face sur la diffusion rapide.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-08-diffusers-obtain-better-results/#resultat-final","title":"R\u00e9sultat final","text":"<p>Avant de pr\u00e9senter les r\u00e9sultats finaux, je tiens \u00e0 fixer des attentes r\u00e9alistes. Avec le jeu de donn\u00e9es actuel (exempt d'exemples de mauvaise qualit\u00e9), nous pouvons nous attendre \u00e0 de bons r\u00e9sultats. Cependant, gardez \u00e0 l'esprit que nous travaillons avec seulement cinq cat\u00e9gories dans notre ground truth, et les exemples au sein de chaque cat\u00e9gorie sont assez h\u00e9t\u00e9rog\u00e8nes. En cons\u00e9quence, bien que le mod\u00e8le apprendra \u00e0 distinguer les cat\u00e9gories, il ne capturera pas les \"sous-cat\u00e9gories\", limitant notre contr\u00f4le sur celles-ci. Ce manque d'homog\u00e9n\u00e9it\u00e9 au sein des cat\u00e9gories permet au ground truth d'\u00eatre plus facilement m\u00e9lang\u00e9.</p> <p>Regardons maintenant quelques \u00e9chantillons de chaque classe :</p> <p></p> <p>Comme vous pouvez le voir, les images semblent assez soign\u00e9es, mais de nombreux r\u00e9sultats g\u00e9n\u00e9r\u00e9s sont presque identiques \u00e0 certains exemples d'entra\u00eenement. Vous remarquerez peut-\u00eatre des diff\u00e9rences subtiles entre les originaux et les \u00e9chantillons g\u00e9n\u00e9r\u00e9s (notamment avec les petits d\u00e9tails sur les flammes). Pour mieux contr\u00f4ler les sous-cat\u00e9gories, nous pourrions am\u00e9liorer la v\u00e9rit\u00e9 terrain en subdivisant les cat\u00e9gories existantes. Ce travail est laborieux s'il est effectu\u00e9 manuellement. Une approche plus efficace consiste \u00e0 l'automatiser en appliquant des algorithmes comme k-means dans l'espace encod\u00e9 de notre UNet sur les \u00e9chantillons d'entra\u00eenement et \u00e0 attribuer des noms aux cat\u00e9gories r\u00e9sultantes. Cela rendrait le m\u00e9lange de cat\u00e9gories plus significatif et permettrait plus de flexibilit\u00e9 dans l'ajustement des embeddings.</p> <p>Cependant, comme je dois fixer des limites pour ce tutoriel afin d'\u00e9viter qu'il ne devienne trop vaste, je consid\u00e8re cette partie hors du cadre.</p> <p>J'esp\u00e8re que cet article aidera et/ou inspirera certains d'entre vous. Si vous avez des suggestions, n'h\u00e9sitez pas \u00e0 commenter ou \u00e0 me contacter.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["Apprentissage Automatique","D\u00e9veloppement","G\u00e9n\u00e9ration","Python"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/","title":"Migration de ce site web vers MkDocs","text":"<p>Apr\u00e8s avoir maintenu ce site avec Jekyll pendant quelques ann\u00e9es, j'ai d\u00e9cid\u00e9 qu'il \u00e9tait temps de changer. Autant j'appr\u00e9ciais la flexibilit\u00e9 offerte par Jekyll, les besoins de mes projets avaient \u00e9volu\u00e9. J'utilisais d\u00e9j\u00e0 MkDocs pour documenter des projets Python, alors j'ai d\u00e9cid\u00e9 de tout consolider sous MkDocs. Cet article explique la raison de cette migration, les avantages, les inconv\u00e9nients, et pourquoi j'ai remplac\u00e9 Disqus par Giscus pour g\u00e9rer les commentaires.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#pourquoi-passer-de-jekyll-a-mkdocs","title":"Pourquoi passer de Jekyll \u00e0 MkDocs ?","text":"<p>Jekyll est un excellent g\u00e9n\u00e9rateur de site statique, surtout si vous h\u00e9bergez sur GitHub Pages. Il prend bien en charge les blogs, les th\u00e8mes, et les plugins, mais il ne r\u00e9pondait plus \u00e0 tous mes besoins. Il manquait une barre de recherche, un mode clair/sombre, et mon site semblait bricol\u00e9 dans son ensemble. Maintenir des plateformes s\u00e9par\u00e9es pour le blog et la documentation commen\u00e7ait \u00e0 \u00eatre contre-productif. MkDocs, un g\u00e9n\u00e9rateur de site statique con\u00e7u principalement pour la documentation, est apparu comme l'alternative parfaite pour tout unifier.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#avantages-dutiliser-mkdocs-pour-mon-site","title":"Avantages d'utiliser MkDocs pour mon site","text":"","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#documentation-integree","title":"Documentation int\u00e9gr\u00e9e","text":"<p>MkDocs est con\u00e7u pour la documentation, ce qui le rend plus facile \u00e0 utiliser pour cr\u00e9er des tutoriels avec des blocs de notes, une barre de recherche et des fonctionnalit\u00e9s de navigation. Cette int\u00e9gration me permet de travailler facilement entre mes blogs et mes articles techniques sans changer de framework.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#workflow-simplifie","title":"Workflow simplifi\u00e9","text":"<p>MkDocs est extr\u00eamement simple. Avec sa structure bas\u00e9e sur Markdown, il n'est pas n\u00e9cessaire de g\u00e9rer des langages de template complexes tout comme Jekyll. Je peux tout \u00e9crire en Markdown, ce qui acc\u00e9l\u00e8re consid\u00e9rablement la cr\u00e9ation de contenu.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#support-multilingue-simplifie","title":"Support multilingue simplifi\u00e9","text":"<p>L'un de mes besoins essentiels \u00e9tait de prendre en charge \u00e0 la fois l'anglais et le fran\u00e7ais. Bien que Jekyll fournisse des moyens de g\u00e9rer les sites multilingues, MkDocs dispose de plugins comme mkdocs-static-i18n qui simplifient la gestion des traductions des pages. Il est \u00e9galement plus facile de suivre la documentation sp\u00e9cifique \u00e0 chaque langue sans structures de dossiers ou configurations compliqu\u00e9es.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#theme-material-pour-mkdocs","title":"Th\u00e8me Material pour MkDocs","text":"<p>Le th\u00e8me Material pour MkDocs offre une apparence \u00e9l\u00e9gante et moderne d\u00e8s l'installation. Avec quelques ajustements, j'ai pu donner un aspect visuellement attrayant au blog sans passer beaucoup de temps sur du CSS personnalis\u00e9 ou sur le d\u00e9veloppement de th\u00e8mes, contrairement \u00e0 Jekyll o\u00f9 je devais souvent bricoler les th\u00e8mes pour obtenir un r\u00e9sultat correct.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#inconvenients-de-passer-a-mkdocs","title":"Inconv\u00e9nients de passer \u00e0 MkDocs","text":"","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#moins-de-plugins-pour-les-blogs","title":"Moins de plugins pour les blogs","text":"<p>Jekyll dispose d'un \u00e9cosyst\u00e8me riche en plugins sp\u00e9cialement con\u00e7us pour les blogueurs, tandis que MkDocs en a moins. Par exemple, la gestion d'une s\u00e9rie d'articles ou de taxonomies personnalis\u00e9es est plus mature avec Jekyll, tandis que MkDocs n\u00e9cessite quelques contournements pour obtenir des fonctionnalit\u00e9s similaires. N\u00e9anmoins, je n'utilisais pas cette fonctionnalit\u00e9, donc ce n'est pas une grosse perte pour moi.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#flexibilite-limitee-des-themes","title":"Flexibilit\u00e9 limit\u00e9e des th\u00e8mes","text":"<p>MkDocs est plus rigide en termes de personnalisation des th\u00e8mes par rapport \u00e0 Jekyll. Bien que le th\u00e8me Material soit agr\u00e9able visuellement, il ne permet pas le m\u00eame niveau de personnalisation que les th\u00e8mes Jekyll sans plonger dans le CSS personnalis\u00e9. Cela est un gros avantage pour moi, car j'ai pass\u00e9 trop de temps \u00e0 modifier le th\u00e8me par d\u00e9faut de Jekyll, pour un r\u00e9sultat quelque peu moyen.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#giscus-au-lieu-de-disqus","title":"Giscus au lieu de Disqus","text":"<p>Un autre changement technique que j'ai effectu\u00e9 est le passage de Disqus \u00e0 Giscus pour la gestion des commentaires. Voici pourquoi :</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#respect-de-la-vie-privee-et-performances","title":"Respect de la vie priv\u00e9e et performances","text":"<p>Disqus, bien qu'il soit populaire, compromet la vie priv\u00e9e et les performances des pages. Il injecte des publicit\u00e9s et charge plusieurs trackers tiers, ce qui ralentit le chargement des pages. Giscus, en revanche, est l\u00e9ger et utilise GitHub Discussions pour les commentaires. Il est rapide, sans publicit\u00e9, et correspond mieux \u00e0 mon approche ax\u00e9e sur la confidentialit\u00e9.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#integration-avec-github","title":"Int\u00e9gration avec GitHub","text":"<p>Comme j'utilise d\u00e9j\u00e0 GitHub pour le contr\u00f4le de version et la collaboration sur divers projets, Giscus int\u00e8gre parfaitement les commentaires dans GitHub Discussions. Cela cr\u00e9e un espace unifi\u00e9 o\u00f9 les utilisateurs peuvent non seulement laisser des commentaires, mais aussi participer \u00e0 des discussions li\u00e9es aux articles de blog sp\u00e9cifiques.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#open-source","title":"Open Source","text":"<p>Giscus est open source, ce qui correspond \u00e0 ma pr\u00e9f\u00e9rence pour l'utilisation d'outils ouverts et transparents autant que possible. Le caract\u00e8re propri\u00e9taire de Disqus et son mod\u00e8le bas\u00e9 sur la publicit\u00e9 avaient \u00e9t\u00e9 un inconv\u00e9nient pour moi pendant un certain temps, donc Giscus \u00e9tait un choix naturel en cherchant des alternatives.</p>","tags":["D\u00e9velopement"]},{"location":"fr/blog/2024/09-25-website-migration-to-mkdocs/#conclusion","title":"Conclusion","text":"<p>Passer de Jekyll \u00e0 MkDocs a simplifi\u00e9 mon workflow, rendant plus facile la gestion \u00e0 la fois du site et de mon blog. Bien que MkDocs ne soit peut-\u00eatre pas le premier choix pour les blogueurs traditionnels, sa simplicit\u00e9 et son orientation vers la documentation en font un outil puissant pour les cr\u00e9ateurs de contenu technique comme moi. La transition m'a permis de simplifier mon installation et de tout organiser de mani\u00e8re plus coh\u00e9rente.</p> <p>Pour ceux qui g\u00e8rent des blogs ax\u00e9s sur la documentation ou qui ont des projets techniques n\u00e9cessitant une documentation d\u00e9taill\u00e9e, je recommande fortement d'essayer MkDocs (surtout avec le th\u00e8me Material). Et si vous cherchez un syst\u00e8me de commentaires rapide et respectueux de la vie priv\u00e9e, Giscus est un choix parfait.</p> <p>J'esp\u00e8re que cet article aidera ou inspirera certains d'entre vous. Si vous avez des suggestions, n'h\u00e9sitez pas \u00e0 commenter ou \u00e0 me contacter.</p> <p>\u00c0 bient\u00f4t, Vincent.</p>","tags":["D\u00e9velopement"]}]}